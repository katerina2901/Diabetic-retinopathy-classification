{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-29T16:45:15.339585Z","iopub.status.busy":"2024-05-29T16:45:15.339321Z","iopub.status.idle":"2024-05-29T16:45:17.022293Z","shell.execute_reply":"2024-05-29T16:45:17.021204Z","shell.execute_reply.started":"2024-05-29T16:45:15.339559Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'MedViT'...\n","remote: Enumerating objects: 146, done.\u001b[K\n","remote: Counting objects: 100% (145/145), done.\u001b[K\n","remote: Compressing objects: 100% (80/80), done.\u001b[K\n","remote: Total 146 (delta 70), reused 125 (delta 57), pack-reused 1\u001b[K\n","Receiving objects: 100% (146/146), 804.62 KiB | 7.06 MiB/s, done.\n","Resolving deltas: 100% (70/70), done.\n"]}],"source":["# Install MedVit\n","!git clone https://github.com/Omid-Nejati/MedViT.git\n","# %cd /MedVit"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:45:17.024733Z","iopub.status.busy":"2024-05-29T16:45:17.024361Z","iopub.status.idle":"2024-05-29T16:45:18.030823Z","shell.execute_reply":"2024-05-29T16:45:18.029561Z","shell.execute_reply.started":"2024-05-29T16:45:17.024693Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;34mMedViT\u001b[0m/\n"]}],"source":["%ls"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:45:28.617438Z","iopub.status.busy":"2024-05-29T16:45:28.617108Z","iopub.status.idle":"2024-05-29T16:45:28.631358Z","shell.execute_reply":"2024-05-29T16:45:28.630264Z","shell.execute_reply.started":"2024-05-29T16:45:28.617411Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['config.json', 'preprocessor_config.json', 'model.safetensors']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","os.listdir('/kaggle/input/medvit-saved/MedViT_saved')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:45:32.873585Z","iopub.status.busy":"2024-05-29T16:45:32.873215Z","iopub.status.idle":"2024-05-29T16:45:32.878115Z","shell.execute_reply":"2024-05-29T16:45:32.877148Z","shell.execute_reply.started":"2024-05-29T16:45:32.873553Z"},"trusted":true},"outputs":[],"source":["import os\n","os.chdir('MedViT')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:45:33.612933Z","iopub.status.busy":"2024-05-29T16:45:33.612552Z","iopub.status.idle":"2024-05-29T16:45:56.245943Z","shell.execute_reply":"2024-05-29T16:45:56.244845Z","shell.execute_reply.started":"2024-05-29T16:45:33.612905Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting chardet\n","  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n","Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n","Collecting fire\n","  Downloading fire-0.6.0.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m864.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting fvcore\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m873.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire) (1.16.0)\n","Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire) (2.4.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore) (1.26.4)\n","Collecting yacs>=0.1.6 (from fvcore)\n","  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (6.0.1)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore) (4.66.1)\n","Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from fvcore) (9.5.0)\n","Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore) (0.9.0)\n","Collecting iopath>=0.1.7 (from fvcore)\n","  Downloading iopath-0.1.10.tar.gz (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m758.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore) (4.9.0)\n","Collecting portalocker (from iopath>=0.1.7->fvcore)\n","  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n","Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n","\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m806.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Building wheels for collected packages: fire, fvcore, iopath\n","  Building wheel for fire (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=8f6ee26f8b6c58702b377c11839b54966a5d2776580f7d3a95fea94f40424452\n","  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n","  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=6a097adbc7745dea05c810b7468d6c3080d167675281ffc55c3abc426c98de69\n","  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n","  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=728c5068ad7c0f7165cebcf77864a5261a411810677d32cdf3f05b075f9e80c5\n","  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n","Successfully built fire fvcore iopath\n","Installing collected packages: yacs, portalocker, fire, einops, chardet, iopath, fvcore\n","Successfully installed chardet-5.2.0 einops-0.8.0 fire-0.6.0 fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install chardet einops fire fvcore"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:45:56.248667Z","iopub.status.busy":"2024-05-29T16:45:56.248261Z","iopub.status.idle":"2024-05-29T16:46:09.405050Z","shell.execute_reply":"2024-05-29T16:46:09.403840Z","shell.execute_reply.started":"2024-05-29T16:45:56.248608Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting evaluate\n","  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\n","Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m881.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.2\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install evaluate transformers accelerate"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:50:47.750283Z","iopub.status.busy":"2024-05-29T16:50:47.749903Z","iopub.status.idle":"2024-05-29T16:50:47.865712Z","shell.execute_reply":"2024-05-29T16:50:47.864539Z","shell.execute_reply.started":"2024-05-29T16:50:47.750241Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","\n","from PIL import Image\n","# from torchinfo import summary\n","import torch\n","import os\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from typing import Tuple\n","from sklearn.metrics import roc_auc_score\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","# import torch.nn.functional as F\n","import torchvision.transforms as T\n","from torchvision.transforms import functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from datasets import load_dataset\n","import pandas as pd\n","\n","import random\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:46:17.790115Z","iopub.status.busy":"2024-05-29T16:46:17.789831Z","iopub.status.idle":"2024-05-29T16:46:17.855309Z","shell.execute_reply":"2024-05-29T16:46:17.854457Z","shell.execute_reply.started":"2024-05-29T16:46:17.790089Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number CUDA Devices: 1\n","Current cuda device:  0  **May not correspond to nvidia-smi ID above, check visibility parameter\n"]}],"source":["print('Number CUDA Devices:', torch.cuda.device_count())\n","print ('Current cuda device: ', torch.cuda.current_device(), ' **May not correspond to nvidia-smi ID above, check visibility parameter')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:46:50.750475Z","iopub.status.busy":"2024-05-29T16:46:50.749566Z","iopub.status.idle":"2024-05-29T16:46:50.759756Z","shell.execute_reply":"2024-05-29T16:46:50.758777Z","shell.execute_reply.started":"2024-05-29T16:46:50.750440Z"},"trusted":true},"outputs":[],"source":["# Define configuration\n","from transformers import PretrainedConfig\n","from typing import List\n","\n","\n","class MedViTConfig(PretrainedConfig):\n","    model_type = \"medvit\"\n","\n","    def __init__(\n","        self,\n","        stem_chs: List[int] = [64, 32, 64],\n","        depths: List[int] = [3, 4, 30, 3],\n","        path_dropout: float = 0.2,\n","        attn_drop: int = 0,\n","        drop: int = 0,\n","        num_classes: int = 5,\n","        strides: List[int] = [1, 2, 2, 2],\n","        sr_ratios: List[int] = [8, 4, 2, 1],\n","        head_dim: int = 32,\n","        mix_block_ratio: float = 0.75,\n","        use_checkpoint: bool = False,\n","        **kwargs\n","    ):\n","        self.stem_chs = stem_chs\n","        self.depths = depths\n","        self.path_dropout = path_dropout\n","        self.attn_drop = attn_drop\n","        self.drop = drop\n","        self.num_classes = num_classes\n","        self.strides = strides\n","        self.sr_ratios = sr_ratios\n","        self.head_dim = head_dim\n","        self.mix_block_ratio = mix_block_ratio\n","        self.use_checkpoint = use_checkpoint\n","        super().__init__(**kwargs)\n","\n","medvit_config = MedViTConfig()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:46:51.527124Z","iopub.status.busy":"2024-05-29T16:46:51.526147Z","iopub.status.idle":"2024-05-29T16:46:53.846703Z","shell.execute_reply":"2024-05-29T16:46:53.845680Z","shell.execute_reply.started":"2024-05-29T16:46:51.527068Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'MedViT.MedViT'>\n"]}],"source":["# Initialise a MedViT class\n","from transformers import PreTrainedModel\n","from MedViT import MedViT\n","print(MedViT)\n","class MedViTClassification(PreTrainedModel):\n","    config_class = MedViTConfig\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.model = MedViT(\n","        stem_chs = config.stem_chs,\n","        depths = config.depths,\n","        path_dropout = config.path_dropout,\n","        attn_drop = config.attn_drop,\n","        drop = config.drop,\n","        num_classes = config.num_classes,\n","        strides = config.strides,\n","        sr_ratios = config.sr_ratios,\n","        head_dim = config.head_dim,\n","        mix_block_ratio = config.mix_block_ratio,\n","        use_checkpoint = config.use_checkpoint,\n","        )\n","\n","    def forward(self, pixel_values, labels=None):\n","        logits = self.model(pixel_values)\n","        # loss = torch.nn.CrossEntropyLoss(logits, labels)\n","        loss = torch.nn.functional.cross_entropy(logits, labels)\n","        return {\"loss\": loss, \"logits\": logits}"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:46:53.848559Z","iopub.status.busy":"2024-05-29T16:46:53.848260Z","iopub.status.idle":"2024-05-29T16:46:55.225637Z","shell.execute_reply":"2024-05-29T16:46:55.224486Z","shell.execute_reply.started":"2024-05-29T16:46:53.848532Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["initialize_weights...\n"]}],"source":["# Initialize a model\n","model = MedViTClassification(medvit_config)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:46:55.227086Z","iopub.status.busy":"2024-05-29T16:46:55.226809Z","iopub.status.idle":"2024-05-29T16:46:55.233951Z","shell.execute_reply":"2024-05-29T16:46:55.232814Z","shell.execute_reply.started":"2024-05-29T16:46:55.227061Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working\n"]}],"source":["%cd ../"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:46:55.237773Z","iopub.status.busy":"2024-05-29T16:46:55.236639Z","iopub.status.idle":"2024-05-29T16:46:55.246025Z","shell.execute_reply":"2024-05-29T16:46:55.245194Z","shell.execute_reply.started":"2024-05-29T16:46:55.237740Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['image_labels.csv']"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["os.listdir(r'../input/labels')"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:46:55.247436Z","iopub.status.busy":"2024-05-29T16:46:55.247071Z","iopub.status.idle":"2024-05-29T16:46:55.290015Z","shell.execute_reply":"2024-05-29T16:46:55.289149Z","shell.execute_reply.started":"2024-05-29T16:46:55.247410Z"},"trusted":true},"outputs":[],"source":["# dataset_folder_name = r\"../input/short-imbalanced-dataset/short_imbalanced_dataset\"\n","# dataset_folder_name = r\"../input/dr-dataset\"\n","\n","def load_dataset_path2images(dataset_folder_name):\n","    train_test_folders = os.listdir(dataset_folder_name)\n","    datasets = {}\n","    for trts_split in train_test_folders:\n","        class_folders = os.listdir(dataset_folder_name+'/'+trts_split)\n","#         class_folders = os.listdir(dataset_folder_name + '\\\\' + trts_split)\n","        labels = []\n","        paths = []\n","        for class_folder in class_folders:\n","            image_names = os.listdir(dataset_folder_name+'/'+trts_split+'/'+class_folder)\n","            image_paths = [dataset_folder_name+'/'+trts_split+'/'+class_folder+'/'+x for x in image_names]\n","#             image_names = os.listdir(dataset_folder_name + '\\\\' + trts_split + '\\\\' + class_folder)\n","#             image_paths = [dataset_folder_name + '\\\\' + trts_split + '\\\\' + class_folder + '\\\\' + x for x in image_names]\n","            class_labels = [int(class_folder)] * len(image_paths)\n","            labels.extend(class_labels)\n","            paths.extend(image_paths)\n","        local_dataset = {'image_path' : paths, 'label' : labels}\n","        datasets[trts_split] = pd.DataFrame.from_dict(local_dataset)\n","\n","    return datasets\n","\n","# dataset = load_dataset_path2images(dataset_folder_name)\n","dataset = pd.read_csv(r'/kaggle/input/labels/image_labels.csv')\n","dataset['path'], dataset['ext'] = '/kaggle/input/dr-train/train/', '.jpeg'\n","dataset['image_path'] = dataset['path'] + dataset['image'] + dataset['ext']\n","dataset.drop(columns = ['image', 'path', 'ext'], inplace = True)\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:46:55.291341Z","iopub.status.busy":"2024-05-29T16:46:55.291051Z","iopub.status.idle":"2024-05-29T16:46:55.300797Z","shell.execute_reply":"2024-05-29T16:46:55.299918Z","shell.execute_reply.started":"2024-05-29T16:46:55.291315Z"},"trusted":true},"outputs":[],"source":["\n","# oversampling just repeating minority class items\n","# enought times to be equal to major dataset in size\n","\n","##############################################################################################\n","\n","# max_size = train_dataset['label'].value_counts().max()\n","# lst = [train_dataset]\n","def resample(_dataset, ratio = 3):\n","    min_size = _dataset['label'].value_counts().min()\n","    lst = []\n","    added_unique_rows = 0\n","    all_n_rows = 0\n","\n","    for class_index, group in _dataset.groupby('label'):\n","        # lst.append(group.sample(max_size-len(group), replace=True))\n","        all_n_rows += len(group)\n","        if class_index == 0:\n","            added_unique_rows += min_size*ratio\n","            lst.append(group.sample(min_size*ratio, replace=False))\n","        else:\n","            if len(group) > min_size*ratio:\n","                added_unique_rows += min_size*ratio\n","                lst.append(group.sample(min_size*ratio, replace=False))\n","            else:\n","                lst.append(group)\n","                added_unique_rows += len(group)\n","                lst.append(group.sample(min_size*ratio-len(group), replace=True))\n","\n","    _dataset = pd.concat(lst)\n","\n","    for class_index, group in _dataset.groupby('label'):\n","        print(f'{class_index}: length: {len(group)}')\n","\n","    print('N_added_rows: ', added_unique_rows)\n","    print('N_all_rows: ', all_n_rows)\n","    print('Ratio of used rows: ', added_unique_rows/all_n_rows)\n","    return _dataset"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:46:55.302308Z","iopub.status.busy":"2024-05-29T16:46:55.302015Z","iopub.status.idle":"2024-05-29T16:46:55.329675Z","shell.execute_reply":"2024-05-29T16:46:55.328762Z","shell.execute_reply.started":"2024-05-29T16:46:55.302282Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>image_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>/kaggle/input/dr-train/train/10003_left.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>/kaggle/input/dr-train/train/10003_right.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>/kaggle/input/dr-train/train/10007_left.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>/kaggle/input/dr-train/train/10007_right.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>/kaggle/input/dr-train/train/10009_left.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8403</th>\n","      <td>0</td>\n","      <td>/kaggle/input/dr-train/train/19494_right.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>8404</th>\n","      <td>0</td>\n","      <td>/kaggle/input/dr-train/train/19498_left.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>8405</th>\n","      <td>0</td>\n","      <td>/kaggle/input/dr-train/train/19498_right.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>8406</th>\n","      <td>0</td>\n","      <td>/kaggle/input/dr-train/train/194_left.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>8407</th>\n","      <td>0</td>\n","      <td>/kaggle/input/dr-train/train/194_right.jpeg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8408 rows × 2 columns</p>\n","</div>"],"text/plain":["      label                                     image_path\n","0         0   /kaggle/input/dr-train/train/10003_left.jpeg\n","1         0  /kaggle/input/dr-train/train/10003_right.jpeg\n","2         0   /kaggle/input/dr-train/train/10007_left.jpeg\n","3         0  /kaggle/input/dr-train/train/10007_right.jpeg\n","4         0   /kaggle/input/dr-train/train/10009_left.jpeg\n","...     ...                                            ...\n","8403      0  /kaggle/input/dr-train/train/19494_right.jpeg\n","8404      0   /kaggle/input/dr-train/train/19498_left.jpeg\n","8405      0  /kaggle/input/dr-train/train/19498_right.jpeg\n","8406      0     /kaggle/input/dr-train/train/194_left.jpeg\n","8407      0    /kaggle/input/dr-train/train/194_right.jpeg\n","\n","[8408 rows x 2 columns]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:46:55.777328Z","iopub.status.busy":"2024-05-29T16:46:55.776969Z","iopub.status.idle":"2024-05-29T16:46:55.803504Z","shell.execute_reply":"2024-05-29T16:46:55.802655Z","shell.execute_reply.started":"2024-05-29T16:46:55.777297Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0: length: 3320\n","1: length: 3320\n","2: length: 3320\n","3: length: 3320\n","4: length: 3320\n","N_added_rows:  5578\n","N_all_rows:  8408\n","Ratio of used rows:  0.6634157944814463\n"]}],"source":["##############################################################################################\n","train_dataset = resample(dataset, ratio = 20)\n","# test_dataset = resample(dataset['short_test'], ratio = 1)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:46:56.706937Z","iopub.status.busy":"2024-05-29T16:46:56.706116Z","iopub.status.idle":"2024-05-29T16:47:07.966709Z","shell.execute_reply":"2024-05-29T16:47:07.965803Z","shell.execute_reply.started":"2024-05-29T16:46:56.706905Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-29 16:46:58.808323: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-29 16:46:58.808430: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-29 16:46:58.966578: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5cbefc6338be4f998e64ac9ed2e3ca31","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"]}],"source":["mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n","from datasets import Dataset\n","from transformers import ViTImageProcessor\n","from transformers import AutoImageProcessor\n","\n","# model_name_or_path = 'google/vit-base-patch16-224-in21k'\n","# model_name_or_path = \"microsoft/swinv2-tiny-patch4-window8-256\"\n","model_name_or_path = \"microsoft/swin-base-patch4-window12-384\"\n","# model_name_or_path = \"/kaggle/input/medvit-saved/MedViT_saved\"\n","\n","# processor = ViTImageProcessor.from_pretrained(model_name_or_path)\n","image_processor = AutoImageProcessor.from_pretrained(model_name_or_path)\n","# model = MedViTClassification.from_pretrained(model_name_or_path)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:47:07.968981Z","iopub.status.busy":"2024-05-29T16:47:07.968698Z","iopub.status.idle":"2024-05-29T16:47:08.009555Z","shell.execute_reply":"2024-05-29T16:47:08.008731Z","shell.execute_reply.started":"2024-05-29T16:47:07.968955Z"},"trusted":true},"outputs":[],"source":["from typing import Any\n","class Spot(object):\n","    def __init__(self, size, prob = 0.5):\n","        self.size = size\n","        self.prob = prob\n","        self.center = None\n","        self.radius = None\n","        self.zeros = torch.zeros((self.size, self.size)) #.cuda()\n","        self.ones = torch.ones((3, 1)) #.cuda()\n","        self.tensor_to_image = T.ToPILImage()\n","        self.image_to_tensor = T.ToTensor()\n","\n","    def __call__(self, image_tensors, target = None):\n","        if random.random() < self.prob:\n","            image_tensors = self.image_to_tensor(image_tensors)\n","#             print('Yes')\n","#             modified_image_tensors = image_tensors.clone()\n","            # print(f'Min value: {torch.amin(modified_image_tensors)}')\n","            # print(f'Max value: {torch.amax(modified_image_tensors)}')\n","            n_spots = random.randint(5, 7)\n","            self.initial_mask = self.zeros.clone()\n","\n","            self.dim1_offset = (image_tensors.shape[1] - self.size) // 2\n","            self.dim2_offset = (image_tensors.shape[2] - self.size) // 2\n","\n","            for _ in range(n_spots):\n","                new_image_tensors = self.add_random_spot(image_tensors)\n","#                 modified_image_tensors = self.add_random_spot(modified_image_tensors)\n","            return torch.clamp(new_image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n","#             return torch.clamp(modified_image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n","        else: return image_tensors\n","        \n","    def add_random_spot(self, image_tensor):\n","        self.radius = random.randint(int(0.01 * self.size) + 1, int(0.05 * self.size))\n","        self.center = [random.randint(self.radius + 1, self.size - self.radius - 1), \n","                       random.randint(self.radius + 1, self.size - self.radius - 1)]\n","        y, x = np.ogrid[: self.size, : self.size]\n","        dist_from_center = np.sqrt((x - self.center[0]) ** 2 + (y - self.center[1]) ** 2)\n","        circle = dist_from_center <= (self.radius // 2)\n","\n","        k = 14 / 25 + (1.0 - self.radius / 25)\n","        beta = 0.5 + (1.5 - 0.5) * self.radius / 25\n","        A = k * self.ones.clone()\n","        d = 0.3 * self.radius / 25\n","        t = np.exp(-beta * d)\n","\n","        spot_mask = self.zeros.clone()\n","        spot_mask[circle] = torch.multiply(A[0], torch.tensor(1 - t))\n","\n","        self.initial_mask = self.initial_mask + spot_mask\n","        self.initial_mask[self.initial_mask != 0] = 1\n","\n","        sigma = (5 + (2 - 0) * self.radius / 25) * 2\n","        rad_w = random.randint(int(sigma / 5), int(sigma / 4))\n","        rad_h = random.randint(int(sigma / 5), int(sigma / 4))\n","\n","        if (rad_w % 2) == 0: rad_w = rad_w + 1\n","        if (rad_h % 2) == 0: rad_h = rad_h + 1\n","\n","        spot_mask = F.gaussian_blur(torch.reshape(spot_mask, (1, self.size, self.size)), (rad_w, rad_h), sigma)\n","        spot_mask = torch.stack([spot_mask, spot_mask, spot_mask]) * 255\n","        \n","        image_tensor[:, self.dim1_offset : self.dim1_offset + self.size, self.dim2_offset : self.dim2_offset + self.size] += torch.reshape(spot_mask, (3, self.size, self.size))\n","        return image_tensor\n","\n","class Halo(object):\n","    def __init__(self, size, prob = 0.5, intensity_range = (0.8, 1.2)):\n","        self.size = size\n","        self.prob = prob\n","        self.center = None\n","        self.radius = None\n","        self.intensity_range = intensity_range\n","        self.tensor_to_image = T.ToPILImage()\n","        self.image_to_tensor = T.ToTensor()\n","\n","    def __call__(self, image_tensors, target = None):\n","        if random.random() < self.prob:\n","            image_tensors = self.image_to_tensor(image_tensors)\n","#             print('Yes')\n","#             modified_image_tensors = image_tensors.clone()\n","            # print(f'Min value: {torch.amin(modified_image_tensors)}')\n","            # print(f'Max value: {torch.amax(modified_image_tensors)}')\n","            n_halos = random.randint(5, 7)\n","\n","            self.dim1_offset = (image_tensors.shape[1] - self.size) // 2\n","            self.dim2_offset = (image_tensors.shape[2] - self.size) // 2\n","            \n","            for _ in range(n_halos):\n","                image_tensors = self.add_random_halo(image_tensors)\n","#                 modified_image_tensors = self.add_random_halo(modified_image_tensors)\n","            return torch.clamp(image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n","#             return torch.clamp(modified_image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n","        else: return image_tensors\n","\n","    def add_random_halo(self, image_tensor):\n","        self.radius = random.randint(int(0.01 * self.size), int(0.05 * self.size))\n","        self.center = [random.randint(self.radius + 1, self.size - self.radius - 1),\n","                        random.randint(self.radius + 1, self.size - self.radius - 1)]\n","        \n","        y, x = torch.meshgrid(torch.arange(self.size), torch.arange(self.size))\n","        dist_from_center = torch.sqrt(((x - self.center[0]) ** 2 + (y - self.center[1]) ** 2))\n","        normalized_dist = dist_from_center / self.radius\n","        \n","        halo_intensity = torch.clamp(self.intensity_range[0] + (self.intensity_range[1] - self.intensity_range[0]) * (1 - normalized_dist), min = 0, max = 1)\n","        halo_mask = dist_from_center <= self.radius // 2\n","        halo_effect = halo_intensity * (self.radius - dist_from_center) / self.radius\n","        halo_effect = np.clip(halo_effect, 0, 1)\n","        halo_effect = np.expand_dims(halo_effect, axis = 0)\n","        halo_effect = np.repeat(halo_effect, image_tensor.shape[0], axis = 0)\n","        image_tensor[:, halo_mask] = image_tensor[:, halo_mask] * (1 - halo_effect[:, halo_mask]) + halo_effect[:, halo_mask] * 255\n","\n","        return image_tensor\n","\n","class Hole(object):\n","    def __init__(self, size, prob = 0.5):\n","        self.size = size\n","        self.prob = prob\n","        self.center = None\n","        self.radius = None\n","        self.tensor_to_image = T.ToPILImage()\n","        self.image_to_tensor = T.ToTensor()\n","\n","    def __call__(self, image_tensors, target = None):\n","        if random.random() < self.prob:\n","            image_tensors = self.image_to_tensor(image_tensors)\n","#             print('Yes')\n","#             modified_image_tensors = image_tensors.clone()\n","            # print(f'Min value: {torch.amin(modified_image_tensors)}')\n","            # print(f'Max value: {torch.amax(modified_image_tensors)}')\n","            n_halos = random.randint(5, 7)\n","\n","            self.dim1_offset = (image_tensors.shape[1] - self.size) // 2\n","            self.dim2_offset = (image_tensors.shape[2] - self.size) // 2\n","            \n","            for _ in range(n_halos):\n","                image_tensors = self.add_random_hole(image_tensors)\n","#                 modified_image_tensors = self.add_random_hole(modified_image_tensors)\n","            return torch.clamp(image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n","#             return torch.clamp(modified_image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n","        else: return image_tensors\n","\n","    def add_random_hole(self, image_tensor):\n","        self.radius = random.randint(int(0.01 * self.size), int(0.05 * self.size))\n","        self.center = [random.randint(self.radius + 1, self.size - self.radius - 1),\n","                        random.randint(self.radius + 1, self.size - self.radius - 1)]\n","        \n","        y, x = torch.meshgrid(torch.arange(self.size), torch.arange(self.size))\n","        dist_from_center = torch.sqrt(((x - self.center[0]) ** 2 + (y - self.center[1]) ** 2))\n","        \n","        hole_mask = dist_from_center <= self.radius // 2\n","        image_tensor[:, hole_mask] = 0\n","\n","        return image_tensor"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:47:08.011111Z","iopub.status.busy":"2024-05-29T16:47:08.010806Z","iopub.status.idle":"2024-05-29T16:47:08.033211Z","shell.execute_reply":"2024-05-29T16:47:08.032383Z","shell.execute_reply.started":"2024-05-29T16:47:08.011059Z"},"trusted":true},"outputs":[],"source":["from PIL import ImageEnhance\n","\n","def RandomSharpen(image, alpha = 0.2):\n","    sharpener = ImageEnhance.Sharpness(image)\n","    factor = 0.5  \n","    image = sharpener.enhance(1.0 + alpha * factor)\n","    return image"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:47:08.035546Z","iopub.status.busy":"2024-05-29T16:47:08.035251Z","iopub.status.idle":"2024-05-29T16:47:08.044644Z","shell.execute_reply":"2024-05-29T16:47:08.043660Z","shell.execute_reply.started":"2024-05-29T16:47:08.035523Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(384, 384)\n"]}],"source":["size = (\n","    image_processor.size[\"shortest_edge\"]\n","    if \"shortest_edge\" in image_processor.size\n","    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",")\n","\n","print(size)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:47:08.046107Z","iopub.status.busy":"2024-05-29T16:47:08.045813Z","iopub.status.idle":"2024-05-29T16:47:08.053190Z","shell.execute_reply":"2024-05-29T16:47:08.052358Z","shell.execute_reply.started":"2024-05-29T16:47:08.046068Z"},"trusted":true},"outputs":[],"source":["_transforms_train = T.Compose([\n","    T.RandomHorizontalFlip(p = 0.5),\n","    T.RandomVerticalFlip(p = 0.5),\n","    T.RandomCrop(2000, padding_mode='symmetric', pad_if_needed=True),\n","#     Spot(size[0]),\n","    # Halo(),\n","    # Hole(),\n","#     T.Lambda(RandomSharpen),\n","    # Blur()\n","])\n","\n","_transforms_test = T.Compose([\n","    T.CenterCrop(2000),\n","])"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:47:08.054857Z","iopub.status.busy":"2024-05-29T16:47:08.054508Z","iopub.status.idle":"2024-05-29T16:47:08.067694Z","shell.execute_reply":"2024-05-29T16:47:08.066774Z","shell.execute_reply.started":"2024-05-29T16:47:08.054826Z"},"trusted":true},"outputs":[],"source":["def load_image(path_image, label, mode):\n","    # load image\n","    try:\n","        image = Image.open(path_image)\n","#         print(image)\n","#         image.verify()  # Verify the image is valid\n","        if mode == 'train':\n","            image = _transforms_train(image)\n","\n","            return image\n","        else:\n","            return image\n","    except (IOError, SyntaxError) as e:\n","        \n","        image = Image.open('/kaggle/input/dr-train/train/10003_left.jpeg')\n","#         image.verify()  # Verify the image is valid\n","        if mode == 'train':\n","            image = _transforms_train(image)\n","\n","            return image\n","        else:\n","            return image\n","\n","\n","def func_transform(examples):\n","\n","    # loaded_images = [load_image(path, lb, 'train').convert(\"RGB\") for path, lb in zip(examples['image_path'], examples['label'])]\n","    # _transforms(img.convert(\"RGB\"))\n","    inputs = image_processor([load_image(path, lb, 'train')\n","                                for path, lb in zip(examples['image_path'], examples['label'])], return_tensors='pt')\n","    inputs['label'] = examples['label']\n","    return inputs\n","\n","    ###############################\n","\n","    # examples[\"pixel_values\"] = [load_image(path, lb, 'train')\n","    #                             for path, lb in zip(examples['image_path'], examples['label'])]\n","    # del examples[\"image_path\"]\n","    # return examples\n","\n","\n","\n","def func_transform_test(examples):\n","\n","    # loaded_images = [load_image(path, lb, 'test').convert(\"RGB\") for path, lb in zip(examples['image_path'], examples['label'])]\n","    inputs = image_processor([load_image(path, lb, 'test')\n","                                for path, lb in zip(examples['image_path'], examples['label'])], return_tensors='pt')\n","    inputs['label'] = examples['label']\n","    return inputs\n","\n","    ########################################\n","    # examples[\"pixel_values\"] = [load_image(path, lb, 'test')\n","    #                             for path, lb in zip(examples['image_path'], examples['label'])]\n","    # del examples[\"image_path\"]\n","    # return examples"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:47:08.069166Z","iopub.status.busy":"2024-05-29T16:47:08.068838Z","iopub.status.idle":"2024-05-29T16:47:08.112986Z","shell.execute_reply":"2024-05-29T16:47:08.112294Z","shell.execute_reply.started":"2024-05-29T16:47:08.069140Z"},"trusted":true},"outputs":[],"source":["train_ds = Dataset.from_pandas(train_dataset, preserve_index=False)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:47:08.114244Z","iopub.status.busy":"2024-05-29T16:47:08.113980Z","iopub.status.idle":"2024-05-29T16:47:08.138047Z","shell.execute_reply":"2024-05-29T16:47:08.137396Z","shell.execute_reply.started":"2024-05-29T16:47:08.114221Z"},"trusted":true},"outputs":[],"source":["train_test_dataset = train_ds.train_test_split(test_size = 0.20, seed = 42)\n","train_dataset, test_dataset = train_test_dataset['train'], train_test_dataset['test']\n","# train_dataset = Dataset.from_pandas(train_dataset, preserve_index=False)\n","# test_dataset = Dataset.from_pandas(test_dataset, preserve_index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:47:08.139299Z","iopub.status.busy":"2024-05-29T16:47:08.139043Z","iopub.status.idle":"2024-05-29T16:47:08.184457Z","shell.execute_reply":"2024-05-29T16:47:08.183600Z","shell.execute_reply.started":"2024-05-29T16:47:08.139276Z"},"trusted":true},"outputs":[],"source":["prepared_train_dataset = train_dataset.with_transform(func_transform)\n","prepared_test_dataset = test_dataset.with_transform(func_transform_test)\n","prepared_train_dataset = prepared_train_dataset.shuffle(seed = 42)\n","prepared_test_dataset = prepared_test_dataset.shuffle(seed = 42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:47:08.187964Z","iopub.status.busy":"2024-05-29T16:47:08.187640Z","iopub.status.idle":"2024-05-29T16:47:08.194093Z","shell.execute_reply":"2024-05-29T16:47:08.193156Z","shell.execute_reply.started":"2024-05-29T16:47:08.187940Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["rows in train_dataset:  13280\n","ID2label:  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n"]}],"source":["print(\"rows in train_dataset: \", len(prepared_train_dataset))\n","# print(\"rows in test_dataset: \", len(prepared_test_dataset))\n","\n","# labels = prepared_ds_train.features[\"label\"].names()\n","labels = [0, 1, 2, 3, 4]\n","label2id, id2label = dict(), dict()\n","\n","for i, label in enumerate(labels):\n","    label2id[label] = i\n","    id2label[i] = label\n","\n","print(\"ID2label: \", id2label)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:47:08.195676Z","iopub.status.busy":"2024-05-29T16:47:08.195337Z","iopub.status.idle":"2024-05-29T16:47:08.203946Z","shell.execute_reply":"2024-05-29T16:47:08.203148Z","shell.execute_reply.started":"2024-05-29T16:47:08.195646Z"},"trusted":true},"outputs":[],"source":["def collate_fn(batch):\n","    return {\n","        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n","        # 'tensor': torch.stack([x['tensor'] for x in batch]),\n","        'labels': torch.tensor([x['label'] for x in batch])\n","    }\n","\n","def calculate_per_class_accuracy(confusion_matrix):\n","        num_classes = confusion_matrix.shape[0]\n","        per_class_accuracy = []\n","\n","        for i in range(num_classes):\n","            TP = confusion_matrix[i, i]\n","            FN = np.sum(confusion_matrix[i, :]) - TP\n","            FP = np.sum(confusion_matrix[:, i]) - TP\n","            TN = np.sum(confusion_matrix) - (TP + FP + FN)\n","\n","            accuracy = (TP + TN) / (TP + TN + FP + FN)\n","            per_class_accuracy.append(accuracy)\n","\n","        return per_class_accuracy"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:57:07.433976Z","iopub.status.busy":"2024-05-29T16:57:07.432749Z","iopub.status.idle":"2024-05-29T16:57:07.813700Z","shell.execute_reply":"2024-05-29T16:57:07.812624Z","shell.execute_reply.started":"2024-05-29T16:57:07.433932Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import cohen_kappa_score, confusion_matrix\n","from sklearn.metrics import f1_score #, kappa\n","# from sklearn import metrics\n","\n","import evaluate\n","\n","accuracy = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    predictions_proba, labels = eval_pred\n","\n","    if predictions_proba.shape[1] > 1:  # Check if we have more than one class\n","        predictions_proba = torch.nn.functional.softmax(torch.tensor(predictions_proba), dim=-1).numpy()\n","    \n","    # print(predictions)\n","    predictions = np.argmax(predictions_proba, axis=1)\n","    # print(predictions)\n","    # print(labels)\n","    result_accuracy = accuracy.compute(predictions=predictions, references=labels)\n","    \n","    cm = confusion_matrix(labels, predictions)\n","    print(cm)\n","    perclass_acc = calculate_per_class_accuracy(cm)\n","    \n","#     print(f'per class accuracies: {perclass_acc}')\n","    \n","    \n","    result = {\n","             'accuracy': np.mean([result_accuracy['accuracy']]),\n","             'kappa': np.mean([cohen_kappa_score(labels, predictions, weights = \"quadratic\")]),\n","             # 'quadratic_kappa': np.mean([kappa(labels, predictions, weights = \"quadratic\")]),\n","             'f1': np.mean([f1_score(labels, predictions, average='weighted')]),\n","             'roc_auc': np.mean([roc_auc_score(labels, predictions_proba, multi_class='ovr')]),\n","             'class_0' : perclass_acc[0],\n","             'class_1' : perclass_acc[1],\n","             'class_2' : perclass_acc[2],\n","             'class_3' : perclass_acc[3],\n","             'class_4' : perclass_acc[4],\n","             }\n","\n","    \n","    \n","#     print(f\"\\nClass 0 Accuracy (vs Others): {class_0:.2f}%\")\n","#     print(f\"Other Classes Accuracy: {remaining_classes:.2f}%\")\n","    \n","    # print(cohen_kappa_score(labels, predictions))\n","    # print(result)\n","\n","    return result\n"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:57:10.281487Z","iopub.status.busy":"2024-05-29T16:57:10.281105Z","iopub.status.idle":"2024-05-29T16:57:10.325377Z","shell.execute_reply":"2024-05-29T16:57:10.324320Z","shell.execute_reply.started":"2024-05-29T16:57:10.281458Z"},"trusted":true},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./MedViT-base\",\n","    evaluation_strategy=\"steps\",\n","    logging_steps=20,\n","\n","    save_steps=20,\n","    eval_steps=20,\n","    save_total_limit=2,\n","\n","    # report_to=\"wandb\",  # enable logging to W&B\n","    # run_name=\"swin384_shrp_rt20\",  # name of the W&B run (optional)\n","\n","    remove_unused_columns=False,\n","    dataloader_num_workers = 2,\n","\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=2,\n","    per_device_eval_batch_size=2,\n","    num_train_epochs=2,\n","    warmup_ratio=0.1,\n","\n","    metric_for_best_model=\"kappa\",\n","    greater_is_better = True,\n","    load_best_model_at_end=True,\n","\n","    push_to_hub=False\n",")"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:57:10.554461Z","iopub.status.busy":"2024-05-29T16:57:10.554172Z","iopub.status.idle":"2024-05-29T16:57:10.583572Z","shell.execute_reply":"2024-05-29T16:57:10.582510Z","shell.execute_reply.started":"2024-05-29T16:57:10.554434Z"},"trusted":true},"outputs":[],"source":["sample_ids = np.random.choice(len(prepared_test_dataset), size=250, replace=False)\n","inv_sample_ids = np.setdiff1d(np.arange(len(prepared_test_dataset)), sample_ids)\n","val_ds = prepared_test_dataset.select(sample_ids)\n","test_ds = prepared_test_dataset.select(inv_sample_ids)"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:57:10.935570Z","iopub.status.busy":"2024-05-29T16:57:10.934860Z","iopub.status.idle":"2024-05-29T16:57:10.943203Z","shell.execute_reply":"2024-05-29T16:57:10.942123Z","shell.execute_reply.started":"2024-05-29T16:57:10.935542Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(Dataset({\n","     features: ['label', 'image_path'],\n","     num_rows: 250\n"," }),\n"," Dataset({\n","     features: ['label', 'image_path'],\n","     num_rows: 3070\n"," }))"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["val_ds, test_ds"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:57:11.318956Z","iopub.status.busy":"2024-05-29T16:57:11.318653Z","iopub.status.idle":"2024-05-29T16:57:11.349388Z","shell.execute_reply":"2024-05-29T16:57:11.348471Z","shell.execute_reply.started":"2024-05-29T16:57:11.318932Z"},"trusted":true},"outputs":[],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=collate_fn,\n","    compute_metrics=compute_metrics,\n","    train_dataset=prepared_train_dataset,\n","#     eval_dataset=prepared_test_dataset,\n","    eval_dataset=val_ds,\n","    tokenizer=image_processor,\n",")"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:57:11.725388Z","iopub.status.busy":"2024-05-29T16:57:11.725083Z","iopub.status.idle":"2024-05-29T16:57:11.836029Z","shell.execute_reply":"2024-05-29T16:57:11.834874Z","shell.execute_reply.started":"2024-05-29T16:57:11.725360Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:57:12.112668Z","iopub.status.busy":"2024-05-29T16:57:12.112366Z","iopub.status.idle":"2024-05-29T16:58:07.130232Z","shell.execute_reply":"2024-05-29T16:58:07.128600Z","shell.execute_reply.started":"2024-05-29T16:57:12.112643Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='27' max='6640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  27/6640 00:51 < 3:48:54, 0.48 it/s, Epoch 0.01/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Kappa</th>\n","      <th>F1</th>\n","      <th>Roc Auc</th>\n","      <th>Class 0</th>\n","      <th>Class 1</th>\n","      <th>Class 2</th>\n","      <th>Class 3</th>\n","      <th>Class 4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>20</td>\n","      <td>1.601500</td>\n","      <td>1.811502</td>\n","      <td>0.148000</td>\n","      <td>0.003003</td>\n","      <td>0.112020</td>\n","      <td>0.454035</td>\n","      <td>0.788000</td>\n","      <td>0.464000</td>\n","      <td>0.584000</td>\n","      <td>0.692000</td>\n","      <td>0.768000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[[ 0 29 14  4  5]\n"," [ 0 20 15  7  4]\n"," [ 1 27 10  5  3]\n"," [ 0 34 19  3  2]\n"," [ 0 18 20  6  4]]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# CUDA_LAUNCH_BLOCKING=1\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[1;32m      4\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlog_metrics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_results\u001b[38;5;241m.\u001b[39mmetrics)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# CUDA_LAUNCH_BLOCKING=1\n","train_results = trainer.train()\n","trainer.save_model()\n","trainer.log_metrics(\"train\", train_results.metrics)\n","trainer.save_metrics(\"train\", train_results.metrics)\n","trainer.save_state()"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T13:43:44.360898Z","iopub.status.busy":"2024-05-26T13:43:44.360228Z","iopub.status.idle":"2024-05-26T13:43:44.366222Z","shell.execute_reply":"2024-05-26T13:43:44.365064Z","shell.execute_reply.started":"2024-05-26T13:43:44.360862Z"},"trusted":true},"outputs":[],"source":["# 471257f8658cc55c4ec33930066c1c6d1f101821"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = trainer.evaluate(test_ds)\n","trainer.log_metrics(\"eval\", metrics)\n","trainer.save_metrics(\"eval\", metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5088496,"sourceId":8522001,"sourceType":"datasetVersion"},{"datasetId":5090145,"sourceId":8524305,"sourceType":"datasetVersion"},{"datasetId":5090620,"sourceId":8524979,"sourceType":"datasetVersion"},{"datasetId":5090862,"sourceId":8525303,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
