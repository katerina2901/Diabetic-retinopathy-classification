{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8522001,"sourceType":"datasetVersion","datasetId":5088496},{"sourceId":8524305,"sourceType":"datasetVersion","datasetId":5090145},{"sourceId":8524979,"sourceType":"datasetVersion","datasetId":5090620},{"sourceId":8525303,"sourceType":"datasetVersion","datasetId":5090862}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install MedVit\n!git clone https://github.com/Omid-Nejati/MedViT.git\n# %cd /MedVit","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-29T16:45:15.339321Z","iopub.execute_input":"2024-05-29T16:45:15.339585Z","iopub.status.idle":"2024-05-29T16:45:17.022293Z","shell.execute_reply.started":"2024-05-29T16:45:15.339559Z","shell.execute_reply":"2024-05-29T16:45:17.021204Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'MedViT'...\nremote: Enumerating objects: 146, done.\u001b[K\nremote: Counting objects: 100% (145/145), done.\u001b[K\nremote: Compressing objects: 100% (80/80), done.\u001b[K\nremote: Total 146 (delta 70), reused 125 (delta 57), pack-reused 1\u001b[K\nReceiving objects: 100% (146/146), 804.62 KiB | 7.06 MiB/s, done.\nResolving deltas: 100% (70/70), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%ls","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:45:17.024361Z","iopub.execute_input":"2024-05-29T16:45:17.024733Z","iopub.status.idle":"2024-05-29T16:45:18.030823Z","shell.execute_reply.started":"2024-05-29T16:45:17.024693Z","shell.execute_reply":"2024-05-29T16:45:18.029561Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[0m\u001b[01;34mMedViT\u001b[0m/\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.listdir('/kaggle/input/medvit-saved/MedViT_saved')","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:45:28.617108Z","iopub.execute_input":"2024-05-29T16:45:28.617438Z","iopub.status.idle":"2024-05-29T16:45:28.631358Z","shell.execute_reply.started":"2024-05-29T16:45:28.617411Z","shell.execute_reply":"2024-05-29T16:45:28.630264Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['config.json', 'preprocessor_config.json', 'model.safetensors']"},"metadata":{}}]},{"cell_type":"code","source":"import os\nos.chdir('MedViT')","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:45:32.873215Z","iopub.execute_input":"2024-05-29T16:45:32.873585Z","iopub.status.idle":"2024-05-29T16:45:32.878115Z","shell.execute_reply.started":"2024-05-29T16:45:32.873553Z","shell.execute_reply":"2024-05-29T16:45:32.877148Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"pip install chardet einops fire fvcore","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:45:33.612552Z","iopub.execute_input":"2024-05-29T16:45:33.612933Z","iopub.status.idle":"2024-05-29T16:45:56.245943Z","shell.execute_reply.started":"2024-05-29T16:45:33.612905Z","shell.execute_reply":"2024-05-29T16:45:56.244845Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting chardet\n  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting fire\n  Downloading fire-0.6.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m864.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting fvcore\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m873.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (6.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore) (4.66.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from fvcore) (9.5.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore) (0.9.0)\nCollecting iopath>=0.1.7 (from fvcore)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m758.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore) (4.9.0)\nCollecting portalocker (from iopath>=0.1.7->fvcore)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m806.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: fire, fvcore, iopath\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=8f6ee26f8b6c58702b377c11839b54966a5d2776580f7d3a95fea94f40424452\n  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=6a097adbc7745dea05c810b7468d6c3080d167675281ffc55c3abc426c98de69\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=728c5068ad7c0f7165cebcf77864a5261a411810677d32cdf3f05b075f9e80c5\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built fire fvcore iopath\nInstalling collected packages: yacs, portalocker, fire, einops, chardet, iopath, fvcore\nSuccessfully installed chardet-5.2.0 einops-0.8.0 fire-0.6.0 fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install evaluate transformers accelerate","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:45:56.248261Z","iopub.execute_input":"2024-05-29T16:45:56.248667Z","iopub.status.idle":"2024-05-29T16:46:09.405050Z","shell.execute_reply.started":"2024-05-29T16:45:56.248608Z","shell.execute_reply":"2024-05-29T16:46:09.403840Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m881.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n\nfrom PIL import Image\n# from torchinfo import summary\nimport torch\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom typing import Tuple\nfrom sklearn.metrics import roc_auc_score\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\n# import torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom datasets import load_dataset\nimport pandas as pd\n\nimport random\n\n# import wandb\n# wandb.login(\"1fa58b4e42c64c2531b3abeb43c04f5991be307e\")\n\ntorch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:50:47.749903Z","iopub.execute_input":"2024-05-29T16:50:47.750283Z","iopub.status.idle":"2024-05-29T16:50:47.865712Z","shell.execute_reply.started":"2024-05-29T16:50:47.750241Z","shell.execute_reply":"2024-05-29T16:50:47.864539Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# 471257f8658cc55c4ec33930066c1c6d1f101821","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:46:17.783729Z","iopub.execute_input":"2024-05-29T16:46:17.784203Z","iopub.status.idle":"2024-05-29T16:46:17.788363Z","shell.execute_reply.started":"2024-05-29T16:46:17.784174Z","shell.execute_reply":"2024-05-29T16:46:17.787304Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print('Number CUDA Devices:', torch.cuda.device_count())\nprint ('Current cuda device: ', torch.cuda.current_device(), ' **May not correspond to nvidia-smi ID above, check visibility parameter')","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:46:17.789831Z","iopub.execute_input":"2024-05-29T16:46:17.790115Z","iopub.status.idle":"2024-05-29T16:46:17.855309Z","shell.execute_reply.started":"2024-05-29T16:46:17.790089Z","shell.execute_reply":"2024-05-29T16:46:17.854457Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Number CUDA Devices: 1\nCurrent cuda device:  0  **May not correspond to nvidia-smi ID above, check visibility parameter\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define configuration\nfrom transformers import PretrainedConfig\nfrom typing import List\n\n\nclass MedViTConfig(PretrainedConfig):\n    model_type = \"medvit\"\n\n    def __init__(\n        self,\n        stem_chs: List[int] = [64, 32, 64],\n        depths: List[int] = [3, 4, 30, 3],\n        path_dropout: float = 0.2,\n        attn_drop: int = 0,\n        drop: int = 0,\n        num_classes: int = 5,\n        strides: List[int] = [1, 2, 2, 2],\n        sr_ratios: List[int] = [8, 4, 2, 1],\n        head_dim: int = 32,\n        mix_block_ratio: float = 0.75,\n        use_checkpoint: bool = False,\n        **kwargs\n    ):\n        self.stem_chs = stem_chs\n        self.depths = depths\n        self.path_dropout = path_dropout\n        self.attn_drop = attn_drop\n        self.drop = drop\n        self.num_classes = num_classes\n        self.strides = strides\n        self.sr_ratios = sr_ratios\n        self.head_dim = head_dim\n        self.mix_block_ratio = mix_block_ratio\n        self.use_checkpoint = use_checkpoint\n        super().__init__(**kwargs)\n\nmedvit_config = MedViTConfig()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:46:50.749566Z","iopub.execute_input":"2024-05-29T16:46:50.750475Z","iopub.status.idle":"2024-05-29T16:46:50.759756Z","shell.execute_reply.started":"2024-05-29T16:46:50.750440Z","shell.execute_reply":"2024-05-29T16:46:50.758777Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Initialise a MedViT class\nfrom transformers import PreTrainedModel\nfrom MedViT import MedViT\nprint(MedViT)\nclass MedViTClassification(PreTrainedModel):\n    config_class = MedViTConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = MedViT(\n        stem_chs = config.stem_chs,\n        depths = config.depths,\n        path_dropout = config.path_dropout,\n        attn_drop = config.attn_drop,\n        drop = config.drop,\n        num_classes = config.num_classes,\n        strides = config.strides,\n        sr_ratios = config.sr_ratios,\n        head_dim = config.head_dim,\n        mix_block_ratio = config.mix_block_ratio,\n        use_checkpoint = config.use_checkpoint,\n        )\n\n    def forward(self, pixel_values, labels=None):\n        logits = self.model(pixel_values)\n        # loss = torch.nn.CrossEntropyLoss(logits, labels)\n        loss = torch.nn.functional.cross_entropy(logits, labels)\n        return {\"loss\": loss, \"logits\": logits}","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:46:51.526147Z","iopub.execute_input":"2024-05-29T16:46:51.527124Z","iopub.status.idle":"2024-05-29T16:46:53.846703Z","shell.execute_reply.started":"2024-05-29T16:46:51.527068Z","shell.execute_reply":"2024-05-29T16:46:53.845680Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"<class 'MedViT.MedViT'>\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize a model\nmodel = MedViTClassification(medvit_config)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:46:53.848260Z","iopub.execute_input":"2024-05-29T16:46:53.848559Z","iopub.status.idle":"2024-05-29T16:46:55.225637Z","shell.execute_reply.started":"2024-05-29T16:46:53.848532Z","shell.execute_reply":"2024-05-29T16:46:55.224486Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"initialize_weights...\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd ../","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:46:55.226809Z","iopub.execute_input":"2024-05-29T16:46:55.227086Z","iopub.status.idle":"2024-05-29T16:46:55.233951Z","shell.execute_reply.started":"2024-05-29T16:46:55.227061Z","shell.execute_reply":"2024-05-29T16:46:55.232814Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"os.listdir(r'../input/labels')","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:46:55.236639Z","iopub.execute_input":"2024-05-29T16:46:55.237773Z","iopub.status.idle":"2024-05-29T16:46:55.246025Z","shell.execute_reply.started":"2024-05-29T16:46:55.237740Z","shell.execute_reply":"2024-05-29T16:46:55.245194Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"['image_labels.csv']"},"metadata":{}}]},{"cell_type":"code","source":"# dataset_folder_name = r\"../input/short-imbalanced-dataset/short_imbalanced_dataset\"\n# dataset_folder_name = r\"../input/dr-dataset\"\n\ndef load_dataset_path2images(dataset_folder_name):\n    train_test_folders = os.listdir(dataset_folder_name)\n    datasets = {}\n    for trts_split in train_test_folders:\n        class_folders = os.listdir(dataset_folder_name+'/'+trts_split)\n#         class_folders = os.listdir(dataset_folder_name + '\\\\' + trts_split)\n        labels = []\n        paths = []\n        for class_folder in class_folders:\n            image_names = os.listdir(dataset_folder_name+'/'+trts_split+'/'+class_folder)\n            image_paths = [dataset_folder_name+'/'+trts_split+'/'+class_folder+'/'+x for x in image_names]\n#             image_names = os.listdir(dataset_folder_name + '\\\\' + trts_split + '\\\\' + class_folder)\n#             image_paths = [dataset_folder_name + '\\\\' + trts_split + '\\\\' + class_folder + '\\\\' + x for x in image_names]\n            class_labels = [int(class_folder)] * len(image_paths)\n            labels.extend(class_labels)\n            paths.extend(image_paths)\n        local_dataset = {'image_path' : paths, 'label' : labels}\n        datasets[trts_split] = pd.DataFrame.from_dict(local_dataset)\n\n    return datasets\n\n# dataset = load_dataset_path2images(dataset_folder_name)\ndataset = pd.read_csv(r'/kaggle/input/labels/image_labels.csv')\ndataset['path'], dataset['ext'] = '/kaggle/input/dr-train/train/', '.jpeg'\ndataset['image_path'] = dataset['path'] + dataset['image'] + dataset['ext']\ndataset.drop(columns = ['image', 'path', 'ext'], inplace = True)\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:46:55.247071Z","iopub.execute_input":"2024-05-29T16:46:55.247436Z","iopub.status.idle":"2024-05-29T16:46:55.290015Z","shell.execute_reply.started":"2024-05-29T16:46:55.247410Z","shell.execute_reply":"2024-05-29T16:46:55.289149Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\n# oversampling just repeating minority class items\n# enought times to be equal to major dataset in size\n\n##############################################################################################\n\n# max_size = train_dataset['label'].value_counts().max()\n# lst = [train_dataset]\ndef resample(_dataset, ratio = 3):\n    min_size = _dataset['label'].value_counts().min()\n    lst = []\n    added_unique_rows = 0\n    all_n_rows = 0\n\n    for class_index, group in _dataset.groupby('label'):\n        # lst.append(group.sample(max_size-len(group), replace=True))\n        all_n_rows += len(group)\n        if class_index == 0:\n            added_unique_rows += min_size*ratio\n            lst.append(group.sample(min_size*ratio, replace=False))\n        else:\n            if len(group) > min_size*ratio:\n                added_unique_rows += min_size*ratio\n                lst.append(group.sample(min_size*ratio, replace=False))\n            else:\n                lst.append(group)\n                added_unique_rows += len(group)\n                lst.append(group.sample(min_size*ratio-len(group), replace=True))\n\n    _dataset = pd.concat(lst)\n\n    for class_index, group in _dataset.groupby('label'):\n        print(f'{class_index}: length: {len(group)}')\n\n    print('N_added_rows: ', added_unique_rows)\n    print('N_all_rows: ', all_n_rows)\n    print('Ratio of used rows: ', added_unique_rows/all_n_rows)\n    return _dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:46:55.291051Z","iopub.execute_input":"2024-05-29T16:46:55.291341Z","iopub.status.idle":"2024-05-29T16:46:55.300797Z","shell.execute_reply.started":"2024-05-29T16:46:55.291315Z","shell.execute_reply":"2024-05-29T16:46:55.299918Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:46:55.302015Z","iopub.execute_input":"2024-05-29T16:46:55.302308Z","iopub.status.idle":"2024-05-29T16:46:55.329675Z","shell.execute_reply.started":"2024-05-29T16:46:55.302282Z","shell.execute_reply":"2024-05-29T16:46:55.328762Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"      label                                     image_path\n0         0   /kaggle/input/dr-train/train/10003_left.jpeg\n1         0  /kaggle/input/dr-train/train/10003_right.jpeg\n2         0   /kaggle/input/dr-train/train/10007_left.jpeg\n3         0  /kaggle/input/dr-train/train/10007_right.jpeg\n4         0   /kaggle/input/dr-train/train/10009_left.jpeg\n...     ...                                            ...\n8403      0  /kaggle/input/dr-train/train/19494_right.jpeg\n8404      0   /kaggle/input/dr-train/train/19498_left.jpeg\n8405      0  /kaggle/input/dr-train/train/19498_right.jpeg\n8406      0     /kaggle/input/dr-train/train/194_left.jpeg\n8407      0    /kaggle/input/dr-train/train/194_right.jpeg\n\n[8408 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/10003_left.jpeg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/10003_right.jpeg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/10007_left.jpeg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/10007_right.jpeg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/10009_left.jpeg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8403</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/19494_right.jpeg</td>\n    </tr>\n    <tr>\n      <th>8404</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/19498_left.jpeg</td>\n    </tr>\n    <tr>\n      <th>8405</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/19498_right.jpeg</td>\n    </tr>\n    <tr>\n      <th>8406</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/194_left.jpeg</td>\n    </tr>\n    <tr>\n      <th>8407</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/194_right.jpeg</td>\n    </tr>\n  </tbody>\n</table>\n<p>8408 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"##############################################################################################\ntrain_dataset = resample(dataset, ratio = 20)\n# test_dataset = resample(dataset['short_test'], ratio = 1)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:46:55.776969Z","iopub.execute_input":"2024-05-29T16:46:55.777328Z","iopub.status.idle":"2024-05-29T16:46:55.803504Z","shell.execute_reply.started":"2024-05-29T16:46:55.777297Z","shell.execute_reply":"2024-05-29T16:46:55.802655Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"0: length: 3320\n1: length: 3320\n2: length: 3320\n3: length: 3320\n4: length: 3320\nN_added_rows:  5578\nN_all_rows:  8408\nRatio of used rows:  0.6634157944814463\n","output_type":"stream"}]},{"cell_type":"code","source":"mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\nfrom datasets import Dataset\nfrom transformers import ViTImageProcessor\nfrom transformers import AutoImageProcessor\n\n# model_name_or_path = 'google/vit-base-patch16-224-in21k'\n# model_name_or_path = \"microsoft/swinv2-tiny-patch4-window8-256\"\nmodel_name_or_path = \"microsoft/swin-base-patch4-window12-384\"\n# model_name_or_path = \"/kaggle/input/medvit-saved/MedViT_saved\"\n\n# processor = ViTImageProcessor.from_pretrained(model_name_or_path)\nimage_processor = AutoImageProcessor.from_pretrained(model_name_or_path)\n# model = MedViTClassification.from_pretrained(model_name_or_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:46:56.706116Z","iopub.execute_input":"2024-05-29T16:46:56.706937Z","iopub.status.idle":"2024-05-29T16:47:07.966709Z","shell.execute_reply.started":"2024-05-29T16:46:56.706905Z","shell.execute_reply":"2024-05-29T16:47:07.965803Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"2024-05-29 16:46:58.808323: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-29 16:46:58.808430: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-29 16:46:58.966578: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cbefc6338be4f998e64ac9ed2e3ca31"}},"metadata":{}},{"name":"stderr","text":"Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n","output_type":"stream"}]},{"cell_type":"code","source":"from typing import Any\nclass Spot(object):\n    def __init__(self, size, prob = 0.5):\n        self.size = size\n        self.prob = prob\n        self.center = None\n        self.radius = None\n        self.zeros = torch.zeros((self.size, self.size)) #.cuda()\n        self.ones = torch.ones((3, 1)) #.cuda()\n        self.tensor_to_image = T.ToPILImage()\n        self.image_to_tensor = T.ToTensor()\n\n    def __call__(self, image_tensors, target = None):\n        if random.random() < self.prob:\n            image_tensors = self.image_to_tensor(image_tensors)\n#             print('Yes')\n#             modified_image_tensors = image_tensors.clone()\n            # print(f'Min value: {torch.amin(modified_image_tensors)}')\n            # print(f'Max value: {torch.amax(modified_image_tensors)}')\n            n_spots = random.randint(5, 7)\n            self.initial_mask = self.zeros.clone()\n\n            self.dim1_offset = (image_tensors.shape[1] - self.size) // 2\n            self.dim2_offset = (image_tensors.shape[2] - self.size) // 2\n\n            for _ in range(n_spots):\n                new_image_tensors = self.add_random_spot(image_tensors)\n#                 modified_image_tensors = self.add_random_spot(modified_image_tensors)\n            return torch.clamp(new_image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n#             return torch.clamp(modified_image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n        else: return image_tensors\n        \n    def add_random_spot(self, image_tensor):\n        self.radius = random.randint(int(0.01 * self.size) + 1, int(0.05 * self.size))\n        self.center = [random.randint(self.radius + 1, self.size - self.radius - 1), \n                       random.randint(self.radius + 1, self.size - self.radius - 1)]\n        y, x = np.ogrid[: self.size, : self.size]\n        dist_from_center = np.sqrt((x - self.center[0]) ** 2 + (y - self.center[1]) ** 2)\n        circle = dist_from_center <= (self.radius // 2)\n\n        k = 14 / 25 + (1.0 - self.radius / 25)\n        beta = 0.5 + (1.5 - 0.5) * self.radius / 25\n        A = k * self.ones.clone()\n        d = 0.3 * self.radius / 25\n        t = np.exp(-beta * d)\n\n        spot_mask = self.zeros.clone()\n        spot_mask[circle] = torch.multiply(A[0], torch.tensor(1 - t))\n\n        self.initial_mask = self.initial_mask + spot_mask\n        self.initial_mask[self.initial_mask != 0] = 1\n\n        sigma = (5 + (2 - 0) * self.radius / 25) * 2\n        rad_w = random.randint(int(sigma / 5), int(sigma / 4))\n        rad_h = random.randint(int(sigma / 5), int(sigma / 4))\n\n        if (rad_w % 2) == 0: rad_w = rad_w + 1\n        if (rad_h % 2) == 0: rad_h = rad_h + 1\n\n        spot_mask = F.gaussian_blur(torch.reshape(spot_mask, (1, self.size, self.size)), (rad_w, rad_h), sigma)\n        spot_mask = torch.stack([spot_mask, spot_mask, spot_mask]) * 255\n        \n        image_tensor[:, self.dim1_offset : self.dim1_offset + self.size, self.dim2_offset : self.dim2_offset + self.size] += torch.reshape(spot_mask, (3, self.size, self.size))\n        return image_tensor\n\nclass Halo(object):\n    def __init__(self, size, prob = 0.5, intensity_range = (0.8, 1.2)):\n        self.size = size\n        self.prob = prob\n        self.center = None\n        self.radius = None\n        self.intensity_range = intensity_range\n        self.tensor_to_image = T.ToPILImage()\n        self.image_to_tensor = T.ToTensor()\n\n    def __call__(self, image_tensors, target = None):\n        if random.random() < self.prob:\n            image_tensors = self.image_to_tensor(image_tensors)\n#             print('Yes')\n#             modified_image_tensors = image_tensors.clone()\n            # print(f'Min value: {torch.amin(modified_image_tensors)}')\n            # print(f'Max value: {torch.amax(modified_image_tensors)}')\n            n_halos = random.randint(5, 7)\n\n            self.dim1_offset = (image_tensors.shape[1] - self.size) // 2\n            self.dim2_offset = (image_tensors.shape[2] - self.size) // 2\n            \n            for _ in range(n_halos):\n                image_tensors = self.add_random_halo(image_tensors)\n#                 modified_image_tensors = self.add_random_halo(modified_image_tensors)\n            return torch.clamp(image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n#             return torch.clamp(modified_image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n        else: return image_tensors\n\n    def add_random_halo(self, image_tensor):\n        self.radius = random.randint(int(0.01 * self.size), int(0.05 * self.size))\n        self.center = [random.randint(self.radius + 1, self.size - self.radius - 1),\n                        random.randint(self.radius + 1, self.size - self.radius - 1)]\n        \n        y, x = torch.meshgrid(torch.arange(self.size), torch.arange(self.size))\n        dist_from_center = torch.sqrt(((x - self.center[0]) ** 2 + (y - self.center[1]) ** 2))\n        normalized_dist = dist_from_center / self.radius\n        \n        halo_intensity = torch.clamp(self.intensity_range[0] + (self.intensity_range[1] - self.intensity_range[0]) * (1 - normalized_dist), min = 0, max = 1)\n        halo_mask = dist_from_center <= self.radius // 2\n        halo_effect = halo_intensity * (self.radius - dist_from_center) / self.radius\n        halo_effect = np.clip(halo_effect, 0, 1)\n        halo_effect = np.expand_dims(halo_effect, axis = 0)\n        halo_effect = np.repeat(halo_effect, image_tensor.shape[0], axis = 0)\n        image_tensor[:, halo_mask] = image_tensor[:, halo_mask] * (1 - halo_effect[:, halo_mask]) + halo_effect[:, halo_mask] * 255\n\n        return image_tensor\n\nclass Hole(object):\n    def __init__(self, size, prob = 0.5):\n        self.size = size\n        self.prob = prob\n        self.center = None\n        self.radius = None\n        self.tensor_to_image = T.ToPILImage()\n        self.image_to_tensor = T.ToTensor()\n\n    def __call__(self, image_tensors, target = None):\n        if random.random() < self.prob:\n            image_tensors = self.image_to_tensor(image_tensors)\n#             print('Yes')\n#             modified_image_tensors = image_tensors.clone()\n            # print(f'Min value: {torch.amin(modified_image_tensors)}')\n            # print(f'Max value: {torch.amax(modified_image_tensors)}')\n            n_halos = random.randint(5, 7)\n\n            self.dim1_offset = (image_tensors.shape[1] - self.size) // 2\n            self.dim2_offset = (image_tensors.shape[2] - self.size) // 2\n            \n            for _ in range(n_halos):\n                image_tensors = self.add_random_hole(image_tensors)\n#                 modified_image_tensors = self.add_random_hole(modified_image_tensors)\n            return torch.clamp(image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n#             return torch.clamp(modified_image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n        else: return image_tensors\n\n    def add_random_hole(self, image_tensor):\n        self.radius = random.randint(int(0.01 * self.size), int(0.05 * self.size))\n        self.center = [random.randint(self.radius + 1, self.size - self.radius - 1),\n                        random.randint(self.radius + 1, self.size - self.radius - 1)]\n        \n        y, x = torch.meshgrid(torch.arange(self.size), torch.arange(self.size))\n        dist_from_center = torch.sqrt(((x - self.center[0]) ** 2 + (y - self.center[1]) ** 2))\n        \n        hole_mask = dist_from_center <= self.radius // 2\n        image_tensor[:, hole_mask] = 0\n\n        return image_tensor","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:47:07.968698Z","iopub.execute_input":"2024-05-29T16:47:07.968981Z","iopub.status.idle":"2024-05-29T16:47:08.009555Z","shell.execute_reply.started":"2024-05-29T16:47:07.968955Z","shell.execute_reply":"2024-05-29T16:47:08.008731Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageEnhance\n\ndef RandomSharpen(image, alpha = 0.2):\n    sharpener = ImageEnhance.Sharpness(image)\n    factor = 0.5  \n    image = sharpener.enhance(1.0 + alpha * factor)\n    return image","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:47:08.010806Z","iopub.execute_input":"2024-05-29T16:47:08.011111Z","iopub.status.idle":"2024-05-29T16:47:08.033211Z","shell.execute_reply.started":"2024-05-29T16:47:08.011059Z","shell.execute_reply":"2024-05-29T16:47:08.032383Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"size = (\n    image_processor.size[\"shortest_edge\"]\n    if \"shortest_edge\" in image_processor.size\n    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n)\n\nprint(size)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:47:08.035251Z","iopub.execute_input":"2024-05-29T16:47:08.035546Z","iopub.status.idle":"2024-05-29T16:47:08.044644Z","shell.execute_reply.started":"2024-05-29T16:47:08.035523Z","shell.execute_reply":"2024-05-29T16:47:08.043660Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"(384, 384)\n","output_type":"stream"}]},{"cell_type":"code","source":"_transforms_train = T.Compose([\n    T.RandomHorizontalFlip(p = 0.5),\n    T.RandomVerticalFlip(p = 0.5),\n    T.RandomCrop(2000, padding_mode='symmetric', pad_if_needed=True),\n#     Spot(size[0]),\n    # Halo(),\n    # Hole(),\n#     T.Lambda(RandomSharpen),\n    # Blur()\n])\n\n_transforms_test = T.Compose([\n    T.CenterCrop(2000),\n])","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:47:08.045813Z","iopub.execute_input":"2024-05-29T16:47:08.046107Z","iopub.status.idle":"2024-05-29T16:47:08.053190Z","shell.execute_reply.started":"2024-05-29T16:47:08.046068Z","shell.execute_reply":"2024-05-29T16:47:08.052358Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def load_image(path_image, label, mode):\n    # load image\n    try:\n        image = Image.open(path_image)\n#         print(image)\n#         image.verify()  # Verify the image is valid\n        if mode == 'train':\n            image = _transforms_train(image)\n\n            return image\n        else:\n            return image\n    except (IOError, SyntaxError) as e:\n        \n        image = Image.open('/kaggle/input/dr-train/train/10003_left.jpeg')\n#         image.verify()  # Verify the image is valid\n        if mode == 'train':\n            image = _transforms_train(image)\n\n            return image\n        else:\n            return image\n\n\ndef func_transform(examples):\n\n    # loaded_images = [load_image(path, lb, 'train').convert(\"RGB\") for path, lb in zip(examples['image_path'], examples['label'])]\n    # _transforms(img.convert(\"RGB\"))\n    inputs = image_processor([load_image(path, lb, 'train')\n                                for path, lb in zip(examples['image_path'], examples['label'])], return_tensors='pt')\n    inputs['label'] = examples['label']\n    return inputs\n\n    ###############################\n\n    # examples[\"pixel_values\"] = [load_image(path, lb, 'train')\n    #                             for path, lb in zip(examples['image_path'], examples['label'])]\n    # del examples[\"image_path\"]\n    # return examples\n\n\n\ndef func_transform_test(examples):\n\n    # loaded_images = [load_image(path, lb, 'test').convert(\"RGB\") for path, lb in zip(examples['image_path'], examples['label'])]\n    inputs = image_processor([load_image(path, lb, 'test')\n                                for path, lb in zip(examples['image_path'], examples['label'])], return_tensors='pt')\n    inputs['label'] = examples['label']\n    return inputs\n\n    ########################################\n    # examples[\"pixel_values\"] = [load_image(path, lb, 'test')\n    #                             for path, lb in zip(examples['image_path'], examples['label'])]\n    # del examples[\"image_path\"]\n    # return examples","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:47:08.054508Z","iopub.execute_input":"2024-05-29T16:47:08.054857Z","iopub.status.idle":"2024-05-29T16:47:08.067694Z","shell.execute_reply.started":"2024-05-29T16:47:08.054826Z","shell.execute_reply":"2024-05-29T16:47:08.066774Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_ds = Dataset.from_pandas(train_dataset, preserve_index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:47:08.068838Z","iopub.execute_input":"2024-05-29T16:47:08.069166Z","iopub.status.idle":"2024-05-29T16:47:08.112986Z","shell.execute_reply.started":"2024-05-29T16:47:08.069140Z","shell.execute_reply":"2024-05-29T16:47:08.112294Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_test_dataset = train_ds.train_test_split(test_size = 0.20, seed = 42)\ntrain_dataset, test_dataset = train_test_dataset['train'], train_test_dataset['test']\n# train_dataset = Dataset.from_pandas(train_dataset, preserve_index=False)\n# test_dataset = Dataset.from_pandas(test_dataset, preserve_index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:47:08.113980Z","iopub.execute_input":"2024-05-29T16:47:08.114244Z","iopub.status.idle":"2024-05-29T16:47:08.138047Z","shell.execute_reply.started":"2024-05-29T16:47:08.114221Z","shell.execute_reply":"2024-05-29T16:47:08.137396Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepared_train_dataset = train_dataset.with_transform(func_transform)\nprepared_test_dataset = test_dataset.with_transform(func_transform_test)\nprepared_train_dataset = prepared_train_dataset.shuffle(seed = 42)\nprepared_test_dataset = prepared_test_dataset.shuffle(seed = 42)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:47:08.139043Z","iopub.execute_input":"2024-05-29T16:47:08.139299Z","iopub.status.idle":"2024-05-29T16:47:08.184457Z","shell.execute_reply.started":"2024-05-29T16:47:08.139276Z","shell.execute_reply":"2024-05-29T16:47:08.183600Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"rows in train_dataset: \", len(prepared_train_dataset))\n# print(\"rows in test_dataset: \", len(prepared_test_dataset))\n\n# labels = prepared_ds_train.features[\"label\"].names()\nlabels = [0, 1, 2, 3, 4]\nlabel2id, id2label = dict(), dict()\n\nfor i, label in enumerate(labels):\n    label2id[label] = i\n    id2label[i] = label\n\nprint(\"ID2label: \", id2label)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:47:08.187640Z","iopub.execute_input":"2024-05-29T16:47:08.187964Z","iopub.status.idle":"2024-05-29T16:47:08.194093Z","shell.execute_reply.started":"2024-05-29T16:47:08.187940Z","shell.execute_reply":"2024-05-29T16:47:08.193156Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"rows in train_dataset:  13280\nID2label:  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n","output_type":"stream"}]},{"cell_type":"code","source":"def collate_fn(batch):\n    return {\n        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n        # 'tensor': torch.stack([x['tensor'] for x in batch]),\n        'labels': torch.tensor([x['label'] for x in batch])\n    }\n\ndef calculate_per_class_accuracy(confusion_matrix):\n        num_classes = confusion_matrix.shape[0]\n        per_class_accuracy = []\n\n        for i in range(num_classes):\n            TP = confusion_matrix[i, i]\n            FN = np.sum(confusion_matrix[i, :]) - TP\n            FP = np.sum(confusion_matrix[:, i]) - TP\n            TN = np.sum(confusion_matrix) - (TP + FP + FN)\n\n            accuracy = (TP + TN) / (TP + TN + FP + FN)\n            per_class_accuracy.append(accuracy)\n\n        return per_class_accuracy","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:47:08.195337Z","iopub.execute_input":"2024-05-29T16:47:08.195676Z","iopub.status.idle":"2024-05-29T16:47:08.203946Z","shell.execute_reply.started":"2024-05-29T16:47:08.195646Z","shell.execute_reply":"2024-05-29T16:47:08.203148Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score, confusion_matrix\nfrom sklearn.metrics import f1_score #, kappa\n# from sklearn import metrics\n\nimport evaluate\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions_proba, labels = eval_pred\n\n    if predictions_proba.shape[1] > 1:  # Check if we have more than one class\n        predictions_proba = torch.nn.functional.softmax(torch.tensor(predictions_proba), dim=-1).numpy()\n    \n    # print(predictions)\n    predictions = np.argmax(predictions_proba, axis=1)\n    # print(predictions)\n    # print(labels)\n    result_accuracy = accuracy.compute(predictions=predictions, references=labels)\n    \n    cm = confusion_matrix(labels, predictions)\n    print(cm)\n    perclass_acc = calculate_per_class_accuracy(cm)\n    \n#     print(f'per class accuracies: {perclass_acc}')\n    \n    \n    result = {\n             'accuracy': np.mean([result_accuracy['accuracy']]),\n             'kappa': np.mean([cohen_kappa_score(labels, predictions, weights = \"quadratic\")]),\n             # 'quadratic_kappa': np.mean([kappa(labels, predictions, weights = \"quadratic\")]),\n             'f1': np.mean([f1_score(labels, predictions, average='weighted')]),\n             'roc_auc': np.mean([roc_auc_score(labels, predictions_proba, multi_class='ovr')]),\n             'class_0' : perclass_acc[0],\n             'class_1' : perclass_acc[1],\n             'class_2' : perclass_acc[2],\n             'class_3' : perclass_acc[3],\n             'class_4' : perclass_acc[4],\n             }\n\n    \n    \n#     print(f\"\\nClass 0 Accuracy (vs Others): {class_0:.2f}%\")\n#     print(f\"Other Classes Accuracy: {remaining_classes:.2f}%\")\n    \n    # print(cohen_kappa_score(labels, predictions))\n    # print(result)\n\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:57:07.432749Z","iopub.execute_input":"2024-05-29T16:57:07.433976Z","iopub.status.idle":"2024-05-29T16:57:07.813700Z","shell.execute_reply.started":"2024-05-29T16:57:07.433932Z","shell.execute_reply":"2024-05-29T16:57:07.812624Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./MedViT-base\",\n    evaluation_strategy=\"steps\",\n    logging_steps=20,\n\n    save_steps=20,\n    eval_steps=20,\n    save_total_limit=2,\n\n    # report_to=\"wandb\",  # enable logging to W&B\n    # run_name=\"swin384_shrp_rt20\",  # name of the W&B run (optional)\n\n    remove_unused_columns=False,\n    dataloader_num_workers = 2,\n\n    learning_rate=2e-5,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=2,\n    warmup_ratio=0.1,\n\n    metric_for_best_model=\"kappa\",\n    greater_is_better = True,\n    load_best_model_at_end=True,\n\n    push_to_hub=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:57:10.281105Z","iopub.execute_input":"2024-05-29T16:57:10.281487Z","iopub.status.idle":"2024-05-29T16:57:10.325377Z","shell.execute_reply.started":"2024-05-29T16:57:10.281458Z","shell.execute_reply":"2024-05-29T16:57:10.324320Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"sample_ids = np.random.choice(len(prepared_test_dataset), size=250, replace=False)\ninv_sample_ids = np.setdiff1d(np.arange(len(prepared_test_dataset)), sample_ids)\nval_ds = prepared_test_dataset.select(sample_ids)\ntest_ds = prepared_test_dataset.select(inv_sample_ids)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:57:10.554172Z","iopub.execute_input":"2024-05-29T16:57:10.554461Z","iopub.status.idle":"2024-05-29T16:57:10.583572Z","shell.execute_reply.started":"2024-05-29T16:57:10.554434Z","shell.execute_reply":"2024-05-29T16:57:10.582510Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"val_ds, test_ds","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:57:10.934860Z","iopub.execute_input":"2024-05-29T16:57:10.935570Z","iopub.status.idle":"2024-05-29T16:57:10.943203Z","shell.execute_reply.started":"2024-05-29T16:57:10.935542Z","shell.execute_reply":"2024-05-29T16:57:10.942123Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['label', 'image_path'],\n     num_rows: 250\n }),\n Dataset({\n     features: ['label', 'image_path'],\n     num_rows: 3070\n }))"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n    train_dataset=prepared_train_dataset,\n#     eval_dataset=prepared_test_dataset,\n    eval_dataset=val_ds,\n    tokenizer=image_processor,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:57:11.318653Z","iopub.execute_input":"2024-05-29T16:57:11.318956Z","iopub.status.idle":"2024-05-29T16:57:11.349388Z","shell.execute_reply.started":"2024-05-29T16:57:11.318932Z","shell.execute_reply":"2024-05-29T16:57:11.348471Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:57:11.725083Z","iopub.execute_input":"2024-05-29T16:57:11.725388Z","iopub.status.idle":"2024-05-29T16:57:11.836029Z","shell.execute_reply.started":"2024-05-29T16:57:11.725360Z","shell.execute_reply":"2024-05-29T16:57:11.834874Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# CUDA_LAUNCH_BLOCKING=1\ntrain_results = trainer.train()\ntrainer.save_model()\ntrainer.log_metrics(\"train\", train_results.metrics)\ntrainer.save_metrics(\"train\", train_results.metrics)\ntrainer.save_state()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:57:12.112366Z","iopub.execute_input":"2024-05-29T16:57:12.112668Z","iopub.status.idle":"2024-05-29T16:58:07.130232Z","shell.execute_reply.started":"2024-05-29T16:57:12.112643Z","shell.execute_reply":"2024-05-29T16:58:07.128600Z"},"trusted":true},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='27' max='6640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  27/6640 00:51 < 3:48:54, 0.48 it/s, Epoch 0.01/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Kappa</th>\n      <th>F1</th>\n      <th>Roc Auc</th>\n      <th>Class 0</th>\n      <th>Class 1</th>\n      <th>Class 2</th>\n      <th>Class 3</th>\n      <th>Class 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.601500</td>\n      <td>1.811502</td>\n      <td>0.148000</td>\n      <td>0.003003</td>\n      <td>0.112020</td>\n      <td>0.454035</td>\n      <td>0.788000</td>\n      <td>0.464000</td>\n      <td>0.584000</td>\n      <td>0.692000</td>\n      <td>0.768000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"[[ 0 29 14  4  5]\n [ 0 20 15  7  4]\n [ 1 27 10  5  3]\n [ 0 34 19  3  2]\n [ 0 18 20  6  4]]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# CUDA_LAUNCH_BLOCKING=1\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[1;32m      4\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlog_metrics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_results\u001b[38;5;241m.\u001b[39mmetrics)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# 471257f8658cc55c4ec33930066c1c6d1f101821","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:43:44.360228Z","iopub.execute_input":"2024-05-26T13:43:44.360898Z","iopub.status.idle":"2024-05-26T13:43:44.366222Z","shell.execute_reply.started":"2024-05-26T13:43:44.360862Z","shell.execute_reply":"2024-05-26T13:43:44.365064Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"metrics = trainer.evaluate(test_ds)\ntrainer.log_metrics(\"eval\", metrics)\ntrainer.save_metrics(\"eval\", metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}