{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8522001,"sourceType":"datasetVersion","datasetId":5088496},{"sourceId":8524305,"sourceType":"datasetVersion","datasetId":5090145},{"sourceId":8524979,"sourceType":"datasetVersion","datasetId":5090620},{"sourceId":8525303,"sourceType":"datasetVersion","datasetId":5090862}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install MedVit\n!git clone https://github.com/Omid-Nejati/MedViT.git\n# %cd /MedVit","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-26T22:43:41.115609Z","iopub.execute_input":"2024-05-26T22:43:41.115980Z","iopub.status.idle":"2024-05-26T22:43:42.673178Z","shell.execute_reply.started":"2024-05-26T22:43:41.115950Z","shell.execute_reply":"2024-05-26T22:43:42.671970Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Cloning into 'MedViT'...\nremote: Enumerating objects: 130, done.\u001b[K\nremote: Counting objects: 100% (129/129), done.\u001b[K\nremote: Compressing objects: 100% (64/64), done.\u001b[K\nremote: Total 130 (delta 58), reused 128 (delta 57), pack-reused 1\u001b[K\nReceiving objects: 100% (130/130), 800.27 KiB | 22.86 MiB/s, done.\nResolving deltas: 100% (58/58), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%ls","metadata":{"execution":{"iopub.status.busy":"2024-05-26T22:43:43.746708Z","iopub.execute_input":"2024-05-26T22:43:43.747083Z","iopub.status.idle":"2024-05-26T22:43:44.709582Z","shell.execute_reply.started":"2024-05-26T22:43:43.747049Z","shell.execute_reply":"2024-05-26T22:43:44.708557Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"\u001b[0m\u001b[01;34mMedViT\u001b[0m/\n","output_type":"stream"}]},{"cell_type":"code","source":"os.listdir('/kaggle/input/medvit-saved/MedViT_saved')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T22:43:45.686018Z","iopub.execute_input":"2024-05-26T22:43:45.686390Z","iopub.status.idle":"2024-05-26T22:43:45.696545Z","shell.execute_reply.started":"2024-05-26T22:43:45.686357Z","shell.execute_reply":"2024-05-26T22:43:45.695627Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"['config.json', 'preprocessor_config.json', 'model.safetensors']"},"metadata":{}}]},{"cell_type":"code","source":"import os\nos.chdir('MedViT')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T22:43:48.120169Z","iopub.execute_input":"2024-05-26T22:43:48.120518Z","iopub.status.idle":"2024-05-26T22:43:48.124924Z","shell.execute_reply.started":"2024-05-26T22:43:48.120492Z","shell.execute_reply":"2024-05-26T22:43:48.123879Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"pip install chardet einops fire fvcore","metadata":{"execution":{"iopub.status.busy":"2024-05-26T22:43:50.753072Z","iopub.execute_input":"2024-05-26T22:43:50.753738Z","iopub.status.idle":"2024-05-26T22:44:12.134528Z","shell.execute_reply.started":"2024-05-26T22:43:50.753706Z","shell.execute_reply":"2024-05-26T22:44:12.133401Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Collecting chardet\n  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting fire\n  Downloading fire-0.6.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting fvcore\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (6.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore) (4.66.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from fvcore) (9.5.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore) (0.9.0)\nCollecting iopath>=0.1.7 (from fvcore)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore) (4.9.0)\nCollecting portalocker (from iopath>=0.1.7->fvcore)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: fire, fvcore, iopath\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=8664b901e3fd94889772d43d7b74e7b4c7ce5dd05a39b9d0373db8923b9a1398\n  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=a9691a1da61549381a6f596c1542e6fc76d97e618d7424c2be82a6b06f55d8ce\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=1ede1493246c40c5a166492276216e41e12db8fb0492486cfab8c567736655ea\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built fire fvcore iopath\nInstalling collected packages: yacs, portalocker, fire, einops, chardet, iopath, fvcore\nSuccessfully installed chardet-5.2.0 einops-0.8.0 fire-0.6.0 fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install evaluate transformers accelerate","metadata":{"execution":{"iopub.status.busy":"2024-05-26T22:44:12.136414Z","iopub.execute_input":"2024-05-26T22:44:12.136724Z","iopub.status.idle":"2024-05-26T22:44:24.880466Z","shell.execute_reply.started":"2024-05-26T22:44:12.136693Z","shell.execute_reply":"2024-05-26T22:44:24.879169Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n\nfrom PIL import Image\n# from torchinfo import summary\nimport torch\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom typing import Tuple\n\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\n# import torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom datasets import load_dataset\nimport pandas as pd\n\nimport random\n\n# import wandb\n# wandb.login(\"1fa58b4e42c64c2531b3abeb43c04f5991be307e\")\n\ntorch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T22:44:28.489253Z","iopub.execute_input":"2024-05-26T22:44:28.489665Z","iopub.status.idle":"2024-05-26T22:44:28.501755Z","shell.execute_reply.started":"2024-05-26T22:44:28.489634Z","shell.execute_reply":"2024-05-26T22:44:28.500755Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# 471257f8658cc55c4ec33930066c1c6d1f101821","metadata":{"execution":{"iopub.status.busy":"2024-05-26T22:44:29.208284Z","iopub.execute_input":"2024-05-26T22:44:29.209173Z","iopub.status.idle":"2024-05-26T22:44:29.212944Z","shell.execute_reply.started":"2024-05-26T22:44:29.209137Z","shell.execute_reply":"2024-05-26T22:44:29.211948Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"print('Number CUDA Devices:', torch.cuda.device_count())\nprint ('Current cuda device: ', torch.cuda.current_device(), ' **May not correspond to nvidia-smi ID above, check visibility parameter')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T22:44:29.605759Z","iopub.execute_input":"2024-05-26T22:44:29.606144Z","iopub.status.idle":"2024-05-26T22:44:29.611238Z","shell.execute_reply.started":"2024-05-26T22:44:29.606114Z","shell.execute_reply":"2024-05-26T22:44:29.610351Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Number CUDA Devices: 1\nCurrent cuda device:  0  **May not correspond to nvidia-smi ID above, check visibility parameter\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define configuration\nfrom transformers import PretrainedConfig\nfrom typing import List\n\n\nclass MedViTConfig(PretrainedConfig):\n    model_type = \"medvit\"\n\n    def __init__(\n        self,\n        stem_chs: List[int] = [64, 32, 64],\n        depths: List[int] = [3, 4, 30, 3],\n        path_dropout: float = 0.2,\n        attn_drop: int = 0,\n        drop: int = 0,\n        num_classes: int = 5,\n        strides: List[int] = [1, 2, 2, 2],\n        sr_ratios: List[int] = [8, 4, 2, 1],\n        head_dim: int = 32,\n        mix_block_ratio: float = 0.75,\n        use_checkpoint: bool = False,\n        **kwargs\n    ):\n        self.stem_chs = stem_chs\n        self.depths = depths\n        self.path_dropout = path_dropout\n        self.attn_drop = attn_drop\n        self.drop = drop\n        self.num_classes = num_classes\n        self.strides = strides\n        self.sr_ratios = sr_ratios\n        self.head_dim = head_dim\n        self.mix_block_ratio = mix_block_ratio\n        self.use_checkpoint = use_checkpoint\n        super().__init__(**kwargs)\n\nmedvit_config = MedViTConfig()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T22:44:30.757036Z","iopub.execute_input":"2024-05-26T22:44:30.757734Z","iopub.status.idle":"2024-05-26T22:44:30.767084Z","shell.execute_reply.started":"2024-05-26T22:44:30.757701Z","shell.execute_reply":"2024-05-26T22:44:30.766209Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Initialise a MedViT class\nfrom transformers import PreTrainedModel\nfrom MedViT import MedViT\nprint(MedViT)\nclass MedViTClassification(PreTrainedModel):\n    config_class = MedViTConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = MedViT(\n        stem_chs = config.stem_chs,\n        depths = config.depths,\n        path_dropout = config.path_dropout,\n        attn_drop = config.attn_drop,\n        drop = config.drop,\n        num_classes = config.num_classes,\n        strides = config.strides,\n        sr_ratios = config.sr_ratios,\n        head_dim = config.head_dim,\n        mix_block_ratio = config.mix_block_ratio,\n        use_checkpoint = config.use_checkpoint,\n        )\n\n    def forward(self, pixel_values, labels=None):\n        logits = self.model(pixel_values)\n        # loss = torch.nn.CrossEntropyLoss(logits, labels)\n        loss = torch.nn.functional.cross_entropy(logits, labels)\n        return {\"loss\": loss, \"logits\": logits}","metadata":{"execution":{"iopub.status.busy":"2024-05-26T22:44:33.283388Z","iopub.execute_input":"2024-05-26T22:44:33.283756Z","iopub.status.idle":"2024-05-26T22:44:34.696368Z","shell.execute_reply.started":"2024-05-26T22:44:33.283725Z","shell.execute_reply":"2024-05-26T22:44:34.695416Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"<class 'MedViT.MedViT'>\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize a model\nmodel = MedViTClassification(medvit_config)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:26:31.266587Z","iopub.execute_input":"2024-05-26T23:26:31.267487Z","iopub.status.idle":"2024-05-26T23:26:32.474366Z","shell.execute_reply.started":"2024-05-26T23:26:31.267451Z","shell.execute_reply":"2024-05-26T23:26:32.472896Z"},"trusted":true},"execution_count":118,"outputs":[{"name":"stdout","text":"initialize_weights...\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd ../","metadata":{"execution":{"iopub.status.busy":"2024-05-26T22:44:34.929794Z","iopub.execute_input":"2024-05-26T22:44:34.930664Z","iopub.status.idle":"2024-05-26T22:44:34.936568Z","shell.execute_reply.started":"2024-05-26T22:44:34.930633Z","shell.execute_reply":"2024-05-26T22:44:34.935677Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"os.listdir(r'../input/labels')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T22:44:38.672183Z","iopub.execute_input":"2024-05-26T22:44:38.672544Z","iopub.status.idle":"2024-05-26T22:44:38.679456Z","shell.execute_reply.started":"2024-05-26T22:44:38.672515Z","shell.execute_reply":"2024-05-26T22:44:38.678639Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"['image_labels.csv']"},"metadata":{}}]},{"cell_type":"code","source":"# dataset_folder_name = r\"../input/short-imbalanced-dataset/short_imbalanced_dataset\"\n# dataset_folder_name = r\"../input/dr-dataset\"\n\ndef load_dataset_path2images(dataset_folder_name):\n    train_test_folders = os.listdir(dataset_folder_name)\n    datasets = {}\n    for trts_split in train_test_folders:\n        class_folders = os.listdir(dataset_folder_name+'/'+trts_split)\n#         class_folders = os.listdir(dataset_folder_name + '\\\\' + trts_split)\n        labels = []\n        paths = []\n        for class_folder in class_folders:\n            image_names = os.listdir(dataset_folder_name+'/'+trts_split+'/'+class_folder)\n            image_paths = [dataset_folder_name+'/'+trts_split+'/'+class_folder+'/'+x for x in image_names]\n#             image_names = os.listdir(dataset_folder_name + '\\\\' + trts_split + '\\\\' + class_folder)\n#             image_paths = [dataset_folder_name + '\\\\' + trts_split + '\\\\' + class_folder + '\\\\' + x for x in image_names]\n            class_labels = [int(class_folder)] * len(image_paths)\n            labels.extend(class_labels)\n            paths.extend(image_paths)\n        local_dataset = {'image_path' : paths, 'label' : labels}\n        datasets[trts_split] = pd.DataFrame.from_dict(local_dataset)\n\n    return datasets\n\n# dataset = load_dataset_path2images(dataset_folder_name)\ndataset = pd.read_csv(r'/kaggle/input/labels/image_labels.csv')\ndataset['path'], dataset['ext'] = '/kaggle/input/dr-train/train/', '.jpeg'\ndataset['image_path'] = dataset['path'] + dataset['image'] + dataset['ext']\ndataset.drop(columns = ['image', 'path', 'ext'], inplace = True)\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:26:50.478068Z","iopub.execute_input":"2024-05-26T23:26:50.478847Z","iopub.status.idle":"2024-05-26T23:26:50.514750Z","shell.execute_reply.started":"2024-05-26T23:26:50.478790Z","shell.execute_reply":"2024-05-26T23:26:50.513860Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"\n# oversampling just repeating minority class items\n# enought times to be equal to major dataset in size\n\n##############################################################################################\n\n# max_size = train_dataset['label'].value_counts().max()\n# lst = [train_dataset]\ndef resample(_dataset, ratio = 3):\n    min_size = _dataset['label'].value_counts().min()\n    lst = []\n    added_unique_rows = 0\n    all_n_rows = 0\n\n    for class_index, group in _dataset.groupby('label'):\n        # lst.append(group.sample(max_size-len(group), replace=True))\n        all_n_rows += len(group)\n        if class_index == 0:\n            added_unique_rows += min_size*ratio\n            lst.append(group.sample(min_size*ratio, replace=False))\n        else:\n            if len(group) > min_size*ratio:\n                added_unique_rows += min_size*ratio\n                lst.append(group.sample(min_size*ratio, replace=False))\n            else:\n                lst.append(group)\n                added_unique_rows += len(group)\n                lst.append(group.sample(min_size*ratio-len(group), replace=True))\n\n    _dataset = pd.concat(lst)\n\n    for class_index, group in _dataset.groupby('label'):\n        print(f'{class_index}: length: {len(group)}')\n\n    print('N_added_rows: ', added_unique_rows)\n    print('N_all_rows: ', all_n_rows)\n    print('Ratio of used rows: ', added_unique_rows/all_n_rows)\n    return _dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:26:51.095880Z","iopub.execute_input":"2024-05-26T23:26:51.096744Z","iopub.status.idle":"2024-05-26T23:26:51.107461Z","shell.execute_reply.started":"2024-05-26T23:26:51.096707Z","shell.execute_reply":"2024-05-26T23:26:51.106223Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:26:51.848275Z","iopub.execute_input":"2024-05-26T23:26:51.848939Z","iopub.status.idle":"2024-05-26T23:26:51.865496Z","shell.execute_reply.started":"2024-05-26T23:26:51.848905Z","shell.execute_reply":"2024-05-26T23:26:51.864123Z"},"trusted":true},"execution_count":121,"outputs":[{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"      label                                     image_path\n0         0   /kaggle/input/dr-train/train/10003_left.jpeg\n1         0  /kaggle/input/dr-train/train/10003_right.jpeg\n2         0   /kaggle/input/dr-train/train/10007_left.jpeg\n3         0  /kaggle/input/dr-train/train/10007_right.jpeg\n4         0   /kaggle/input/dr-train/train/10009_left.jpeg\n...     ...                                            ...\n8403      0  /kaggle/input/dr-train/train/19494_right.jpeg\n8404      0   /kaggle/input/dr-train/train/19498_left.jpeg\n8405      0  /kaggle/input/dr-train/train/19498_right.jpeg\n8406      0     /kaggle/input/dr-train/train/194_left.jpeg\n8407      0    /kaggle/input/dr-train/train/194_right.jpeg\n\n[8408 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/10003_left.jpeg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/10003_right.jpeg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/10007_left.jpeg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/10007_right.jpeg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/10009_left.jpeg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8403</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/19494_right.jpeg</td>\n    </tr>\n    <tr>\n      <th>8404</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/19498_left.jpeg</td>\n    </tr>\n    <tr>\n      <th>8405</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/19498_right.jpeg</td>\n    </tr>\n    <tr>\n      <th>8406</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/194_left.jpeg</td>\n    </tr>\n    <tr>\n      <th>8407</th>\n      <td>0</td>\n      <td>/kaggle/input/dr-train/train/194_right.jpeg</td>\n    </tr>\n  </tbody>\n</table>\n<p>8408 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"##############################################################################################\ntrain_dataset = resample(dataset, ratio = 20)\n# test_dataset = resample(dataset['short_test'], ratio = 1)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:26:53.761484Z","iopub.execute_input":"2024-05-26T23:26:53.761851Z","iopub.status.idle":"2024-05-26T23:26:53.782343Z","shell.execute_reply.started":"2024-05-26T23:26:53.761820Z","shell.execute_reply":"2024-05-26T23:26:53.781174Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stdout","text":"0: length: 3320\n1: length: 3320\n2: length: 3320\n3: length: 3320\n4: length: 3320\nN_added_rows:  5578\nN_all_rows:  8408\nRatio of used rows:  0.6634157944814463\n","output_type":"stream"}]},{"cell_type":"code","source":"mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\nfrom datasets import Dataset\nfrom transformers import ViTImageProcessor\nfrom transformers import AutoImageProcessor\n\n# model_name_or_path = 'google/vit-base-patch16-224-in21k'\n# model_name_or_path = \"microsoft/swinv2-tiny-patch4-window8-256\"\nmodel_name_or_path = \"microsoft/swin-base-patch4-window12-384\"\n# model_name_or_path = \"/kaggle/input/medvit-saved/MedViT_saved\"\n\n# processor = ViTImageProcessor.from_pretrained(model_name_or_path)\nimage_processor = AutoImageProcessor.from_pretrained(model_name_or_path)\n# model = MedViTClassification.from_pretrained(model_name_or_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:26:54.531469Z","iopub.execute_input":"2024-05-26T23:26:54.532370Z","iopub.status.idle":"2024-05-26T23:26:54.832205Z","shell.execute_reply.started":"2024-05-26T23:26:54.532332Z","shell.execute_reply":"2024-05-26T23:26:54.831267Z"},"trusted":true},"execution_count":123,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6efb994ae25d4777a725597192632220"}},"metadata":{}},{"name":"stderr","text":"Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n","output_type":"stream"}]},{"cell_type":"code","source":"from typing import Any\nclass Spot(object):\n    def __init__(self, size, prob = 0.5):\n        self.size = size\n        self.prob = prob\n        self.center = None\n        self.radius = None\n        self.zeros = torch.zeros((self.size, self.size)) #.cuda()\n        self.ones = torch.ones((3, 1)) #.cuda()\n        self.tensor_to_image = T.ToPILImage()\n        self.image_to_tensor = T.ToTensor()\n\n    def __call__(self, image_tensors, target = None):\n        if random.random() < self.prob:\n            image_tensors = self.image_to_tensor(image_tensors)\n#             print('Yes')\n#             modified_image_tensors = image_tensors.clone()\n            # print(f'Min value: {torch.amin(modified_image_tensors)}')\n            # print(f'Max value: {torch.amax(modified_image_tensors)}')\n            n_spots = random.randint(5, 7)\n            self.initial_mask = self.zeros.clone()\n\n            self.dim1_offset = (image_tensors.shape[1] - self.size) // 2\n            self.dim2_offset = (image_tensors.shape[2] - self.size) // 2\n\n            for _ in range(n_spots):\n                new_image_tensors = self.add_random_spot(image_tensors)\n#                 modified_image_tensors = self.add_random_spot(modified_image_tensors)\n            return torch.clamp(new_image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n#             return torch.clamp(modified_image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n        else: return image_tensors\n        \n    def add_random_spot(self, image_tensor):\n        self.radius = random.randint(int(0.01 * self.size) + 1, int(0.05 * self.size))\n        self.center = [random.randint(self.radius + 1, self.size - self.radius - 1), \n                       random.randint(self.radius + 1, self.size - self.radius - 1)]\n        y, x = np.ogrid[: self.size, : self.size]\n        dist_from_center = np.sqrt((x - self.center[0]) ** 2 + (y - self.center[1]) ** 2)\n        circle = dist_from_center <= (self.radius // 2)\n\n        k = 14 / 25 + (1.0 - self.radius / 25)\n        beta = 0.5 + (1.5 - 0.5) * self.radius / 25\n        A = k * self.ones.clone()\n        d = 0.3 * self.radius / 25\n        t = np.exp(-beta * d)\n\n        spot_mask = self.zeros.clone()\n        spot_mask[circle] = torch.multiply(A[0], torch.tensor(1 - t))\n\n        self.initial_mask = self.initial_mask + spot_mask\n        self.initial_mask[self.initial_mask != 0] = 1\n\n        sigma = (5 + (2 - 0) * self.radius / 25) * 2\n        rad_w = random.randint(int(sigma / 5), int(sigma / 4))\n        rad_h = random.randint(int(sigma / 5), int(sigma / 4))\n\n        if (rad_w % 2) == 0: rad_w = rad_w + 1\n        if (rad_h % 2) == 0: rad_h = rad_h + 1\n\n        spot_mask = F.gaussian_blur(torch.reshape(spot_mask, (1, self.size, self.size)), (rad_w, rad_h), sigma)\n        spot_mask = torch.stack([spot_mask, spot_mask, spot_mask]) * 255\n        \n        image_tensor[:, self.dim1_offset : self.dim1_offset + self.size, self.dim2_offset : self.dim2_offset + self.size] += torch.reshape(spot_mask, (3, self.size, self.size))\n        return image_tensor\n\nclass Halo(object):\n    def __init__(self, size, prob = 0.5, intensity_range = (0.8, 1.2)):\n        self.size = size\n        self.prob = prob\n        self.center = None\n        self.radius = None\n        self.intensity_range = intensity_range\n        self.tensor_to_image = T.ToPILImage()\n        self.image_to_tensor = T.ToTensor()\n\n    def __call__(self, image_tensors, target = None):\n        if random.random() < self.prob:\n            image_tensors = self.image_to_tensor(image_tensors)\n#             print('Yes')\n#             modified_image_tensors = image_tensors.clone()\n            # print(f'Min value: {torch.amin(modified_image_tensors)}')\n            # print(f'Max value: {torch.amax(modified_image_tensors)}')\n            n_halos = random.randint(5, 7)\n\n            self.dim1_offset = (image_tensors.shape[1] - self.size) // 2\n            self.dim2_offset = (image_tensors.shape[2] - self.size) // 2\n            \n            for _ in range(n_halos):\n                image_tensors = self.add_random_halo(image_tensors)\n#                 modified_image_tensors = self.add_random_halo(modified_image_tensors)\n            return torch.clamp(image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n#             return torch.clamp(modified_image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n        else: return image_tensors\n\n    def add_random_halo(self, image_tensor):\n        self.radius = random.randint(int(0.01 * self.size), int(0.05 * self.size))\n        self.center = [random.randint(self.radius + 1, self.size - self.radius - 1),\n                        random.randint(self.radius + 1, self.size - self.radius - 1)]\n        \n        y, x = torch.meshgrid(torch.arange(self.size), torch.arange(self.size))\n        dist_from_center = torch.sqrt(((x - self.center[0]) ** 2 + (y - self.center[1]) ** 2))\n        normalized_dist = dist_from_center / self.radius\n        \n        halo_intensity = torch.clamp(self.intensity_range[0] + (self.intensity_range[1] - self.intensity_range[0]) * (1 - normalized_dist), min = 0, max = 1)\n        halo_mask = dist_from_center <= self.radius // 2\n        halo_effect = halo_intensity * (self.radius - dist_from_center) / self.radius\n        halo_effect = np.clip(halo_effect, 0, 1)\n        halo_effect = np.expand_dims(halo_effect, axis = 0)\n        halo_effect = np.repeat(halo_effect, image_tensor.shape[0], axis = 0)\n        image_tensor[:, halo_mask] = image_tensor[:, halo_mask] * (1 - halo_effect[:, halo_mask]) + halo_effect[:, halo_mask] * 255\n\n        return image_tensor\n\nclass Hole(object):\n    def __init__(self, size, prob = 0.5):\n        self.size = size\n        self.prob = prob\n        self.center = None\n        self.radius = None\n        self.tensor_to_image = T.ToPILImage()\n        self.image_to_tensor = T.ToTensor()\n\n    def __call__(self, image_tensors, target = None):\n        if random.random() < self.prob:\n            image_tensors = self.image_to_tensor(image_tensors)\n#             print('Yes')\n#             modified_image_tensors = image_tensors.clone()\n            # print(f'Min value: {torch.amin(modified_image_tensors)}')\n            # print(f'Max value: {torch.amax(modified_image_tensors)}')\n            n_halos = random.randint(5, 7)\n\n            self.dim1_offset = (image_tensors.shape[1] - self.size) // 2\n            self.dim2_offset = (image_tensors.shape[2] - self.size) // 2\n            \n            for _ in range(n_halos):\n                image_tensors = self.add_random_hole(image_tensors)\n#                 modified_image_tensors = self.add_random_hole(modified_image_tensors)\n            return torch.clamp(image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n#             return torch.clamp(modified_image_tensors, min = torch.amin(image_tensors), max = torch.amax(image_tensors))\n        else: return image_tensors\n\n    def add_random_hole(self, image_tensor):\n        self.radius = random.randint(int(0.01 * self.size), int(0.05 * self.size))\n        self.center = [random.randint(self.radius + 1, self.size - self.radius - 1),\n                        random.randint(self.radius + 1, self.size - self.radius - 1)]\n        \n        y, x = torch.meshgrid(torch.arange(self.size), torch.arange(self.size))\n        dist_from_center = torch.sqrt(((x - self.center[0]) ** 2 + (y - self.center[1]) ** 2))\n        \n        hole_mask = dist_from_center <= self.radius // 2\n        image_tensor[:, hole_mask] = 0\n\n        return image_tensor","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:03.080727Z","iopub.execute_input":"2024-05-26T23:27:03.081133Z","iopub.status.idle":"2024-05-26T23:27:03.120138Z","shell.execute_reply.started":"2024-05-26T23:27:03.081101Z","shell.execute_reply":"2024-05-26T23:27:03.119171Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageEnhance\n\ndef RandomSharpen(image, alpha = 0.2):\n    sharpener = ImageEnhance.Sharpness(image)\n    factor = 0.5  \n    image = sharpener.enhance(1.0 + alpha * factor)\n    return image","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:04.002078Z","iopub.execute_input":"2024-05-26T23:27:04.002432Z","iopub.status.idle":"2024-05-26T23:27:04.009772Z","shell.execute_reply.started":"2024-05-26T23:27:04.002404Z","shell.execute_reply":"2024-05-26T23:27:04.008761Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"size = (\n    image_processor.size[\"shortest_edge\"]\n    if \"shortest_edge\" in image_processor.size\n    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n)\n\nprint(size)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:04.451221Z","iopub.execute_input":"2024-05-26T23:27:04.451564Z","iopub.status.idle":"2024-05-26T23:27:04.459769Z","shell.execute_reply.started":"2024-05-26T23:27:04.451538Z","shell.execute_reply":"2024-05-26T23:27:04.458578Z"},"trusted":true},"execution_count":126,"outputs":[{"name":"stdout","text":"(384, 384)\n","output_type":"stream"}]},{"cell_type":"code","source":"_transforms_train = T.Compose([\n    T.RandomHorizontalFlip(p = 0.5),\n    T.RandomVerticalFlip(p = 0.5),\n    T.RandomCrop(2000, padding_mode='symmetric', pad_if_needed=True),\n#     Spot(size[0]),\n    # Halo(),\n    # Hole(),\n#     T.Lambda(RandomSharpen),\n    # Blur()\n])\n\n_transforms_test = T.Compose([\n    T.CenterCrop(2000),\n])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:05.014652Z","iopub.execute_input":"2024-05-26T23:27:05.015416Z","iopub.status.idle":"2024-05-26T23:27:05.022305Z","shell.execute_reply.started":"2024-05-26T23:27:05.015378Z","shell.execute_reply":"2024-05-26T23:27:05.021200Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"def load_image(path_image, label, mode):\n    # load image\n    try:\n        image = Image.open(path_image)\n#         print(image)\n#         image.verify()  # Verify the image is valid\n        if mode == 'train':\n            image = _transforms_train(image)\n\n            return image\n        else:\n            return image\n    except (IOError, SyntaxError) as e:\n        \n        image = Image.open('/kaggle/input/dr-train/train/10003_left.jpeg')\n#         image.verify()  # Verify the image is valid\n        if mode == 'train':\n            image = _transforms_train(image)\n\n            return image\n        else:\n            return image\n\n\ndef func_transform(examples):\n\n    # loaded_images = [load_image(path, lb, 'train').convert(\"RGB\") for path, lb in zip(examples['image_path'], examples['label'])]\n    # _transforms(img.convert(\"RGB\"))\n    inputs = image_processor([load_image(path, lb, 'train')\n                                for path, lb in zip(examples['image_path'], examples['label'])], return_tensors='pt')\n    inputs['label'] = examples['label']\n    return inputs\n\n    ###############################\n\n    # examples[\"pixel_values\"] = [load_image(path, lb, 'train')\n    #                             for path, lb in zip(examples['image_path'], examples['label'])]\n    # del examples[\"image_path\"]\n    # return examples\n\n\n\ndef func_transform_test(examples):\n\n    # loaded_images = [load_image(path, lb, 'test').convert(\"RGB\") for path, lb in zip(examples['image_path'], examples['label'])]\n    inputs = image_processor([load_image(path, lb, 'test')\n                                for path, lb in zip(examples['image_path'], examples['label'])], return_tensors='pt')\n    inputs['label'] = examples['label']\n    return inputs\n\n    ########################################\n    # examples[\"pixel_values\"] = [load_image(path, lb, 'test')\n    #                             for path, lb in zip(examples['image_path'], examples['label'])]\n    # del examples[\"image_path\"]\n    # return examples","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:05.652917Z","iopub.execute_input":"2024-05-26T23:27:05.653672Z","iopub.status.idle":"2024-05-26T23:27:05.665787Z","shell.execute_reply.started":"2024-05-26T23:27:05.653638Z","shell.execute_reply":"2024-05-26T23:27:05.663739Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"train_ds = Dataset.from_pandas(train_dataset, preserve_index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:06.186101Z","iopub.execute_input":"2024-05-26T23:27:06.186739Z","iopub.status.idle":"2024-05-26T23:27:06.207614Z","shell.execute_reply.started":"2024-05-26T23:27:06.186709Z","shell.execute_reply":"2024-05-26T23:27:06.206499Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"train_test_dataset = train_ds.train_test_split(test_size = 0.20, seed = 42)\ntrain_dataset, test_dataset = train_test_dataset['train'], train_test_dataset['test']\n# train_dataset = Dataset.from_pandas(train_dataset, preserve_index=False)\n# test_dataset = Dataset.from_pandas(test_dataset, preserve_index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:06.779689Z","iopub.execute_input":"2024-05-26T23:27:06.780063Z","iopub.status.idle":"2024-05-26T23:27:06.806873Z","shell.execute_reply.started":"2024-05-26T23:27:06.780032Z","shell.execute_reply":"2024-05-26T23:27:06.805889Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepared_train_dataset = train_dataset.with_transform(func_transform)\nprepared_test_dataset = test_dataset.with_transform(func_transform_test)\nprepared_train_dataset = prepared_train_dataset.shuffle(seed = 42)\nprepared_test_dataset = prepared_test_dataset.shuffle(seed = 42)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:08.047673Z","iopub.execute_input":"2024-05-26T23:27:08.048420Z","iopub.status.idle":"2024-05-26T23:27:08.092282Z","shell.execute_reply.started":"2024-05-26T23:27:08.048387Z","shell.execute_reply":"2024-05-26T23:27:08.091321Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"rows in train_dataset: \", len(prepared_train_dataset))\n# print(\"rows in test_dataset: \", len(prepared_test_dataset))\n\n# labels = prepared_ds_train.features[\"label\"].names()\nlabels = [0, 1, 2, 3, 4]\nlabel2id, id2label = dict(), dict()\n\nfor i, label in enumerate(labels):\n    label2id[label] = i\n    id2label[i] = label\n\nprint(\"ID2label: \", id2label)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:09.000345Z","iopub.execute_input":"2024-05-26T23:27:09.000719Z","iopub.status.idle":"2024-05-26T23:27:09.009381Z","shell.execute_reply.started":"2024-05-26T23:27:09.000687Z","shell.execute_reply":"2024-05-26T23:27:09.008306Z"},"trusted":true},"execution_count":132,"outputs":[{"name":"stdout","text":"rows in train_dataset:  13280\nID2label:  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n","output_type":"stream"}]},{"cell_type":"code","source":"def collate_fn(batch):\n    return {\n        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n        # 'tensor': torch.stack([x['tensor'] for x in batch]),\n        'labels': torch.tensor([x['label'] for x in batch])\n    }\n\ndef calculate_per_class_accuracy(confusion_matrix):\n        num_classes = confusion_matrix.shape[0]\n        per_class_accuracy = []\n\n        for i in range(num_classes):\n            TP = confusion_matrix[i, i]\n            FN = np.sum(confusion_matrix[i, :]) - TP\n            FP = np.sum(confusion_matrix[:, i]) - TP\n            TN = np.sum(confusion_matrix) - (TP + FP + FN)\n\n            accuracy = (TP + TN) / (TP + TN + FP + FN)\n            per_class_accuracy.append(accuracy)\n\n        return per_class_accuracy","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:10.772742Z","iopub.execute_input":"2024-05-26T23:27:10.773134Z","iopub.status.idle":"2024-05-26T23:27:10.782081Z","shell.execute_reply.started":"2024-05-26T23:27:10.773104Z","shell.execute_reply":"2024-05-26T23:27:10.781063Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score, confusion_matrix\nfrom sklearn.metrics import f1_score #, kappa\n# from sklearn import metrics\n\nimport evaluate\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions_proba, labels = eval_pred\n\n    # print(predictions)\n    predictions = np.argmax(predictions_proba, axis=1)\n    # print(predictions)\n    # print(labels)\n    result_accuracy = accuracy.compute(predictions=predictions, references=labels)\n    \n    cm = confusion_matrix(labels, predictions)\n    print(cm)\n    perclass_acc = calculate_per_class_accuracy(cm)\n    \n#     print(f'per class accuracies: {perclass_acc}')\n    \n    \n    result = {\n             'accuracy': np.mean([result_accuracy['accuracy']]),\n             'kappa': np.mean([cohen_kappa_score(labels, predictions, weights = \"quadratic\")]),\n             # 'quadratic_kappa': np.mean([kappa(labels, predictions, weights = \"quadratic\")]),\n             'f1': np.mean([f1_score(labels, predictions, average='weighted')]),\n             'roc_auc': np.mean([roc_auc_score(labels, predictions_proba, multi_class='ovr')])\n             'class_0' : perclass_acc[0],\n             'class_1' : perclass_acc[1],\n             'class_2' : perclass_acc[2],\n             'class_3' : perclass_acc[3],\n             'class_4' : perclass_acc[4],\n             }\n\n    \n    \n#     print(f\"\\nClass 0 Accuracy (vs Others): {class_0:.2f}%\")\n#     print(f\"Other Classes Accuracy: {remaining_classes:.2f}%\")\n    \n    # print(cohen_kappa_score(labels, predictions))\n    # print(result)\n\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:11.402447Z","iopub.execute_input":"2024-05-26T23:27:11.403312Z","iopub.status.idle":"2024-05-26T23:27:11.950025Z","shell.execute_reply.started":"2024-05-26T23:27:11.403277Z","shell.execute_reply":"2024-05-26T23:27:11.948894Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./MedViT-base\",\n    evaluation_strategy=\"steps\",\n    logging_steps=20,\n\n    save_steps=20,\n    eval_steps=20,\n    save_total_limit=2,\n\n    # report_to=\"wandb\",  # enable logging to W&B\n    # run_name=\"swin384_shrp_rt20\",  # name of the W&B run (optional)\n\n    remove_unused_columns=False,\n    dataloader_num_workers = 2,\n\n    learning_rate=2e-5,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=2,\n    warmup_ratio=0.1,\n\n    metric_for_best_model=\"kappa\",\n    greater_is_better = True,\n    load_best_model_at_end=True,\n\n    push_to_hub=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:12.046667Z","iopub.execute_input":"2024-05-26T23:27:12.047051Z","iopub.status.idle":"2024-05-26T23:27:12.092024Z","shell.execute_reply.started":"2024-05-26T23:27:12.047018Z","shell.execute_reply":"2024-05-26T23:27:12.091016Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"sample_ids = np.random.choice(len(prepared_test_dataset), size=250, replace=False)\ninv_sample_ids = np.setdiff1d(np.arange(len(prepared_test_dataset)), sample_ids)\nval_ds = prepared_test_dataset.select(sample_ids)\ntest_ds = prepared_test_dataset.select(inv_sample_ids)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:12.651457Z","iopub.execute_input":"2024-05-26T23:27:12.651830Z","iopub.status.idle":"2024-05-26T23:27:12.680034Z","shell.execute_reply.started":"2024-05-26T23:27:12.651788Z","shell.execute_reply":"2024-05-26T23:27:12.678884Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"val_ds, test_ds","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:13.256801Z","iopub.execute_input":"2024-05-26T23:27:13.257658Z","iopub.status.idle":"2024-05-26T23:27:13.266835Z","shell.execute_reply.started":"2024-05-26T23:27:13.257624Z","shell.execute_reply":"2024-05-26T23:27:13.265783Z"},"trusted":true},"execution_count":137,"outputs":[{"execution_count":137,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['label', 'image_path'],\n     num_rows: 250\n }),\n Dataset({\n     features: ['label', 'image_path'],\n     num_rows: 3070\n }))"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n    train_dataset=prepared_train_dataset,\n#     eval_dataset=prepared_test_dataset,\n    eval_dataset=val_ds,\n    tokenizer=image_processor,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:15.122596Z","iopub.execute_input":"2024-05-26T23:27:15.122991Z","iopub.status.idle":"2024-05-26T23:27:15.267575Z","shell.execute_reply.started":"2024-05-26T23:27:15.122958Z","shell.execute_reply":"2024-05-26T23:27:15.266480Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:16.066604Z","iopub.execute_input":"2024-05-26T23:27:16.066958Z","iopub.status.idle":"2024-05-26T23:27:16.080181Z","shell.execute_reply.started":"2024-05-26T23:27:16.066930Z","shell.execute_reply":"2024-05-26T23:27:16.079159Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"# CUDA_LAUNCH_BLOCKING=1\ntrain_results = trainer.train()\ntrainer.save_model()\ntrainer.log_metrics(\"train\", train_results.metrics)\ntrainer.save_metrics(\"train\", train_results.metrics)\ntrainer.save_state()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T23:27:19.204305Z","iopub.execute_input":"2024-05-26T23:27:19.204650Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Could not estimate the number of tokens of the input, floating-point operations will not be computed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3321' max='6640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3321/6640 2:11:40 < 2:11:40, 0.42 it/s, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Kappa</th>\n      <th>F1</th>\n      <th>Class 0</th>\n      <th>Class 1</th>\n      <th>Class 2</th>\n      <th>Class 3</th>\n      <th>Class 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.616900</td>\n      <td>1.633851</td>\n      <td>0.232000</td>\n      <td>-0.003865</td>\n      <td>0.094010</td>\n      <td>0.760000</td>\n      <td>0.784000</td>\n      <td>0.832000</td>\n      <td>0.856000</td>\n      <td>0.232000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.634600</td>\n      <td>1.674526</td>\n      <td>0.208000</td>\n      <td>0.006979</td>\n      <td>0.114595</td>\n      <td>0.756000</td>\n      <td>0.700000</td>\n      <td>0.828000</td>\n      <td>0.808000</td>\n      <td>0.324000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.620700</td>\n      <td>1.770850</td>\n      <td>0.160000</td>\n      <td>-0.088584</td>\n      <td>0.133811</td>\n      <td>0.736000</td>\n      <td>0.656000</td>\n      <td>0.500000</td>\n      <td>0.788000</td>\n      <td>0.640000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.620000</td>\n      <td>1.747423</td>\n      <td>0.220000</td>\n      <td>0.037148</td>\n      <td>0.211334</td>\n      <td>0.724000</td>\n      <td>0.612000</td>\n      <td>0.716000</td>\n      <td>0.832000</td>\n      <td>0.556000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.612400</td>\n      <td>1.841839</td>\n      <td>0.224000</td>\n      <td>-0.026912</td>\n      <td>0.196162</td>\n      <td>0.728000</td>\n      <td>0.596000</td>\n      <td>0.800000</td>\n      <td>0.792000</td>\n      <td>0.532000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.622300</td>\n      <td>1.752283</td>\n      <td>0.232000</td>\n      <td>0.097714</td>\n      <td>0.185397</td>\n      <td>0.760000</td>\n      <td>0.684000</td>\n      <td>0.672000</td>\n      <td>0.844000</td>\n      <td>0.504000</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.585800</td>\n      <td>1.721466</td>\n      <td>0.236000</td>\n      <td>0.084240</td>\n      <td>0.219405</td>\n      <td>0.680000</td>\n      <td>0.624000</td>\n      <td>0.788000</td>\n      <td>0.820000</td>\n      <td>0.560000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.573400</td>\n      <td>1.792762</td>\n      <td>0.272000</td>\n      <td>0.115527</td>\n      <td>0.258863</td>\n      <td>0.624000</td>\n      <td>0.696000</td>\n      <td>0.792000</td>\n      <td>0.788000</td>\n      <td>0.644000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.587100</td>\n      <td>1.713181</td>\n      <td>0.244000</td>\n      <td>0.032871</td>\n      <td>0.209878</td>\n      <td>0.588000</td>\n      <td>0.736000</td>\n      <td>0.812000</td>\n      <td>0.776000</td>\n      <td>0.576000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.644200</td>\n      <td>1.778968</td>\n      <td>0.212000</td>\n      <td>0.042720</td>\n      <td>0.206590</td>\n      <td>0.632000</td>\n      <td>0.660000</td>\n      <td>0.760000</td>\n      <td>0.752000</td>\n      <td>0.620000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.590600</td>\n      <td>1.854939</td>\n      <td>0.228000</td>\n      <td>0.107313</td>\n      <td>0.197928</td>\n      <td>0.692000</td>\n      <td>0.636000</td>\n      <td>0.776000</td>\n      <td>0.804000</td>\n      <td>0.548000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.642700</td>\n      <td>1.889563</td>\n      <td>0.248000</td>\n      <td>0.069200</td>\n      <td>0.192675</td>\n      <td>0.736000</td>\n      <td>0.640000</td>\n      <td>0.812000</td>\n      <td>0.840000</td>\n      <td>0.468000</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.605300</td>\n      <td>1.874272</td>\n      <td>0.212000</td>\n      <td>0.093407</td>\n      <td>0.185300</td>\n      <td>0.724000</td>\n      <td>0.708000</td>\n      <td>0.600000</td>\n      <td>0.824000</td>\n      <td>0.568000</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.619300</td>\n      <td>1.847931</td>\n      <td>0.232000</td>\n      <td>0.043139</td>\n      <td>0.224811</td>\n      <td>0.664000</td>\n      <td>0.640000</td>\n      <td>0.756000</td>\n      <td>0.756000</td>\n      <td>0.648000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.639600</td>\n      <td>1.770594</td>\n      <td>0.216000</td>\n      <td>0.173766</td>\n      <td>0.203694</td>\n      <td>0.628000</td>\n      <td>0.724000</td>\n      <td>0.792000</td>\n      <td>0.640000</td>\n      <td>0.648000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.678600</td>\n      <td>1.860850</td>\n      <td>0.236000</td>\n      <td>0.104621</td>\n      <td>0.218260</td>\n      <td>0.632000</td>\n      <td>0.752000</td>\n      <td>0.748000</td>\n      <td>0.756000</td>\n      <td>0.584000</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.698000</td>\n      <td>2.023042</td>\n      <td>0.232000</td>\n      <td>-0.002655</td>\n      <td>0.126715</td>\n      <td>0.728000</td>\n      <td>0.756000</td>\n      <td>0.820000</td>\n      <td>0.856000</td>\n      <td>0.304000</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.601500</td>\n      <td>1.859859</td>\n      <td>0.212000</td>\n      <td>-0.002674</td>\n      <td>0.187231</td>\n      <td>0.672000</td>\n      <td>0.776000</td>\n      <td>0.620000</td>\n      <td>0.856000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.685600</td>\n      <td>1.810220</td>\n      <td>0.220000</td>\n      <td>0.088094</td>\n      <td>0.180705</td>\n      <td>0.656000</td>\n      <td>0.576000</td>\n      <td>0.796000</td>\n      <td>0.848000</td>\n      <td>0.564000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.624900</td>\n      <td>1.745866</td>\n      <td>0.248000</td>\n      <td>0.071454</td>\n      <td>0.232437</td>\n      <td>0.628000</td>\n      <td>0.776000</td>\n      <td>0.716000</td>\n      <td>0.824000</td>\n      <td>0.552000</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.619400</td>\n      <td>1.810758</td>\n      <td>0.224000</td>\n      <td>0.037930</td>\n      <td>0.156856</td>\n      <td>0.692000</td>\n      <td>0.780000</td>\n      <td>0.676000</td>\n      <td>0.848000</td>\n      <td>0.452000</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.644400</td>\n      <td>2.146525</td>\n      <td>0.248000</td>\n      <td>0.148221</td>\n      <td>0.202571</td>\n      <td>0.532000</td>\n      <td>0.724000</td>\n      <td>0.764000</td>\n      <td>0.852000</td>\n      <td>0.624000</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.636600</td>\n      <td>1.833987</td>\n      <td>0.284000</td>\n      <td>0.044952</td>\n      <td>0.283048</td>\n      <td>0.620000</td>\n      <td>0.772000</td>\n      <td>0.684000</td>\n      <td>0.756000</td>\n      <td>0.736000</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.625700</td>\n      <td>1.779777</td>\n      <td>0.196000</td>\n      <td>0.093199</td>\n      <td>0.187102</td>\n      <td>0.724000</td>\n      <td>0.688000</td>\n      <td>0.680000</td>\n      <td>0.556000</td>\n      <td>0.744000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.604600</td>\n      <td>2.092754</td>\n      <td>0.224000</td>\n      <td>0.046926</td>\n      <td>0.141413</td>\n      <td>0.360000</td>\n      <td>0.756000</td>\n      <td>0.820000</td>\n      <td>0.812000</td>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.602300</td>\n      <td>2.456888</td>\n      <td>0.212000</td>\n      <td>-0.027356</td>\n      <td>0.139420</td>\n      <td>0.380000</td>\n      <td>0.664000</td>\n      <td>0.832000</td>\n      <td>0.828000</td>\n      <td>0.720000</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.608100</td>\n      <td>2.601006</td>\n      <td>0.244000</td>\n      <td>0.035331</td>\n      <td>0.195553</td>\n      <td>0.616000</td>\n      <td>0.456000</td>\n      <td>0.804000</td>\n      <td>0.848000</td>\n      <td>0.764000</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>1.671900</td>\n      <td>2.480656</td>\n      <td>0.216000</td>\n      <td>-0.069044</td>\n      <td>0.196827</td>\n      <td>0.636000</td>\n      <td>0.536000</td>\n      <td>0.724000</td>\n      <td>0.812000</td>\n      <td>0.724000</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>1.628600</td>\n      <td>2.370089</td>\n      <td>0.164000</td>\n      <td>-0.009604</td>\n      <td>0.143620</td>\n      <td>0.720000</td>\n      <td>0.672000</td>\n      <td>0.788000</td>\n      <td>0.484000</td>\n      <td>0.664000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.684100</td>\n      <td>2.144201</td>\n      <td>0.196000</td>\n      <td>0.004460</td>\n      <td>0.167579</td>\n      <td>0.728000</td>\n      <td>0.676000</td>\n      <td>0.792000</td>\n      <td>0.568000</td>\n      <td>0.628000</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>1.701600</td>\n      <td>2.118936</td>\n      <td>0.228000</td>\n      <td>0.043414</td>\n      <td>0.190244</td>\n      <td>0.540000</td>\n      <td>0.772000</td>\n      <td>0.692000</td>\n      <td>0.728000</td>\n      <td>0.724000</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>1.617900</td>\n      <td>2.114429</td>\n      <td>0.200000</td>\n      <td>-0.090176</td>\n      <td>0.167825</td>\n      <td>0.632000</td>\n      <td>0.512000</td>\n      <td>0.832000</td>\n      <td>0.780000</td>\n      <td>0.644000</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>1.650800</td>\n      <td>1.952175</td>\n      <td>0.216000</td>\n      <td>0.013997</td>\n      <td>0.169630</td>\n      <td>0.712000</td>\n      <td>0.428000</td>\n      <td>0.828000</td>\n      <td>0.800000</td>\n      <td>0.664000</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>1.674300</td>\n      <td>1.954887</td>\n      <td>0.200000</td>\n      <td>0.083189</td>\n      <td>0.210157</td>\n      <td>0.740000</td>\n      <td>0.740000</td>\n      <td>0.672000</td>\n      <td>0.620000</td>\n      <td>0.628000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.635000</td>\n      <td>2.048541</td>\n      <td>0.216000</td>\n      <td>0.000333</td>\n      <td>0.198793</td>\n      <td>0.648000</td>\n      <td>0.764000</td>\n      <td>0.712000</td>\n      <td>0.772000</td>\n      <td>0.536000</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>1.622500</td>\n      <td>1.885753</td>\n      <td>0.244000</td>\n      <td>0.037488</td>\n      <td>0.196707</td>\n      <td>0.748000</td>\n      <td>0.608000</td>\n      <td>0.800000</td>\n      <td>0.716000</td>\n      <td>0.616000</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>1.694700</td>\n      <td>2.138000</td>\n      <td>0.212000</td>\n      <td>0.086668</td>\n      <td>0.146514</td>\n      <td>0.760000</td>\n      <td>0.384000</td>\n      <td>0.732000</td>\n      <td>0.836000</td>\n      <td>0.712000</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>1.615100</td>\n      <td>2.047534</td>\n      <td>0.132000</td>\n      <td>-0.167453</td>\n      <td>0.114249</td>\n      <td>0.712000</td>\n      <td>0.532000</td>\n      <td>0.576000</td>\n      <td>0.816000</td>\n      <td>0.628000</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>1.636200</td>\n      <td>1.999733</td>\n      <td>0.248000</td>\n      <td>0.158213</td>\n      <td>0.215370</td>\n      <td>0.684000</td>\n      <td>0.688000</td>\n      <td>0.704000</td>\n      <td>0.788000</td>\n      <td>0.632000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.659200</td>\n      <td>2.259361</td>\n      <td>0.184000</td>\n      <td>-0.015999</td>\n      <td>0.141926</td>\n      <td>0.716000</td>\n      <td>0.656000</td>\n      <td>0.756000</td>\n      <td>0.472000</td>\n      <td>0.768000</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>1.645800</td>\n      <td>1.966663</td>\n      <td>0.224000</td>\n      <td>-0.030945</td>\n      <td>0.179897</td>\n      <td>0.700000</td>\n      <td>0.672000</td>\n      <td>0.800000</td>\n      <td>0.552000</td>\n      <td>0.724000</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>1.613200</td>\n      <td>2.461216</td>\n      <td>0.168000</td>\n      <td>-0.147745</td>\n      <td>0.144229</td>\n      <td>0.688000</td>\n      <td>0.604000</td>\n      <td>0.700000</td>\n      <td>0.608000</td>\n      <td>0.736000</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>1.642300</td>\n      <td>2.190793</td>\n      <td>0.164000</td>\n      <td>-0.046105</td>\n      <td>0.139382</td>\n      <td>0.736000</td>\n      <td>0.696000</td>\n      <td>0.760000</td>\n      <td>0.492000</td>\n      <td>0.644000</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>1.602400</td>\n      <td>2.175841</td>\n      <td>0.180000</td>\n      <td>-0.006585</td>\n      <td>0.145750</td>\n      <td>0.716000</td>\n      <td>0.580000</td>\n      <td>0.728000</td>\n      <td>0.628000</td>\n      <td>0.708000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.572800</td>\n      <td>2.073306</td>\n      <td>0.220000</td>\n      <td>0.119520</td>\n      <td>0.197732</td>\n      <td>0.708000</td>\n      <td>0.684000</td>\n      <td>0.724000</td>\n      <td>0.724000</td>\n      <td>0.600000</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>1.656700</td>\n      <td>2.323637</td>\n      <td>0.176000</td>\n      <td>0.048226</td>\n      <td>0.130467</td>\n      <td>0.716000</td>\n      <td>0.736000</td>\n      <td>0.492000</td>\n      <td>0.844000</td>\n      <td>0.564000</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>1.679000</td>\n      <td>1.757404</td>\n      <td>0.212000</td>\n      <td>0.096271</td>\n      <td>0.209169</td>\n      <td>0.752000</td>\n      <td>0.692000</td>\n      <td>0.592000</td>\n      <td>0.728000</td>\n      <td>0.660000</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>1.606700</td>\n      <td>1.706314</td>\n      <td>0.220000</td>\n      <td>0.048461</td>\n      <td>0.189448</td>\n      <td>0.716000</td>\n      <td>0.652000</td>\n      <td>0.828000</td>\n      <td>0.648000</td>\n      <td>0.596000</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>1.654900</td>\n      <td>1.720150</td>\n      <td>0.176000</td>\n      <td>0.003001</td>\n      <td>0.167943</td>\n      <td>0.664000</td>\n      <td>0.616000</td>\n      <td>0.712000</td>\n      <td>0.696000</td>\n      <td>0.664000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.602600</td>\n      <td>1.739980</td>\n      <td>0.252000</td>\n      <td>0.168022</td>\n      <td>0.214286</td>\n      <td>0.732000</td>\n      <td>0.692000</td>\n      <td>0.820000</td>\n      <td>0.568000</td>\n      <td>0.692000</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>1.574700</td>\n      <td>2.100099</td>\n      <td>0.184000</td>\n      <td>0.011740</td>\n      <td>0.140745</td>\n      <td>0.756000</td>\n      <td>0.556000</td>\n      <td>0.800000</td>\n      <td>0.528000</td>\n      <td>0.728000</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>1.630300</td>\n      <td>2.172486</td>\n      <td>0.216000</td>\n      <td>0.032927</td>\n      <td>0.148577</td>\n      <td>0.748000</td>\n      <td>0.392000</td>\n      <td>0.800000</td>\n      <td>0.788000</td>\n      <td>0.704000</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>1.598100</td>\n      <td>2.438382</td>\n      <td>0.220000</td>\n      <td>0.089753</td>\n      <td>0.206721</td>\n      <td>0.696000</td>\n      <td>0.596000</td>\n      <td>0.660000</td>\n      <td>0.816000</td>\n      <td>0.672000</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>1.624700</td>\n      <td>2.064543</td>\n      <td>0.228000</td>\n      <td>0.007668</td>\n      <td>0.204804</td>\n      <td>0.672000</td>\n      <td>0.708000</td>\n      <td>0.816000</td>\n      <td>0.656000</td>\n      <td>0.604000</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.595700</td>\n      <td>2.118161</td>\n      <td>0.216000</td>\n      <td>0.017541</td>\n      <td>0.214600</td>\n      <td>0.600000</td>\n      <td>0.604000</td>\n      <td>0.760000</td>\n      <td>0.756000</td>\n      <td>0.712000</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>1.601000</td>\n      <td>1.894464</td>\n      <td>0.228000</td>\n      <td>-0.006131</td>\n      <td>0.212657</td>\n      <td>0.724000</td>\n      <td>0.660000</td>\n      <td>0.796000</td>\n      <td>0.784000</td>\n      <td>0.492000</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>1.633700</td>\n      <td>1.983597</td>\n      <td>0.240000</td>\n      <td>-0.015351</td>\n      <td>0.209998</td>\n      <td>0.692000</td>\n      <td>0.660000</td>\n      <td>0.764000</td>\n      <td>0.848000</td>\n      <td>0.516000</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>1.668800</td>\n      <td>1.742610</td>\n      <td>0.236000</td>\n      <td>0.031073</td>\n      <td>0.185727</td>\n      <td>0.740000</td>\n      <td>0.588000</td>\n      <td>0.776000</td>\n      <td>0.816000</td>\n      <td>0.552000</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>1.620800</td>\n      <td>2.063380</td>\n      <td>0.228000</td>\n      <td>0.057178</td>\n      <td>0.212439</td>\n      <td>0.648000</td>\n      <td>0.596000</td>\n      <td>0.808000</td>\n      <td>0.804000</td>\n      <td>0.600000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.613500</td>\n      <td>1.905166</td>\n      <td>0.228000</td>\n      <td>0.070064</td>\n      <td>0.161422</td>\n      <td>0.452000</td>\n      <td>0.612000</td>\n      <td>0.824000</td>\n      <td>0.844000</td>\n      <td>0.724000</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>1.554400</td>\n      <td>1.841401</td>\n      <td>0.228000</td>\n      <td>0.070473</td>\n      <td>0.190773</td>\n      <td>0.568000</td>\n      <td>0.736000</td>\n      <td>0.776000</td>\n      <td>0.796000</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>1.685900</td>\n      <td>1.927419</td>\n      <td>0.208000</td>\n      <td>0.108826</td>\n      <td>0.171482</td>\n      <td>0.504000</td>\n      <td>0.688000</td>\n      <td>0.628000</td>\n      <td>0.836000</td>\n      <td>0.760000</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>1.601600</td>\n      <td>1.968540</td>\n      <td>0.220000</td>\n      <td>0.095516</td>\n      <td>0.167932</td>\n      <td>0.560000</td>\n      <td>0.700000</td>\n      <td>0.600000</td>\n      <td>0.848000</td>\n      <td>0.732000</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>1.604100</td>\n      <td>1.940202</td>\n      <td>0.176000</td>\n      <td>-0.052530</td>\n      <td>0.137523</td>\n      <td>0.476000</td>\n      <td>0.600000</td>\n      <td>0.796000</td>\n      <td>0.848000</td>\n      <td>0.632000</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.674500</td>\n      <td>2.090995</td>\n      <td>0.152000</td>\n      <td>-0.061471</td>\n      <td>0.131714</td>\n      <td>0.720000</td>\n      <td>0.640000</td>\n      <td>0.496000</td>\n      <td>0.776000</td>\n      <td>0.672000</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>1.629700</td>\n      <td>1.937944</td>\n      <td>0.212000</td>\n      <td>0.041034</td>\n      <td>0.207545</td>\n      <td>0.700000</td>\n      <td>0.644000</td>\n      <td>0.648000</td>\n      <td>0.820000</td>\n      <td>0.612000</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>1.627000</td>\n      <td>1.668798</td>\n      <td>0.224000</td>\n      <td>0.048282</td>\n      <td>0.197154</td>\n      <td>0.688000</td>\n      <td>0.700000</td>\n      <td>0.832000</td>\n      <td>0.504000</td>\n      <td>0.724000</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>1.636400</td>\n      <td>1.790847</td>\n      <td>0.212000</td>\n      <td>0.010259</td>\n      <td>0.208788</td>\n      <td>0.688000</td>\n      <td>0.736000</td>\n      <td>0.592000</td>\n      <td>0.704000</td>\n      <td>0.704000</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>1.608400</td>\n      <td>1.964263</td>\n      <td>0.180000</td>\n      <td>-0.027037</td>\n      <td>0.127565</td>\n      <td>0.552000</td>\n      <td>0.756000</td>\n      <td>0.460000</td>\n      <td>0.828000</td>\n      <td>0.764000</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.634200</td>\n      <td>1.802656</td>\n      <td>0.172000</td>\n      <td>0.028668</td>\n      <td>0.132789</td>\n      <td>0.652000</td>\n      <td>0.744000</td>\n      <td>0.432000</td>\n      <td>0.768000</td>\n      <td>0.748000</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>1.612000</td>\n      <td>1.732573</td>\n      <td>0.196000</td>\n      <td>0.036189</td>\n      <td>0.158384</td>\n      <td>0.664000</td>\n      <td>0.760000</td>\n      <td>0.488000</td>\n      <td>0.832000</td>\n      <td>0.648000</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>1.598200</td>\n      <td>1.725322</td>\n      <td>0.212000</td>\n      <td>0.084633</td>\n      <td>0.201097</td>\n      <td>0.728000</td>\n      <td>0.728000</td>\n      <td>0.448000</td>\n      <td>0.788000</td>\n      <td>0.732000</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>1.597300</td>\n      <td>1.858308</td>\n      <td>0.248000</td>\n      <td>-0.001660</td>\n      <td>0.224271</td>\n      <td>0.548000</td>\n      <td>0.784000</td>\n      <td>0.744000</td>\n      <td>0.724000</td>\n      <td>0.696000</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>1.643900</td>\n      <td>1.932401</td>\n      <td>0.264000</td>\n      <td>0.045234</td>\n      <td>0.245010</td>\n      <td>0.584000</td>\n      <td>0.792000</td>\n      <td>0.760000</td>\n      <td>0.720000</td>\n      <td>0.672000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.632500</td>\n      <td>1.741848</td>\n      <td>0.260000</td>\n      <td>0.060295</td>\n      <td>0.244212</td>\n      <td>0.668000</td>\n      <td>0.736000</td>\n      <td>0.800000</td>\n      <td>0.600000</td>\n      <td>0.716000</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>1.635900</td>\n      <td>1.684691</td>\n      <td>0.264000</td>\n      <td>0.027529</td>\n      <td>0.248264</td>\n      <td>0.588000</td>\n      <td>0.732000</td>\n      <td>0.808000</td>\n      <td>0.668000</td>\n      <td>0.732000</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>1.623700</td>\n      <td>1.714186</td>\n      <td>0.232000</td>\n      <td>-0.024440</td>\n      <td>0.210728</td>\n      <td>0.616000</td>\n      <td>0.700000</td>\n      <td>0.812000</td>\n      <td>0.756000</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>1.629700</td>\n      <td>1.745315</td>\n      <td>0.308000</td>\n      <td>0.171994</td>\n      <td>0.290695</td>\n      <td>0.748000</td>\n      <td>0.740000</td>\n      <td>0.660000</td>\n      <td>0.764000</td>\n      <td>0.704000</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>1.624100</td>\n      <td>1.684404</td>\n      <td>0.224000</td>\n      <td>0.006444</td>\n      <td>0.193960</td>\n      <td>0.772000</td>\n      <td>0.608000</td>\n      <td>0.768000</td>\n      <td>0.556000</td>\n      <td>0.744000</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.597600</td>\n      <td>1.880211</td>\n      <td>0.260000</td>\n      <td>0.093782</td>\n      <td>0.223451</td>\n      <td>0.580000</td>\n      <td>0.744000</td>\n      <td>0.748000</td>\n      <td>0.844000</td>\n      <td>0.604000</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>1.572200</td>\n      <td>2.065657</td>\n      <td>0.204000</td>\n      <td>0.075391</td>\n      <td>0.174108</td>\n      <td>0.736000</td>\n      <td>0.740000</td>\n      <td>0.428000</td>\n      <td>0.856000</td>\n      <td>0.648000</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>1.611600</td>\n      <td>2.081388</td>\n      <td>0.192000</td>\n      <td>0.024591</td>\n      <td>0.158767</td>\n      <td>0.748000</td>\n      <td>0.676000</td>\n      <td>0.508000</td>\n      <td>0.856000</td>\n      <td>0.596000</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>1.617200</td>\n      <td>2.209081</td>\n      <td>0.224000</td>\n      <td>0.096916</td>\n      <td>0.164556</td>\n      <td>0.752000</td>\n      <td>0.544000</td>\n      <td>0.588000</td>\n      <td>0.856000</td>\n      <td>0.708000</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>1.620700</td>\n      <td>1.890223</td>\n      <td>0.180000</td>\n      <td>0.004597</td>\n      <td>0.136945</td>\n      <td>0.756000</td>\n      <td>0.664000</td>\n      <td>0.480000</td>\n      <td>0.848000</td>\n      <td>0.612000</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>1.621400</td>\n      <td>1.757790</td>\n      <td>0.184000</td>\n      <td>-0.004689</td>\n      <td>0.146666</td>\n      <td>0.716000</td>\n      <td>0.736000</td>\n      <td>0.456000</td>\n      <td>0.812000</td>\n      <td>0.648000</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>1.587200</td>\n      <td>2.359975</td>\n      <td>0.208000</td>\n      <td>0.028340</td>\n      <td>0.134319</td>\n      <td>0.756000</td>\n      <td>0.796000</td>\n      <td>0.260000</td>\n      <td>0.856000</td>\n      <td>0.748000</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>1.657100</td>\n      <td>2.141506</td>\n      <td>0.196000</td>\n      <td>-0.053951</td>\n      <td>0.143992</td>\n      <td>0.736000</td>\n      <td>0.788000</td>\n      <td>0.408000</td>\n      <td>0.796000</td>\n      <td>0.664000</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>1.588900</td>\n      <td>1.894688</td>\n      <td>0.180000</td>\n      <td>-0.015853</td>\n      <td>0.143987</td>\n      <td>0.752000</td>\n      <td>0.780000</td>\n      <td>0.640000</td>\n      <td>0.560000</td>\n      <td>0.628000</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>1.612700</td>\n      <td>2.071516</td>\n      <td>0.184000</td>\n      <td>0.053910</td>\n      <td>0.127639</td>\n      <td>0.760000</td>\n      <td>0.760000</td>\n      <td>0.332000</td>\n      <td>0.768000</td>\n      <td>0.748000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.618200</td>\n      <td>1.871252</td>\n      <td>0.180000</td>\n      <td>-0.068174</td>\n      <td>0.148082</td>\n      <td>0.760000</td>\n      <td>0.692000</td>\n      <td>0.480000</td>\n      <td>0.740000</td>\n      <td>0.688000</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>1.589600</td>\n      <td>2.081079</td>\n      <td>0.180000</td>\n      <td>-0.085978</td>\n      <td>0.142204</td>\n      <td>0.732000</td>\n      <td>0.444000</td>\n      <td>0.680000</td>\n      <td>0.756000</td>\n      <td>0.748000</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>1.591400</td>\n      <td>2.016661</td>\n      <td>0.264000</td>\n      <td>-0.038418</td>\n      <td>0.206593</td>\n      <td>0.464000</td>\n      <td>0.700000</td>\n      <td>0.832000</td>\n      <td>0.772000</td>\n      <td>0.760000</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>1.598500</td>\n      <td>2.150519</td>\n      <td>0.212000</td>\n      <td>-0.010671</td>\n      <td>0.133924</td>\n      <td>0.464000</td>\n      <td>0.788000</td>\n      <td>0.564000</td>\n      <td>0.840000</td>\n      <td>0.768000</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>1.630900</td>\n      <td>1.762964</td>\n      <td>0.172000</td>\n      <td>-0.156803</td>\n      <td>0.120974</td>\n      <td>0.656000</td>\n      <td>0.780000</td>\n      <td>0.396000</td>\n      <td>0.812000</td>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>1.641000</td>\n      <td>1.704911</td>\n      <td>0.196000</td>\n      <td>0.024803</td>\n      <td>0.195300</td>\n      <td>0.596000</td>\n      <td>0.748000</td>\n      <td>0.484000</td>\n      <td>0.836000</td>\n      <td>0.728000</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>1.631700</td>\n      <td>1.722359</td>\n      <td>0.208000</td>\n      <td>0.004636</td>\n      <td>0.199093</td>\n      <td>0.528000</td>\n      <td>0.764000</td>\n      <td>0.708000</td>\n      <td>0.732000</td>\n      <td>0.684000</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>1.619800</td>\n      <td>1.724054</td>\n      <td>0.220000</td>\n      <td>0.006384</td>\n      <td>0.188875</td>\n      <td>0.704000</td>\n      <td>0.772000</td>\n      <td>0.544000</td>\n      <td>0.680000</td>\n      <td>0.740000</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>1.604400</td>\n      <td>1.841790</td>\n      <td>0.208000</td>\n      <td>0.070051</td>\n      <td>0.197271</td>\n      <td>0.692000</td>\n      <td>0.640000</td>\n      <td>0.744000</td>\n      <td>0.624000</td>\n      <td>0.716000</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>1.616900</td>\n      <td>1.866316</td>\n      <td>0.128000</td>\n      <td>0.068164</td>\n      <td>0.088270</td>\n      <td>0.760000</td>\n      <td>0.748000</td>\n      <td>0.432000</td>\n      <td>0.540000</td>\n      <td>0.776000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.616900</td>\n      <td>1.862851</td>\n      <td>0.192000</td>\n      <td>0.020933</td>\n      <td>0.156702</td>\n      <td>0.760000</td>\n      <td>0.684000</td>\n      <td>0.676000</td>\n      <td>0.540000</td>\n      <td>0.724000</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>1.648700</td>\n      <td>1.908278</td>\n      <td>0.192000</td>\n      <td>0.045110</td>\n      <td>0.139185</td>\n      <td>0.744000</td>\n      <td>0.740000</td>\n      <td>0.692000</td>\n      <td>0.680000</td>\n      <td>0.528000</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>1.562200</td>\n      <td>1.951785</td>\n      <td>0.236000</td>\n      <td>0.120324</td>\n      <td>0.222121</td>\n      <td>0.600000</td>\n      <td>0.796000</td>\n      <td>0.708000</td>\n      <td>0.744000</td>\n      <td>0.624000</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>1.636200</td>\n      <td>1.817706</td>\n      <td>0.244000</td>\n      <td>0.019659</td>\n      <td>0.241224</td>\n      <td>0.704000</td>\n      <td>0.684000</td>\n      <td>0.688000</td>\n      <td>0.744000</td>\n      <td>0.668000</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>1.624100</td>\n      <td>1.783945</td>\n      <td>0.208000</td>\n      <td>-0.007194</td>\n      <td>0.191525</td>\n      <td>0.744000</td>\n      <td>0.704000</td>\n      <td>0.748000</td>\n      <td>0.472000</td>\n      <td>0.748000</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>1.621900</td>\n      <td>1.724925</td>\n      <td>0.232000</td>\n      <td>0.005046</td>\n      <td>0.196332</td>\n      <td>0.720000</td>\n      <td>0.744000</td>\n      <td>0.776000</td>\n      <td>0.760000</td>\n      <td>0.464000</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>1.625000</td>\n      <td>1.750773</td>\n      <td>0.212000</td>\n      <td>0.048169</td>\n      <td>0.207308</td>\n      <td>0.660000</td>\n      <td>0.696000</td>\n      <td>0.808000</td>\n      <td>0.572000</td>\n      <td>0.688000</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>1.594500</td>\n      <td>1.641453</td>\n      <td>0.272000</td>\n      <td>0.065443</td>\n      <td>0.240857</td>\n      <td>0.740000</td>\n      <td>0.660000</td>\n      <td>0.792000</td>\n      <td>0.728000</td>\n      <td>0.624000</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>1.614100</td>\n      <td>1.838150</td>\n      <td>0.232000</td>\n      <td>0.047544</td>\n      <td>0.186514</td>\n      <td>0.760000</td>\n      <td>0.656000</td>\n      <td>0.732000</td>\n      <td>0.764000</td>\n      <td>0.552000</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>1.594600</td>\n      <td>1.699352</td>\n      <td>0.268000</td>\n      <td>0.037809</td>\n      <td>0.204894</td>\n      <td>0.756000</td>\n      <td>0.632000</td>\n      <td>0.780000</td>\n      <td>0.800000</td>\n      <td>0.568000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>1.611600</td>\n      <td>1.768015</td>\n      <td>0.248000</td>\n      <td>0.018406</td>\n      <td>0.217952</td>\n      <td>0.632000</td>\n      <td>0.728000</td>\n      <td>0.808000</td>\n      <td>0.812000</td>\n      <td>0.516000</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>1.601500</td>\n      <td>2.001035</td>\n      <td>0.228000</td>\n      <td>0.080628</td>\n      <td>0.154289</td>\n      <td>0.664000</td>\n      <td>0.712000</td>\n      <td>0.804000</td>\n      <td>0.828000</td>\n      <td>0.448000</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>1.649700</td>\n      <td>1.999895</td>\n      <td>0.240000</td>\n      <td>0.090953</td>\n      <td>0.158451</td>\n      <td>0.640000</td>\n      <td>0.776000</td>\n      <td>0.816000</td>\n      <td>0.768000</td>\n      <td>0.480000</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>1.599200</td>\n      <td>1.720367</td>\n      <td>0.268000</td>\n      <td>0.153399</td>\n      <td>0.220210</td>\n      <td>0.628000</td>\n      <td>0.752000</td>\n      <td>0.820000</td>\n      <td>0.684000</td>\n      <td>0.652000</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>1.613600</td>\n      <td>1.884339</td>\n      <td>0.248000</td>\n      <td>0.096960</td>\n      <td>0.204175</td>\n      <td>0.556000</td>\n      <td>0.728000</td>\n      <td>0.828000</td>\n      <td>0.812000</td>\n      <td>0.572000</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>1.604500</td>\n      <td>2.081754</td>\n      <td>0.284000</td>\n      <td>0.167133</td>\n      <td>0.227005</td>\n      <td>0.640000</td>\n      <td>0.760000</td>\n      <td>0.788000</td>\n      <td>0.788000</td>\n      <td>0.592000</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>1.573600</td>\n      <td>1.853529</td>\n      <td>0.244000</td>\n      <td>0.041523</td>\n      <td>0.168657</td>\n      <td>0.684000</td>\n      <td>0.764000</td>\n      <td>0.816000</td>\n      <td>0.796000</td>\n      <td>0.428000</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>1.579000</td>\n      <td>2.193404</td>\n      <td>0.276000</td>\n      <td>0.143446</td>\n      <td>0.204220</td>\n      <td>0.676000</td>\n      <td>0.716000</td>\n      <td>0.816000</td>\n      <td>0.824000</td>\n      <td>0.520000</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>1.595800</td>\n      <td>1.954781</td>\n      <td>0.260000</td>\n      <td>0.164345</td>\n      <td>0.219193</td>\n      <td>0.528000</td>\n      <td>0.628000</td>\n      <td>0.828000</td>\n      <td>0.824000</td>\n      <td>0.712000</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>1.587400</td>\n      <td>1.792315</td>\n      <td>0.256000</td>\n      <td>0.088510</td>\n      <td>0.204549</td>\n      <td>0.464000</td>\n      <td>0.656000</td>\n      <td>0.832000</td>\n      <td>0.852000</td>\n      <td>0.708000</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.602900</td>\n      <td>1.831695</td>\n      <td>0.252000</td>\n      <td>0.124882</td>\n      <td>0.229019</td>\n      <td>0.604000</td>\n      <td>0.712000</td>\n      <td>0.780000</td>\n      <td>0.788000</td>\n      <td>0.620000</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>1.663200</td>\n      <td>1.809513</td>\n      <td>0.244000</td>\n      <td>0.099983</td>\n      <td>0.173530</td>\n      <td>0.348000</td>\n      <td>0.736000</td>\n      <td>0.784000</td>\n      <td>0.856000</td>\n      <td>0.764000</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>1.621100</td>\n      <td>1.808128</td>\n      <td>0.208000</td>\n      <td>-0.021275</td>\n      <td>0.154189</td>\n      <td>0.384000</td>\n      <td>0.664000</td>\n      <td>0.792000</td>\n      <td>0.848000</td>\n      <td>0.728000</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>1.623200</td>\n      <td>1.758456</td>\n      <td>0.220000</td>\n      <td>-0.011321</td>\n      <td>0.134773</td>\n      <td>0.336000</td>\n      <td>0.720000</td>\n      <td>0.788000</td>\n      <td>0.844000</td>\n      <td>0.752000</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>1.624500</td>\n      <td>1.796564</td>\n      <td>0.176000</td>\n      <td>-0.056126</td>\n      <td>0.131693</td>\n      <td>0.448000</td>\n      <td>0.576000</td>\n      <td>0.772000</td>\n      <td>0.804000</td>\n      <td>0.752000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.602300</td>\n      <td>1.848267</td>\n      <td>0.208000</td>\n      <td>-0.032244</td>\n      <td>0.123830</td>\n      <td>0.332000</td>\n      <td>0.692000</td>\n      <td>0.772000</td>\n      <td>0.852000</td>\n      <td>0.768000</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>1.604400</td>\n      <td>1.801085</td>\n      <td>0.220000</td>\n      <td>0.148834</td>\n      <td>0.192734</td>\n      <td>0.560000</td>\n      <td>0.748000</td>\n      <td>0.664000</td>\n      <td>0.748000</td>\n      <td>0.720000</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>1.603500</td>\n      <td>1.741543</td>\n      <td>0.240000</td>\n      <td>0.141182</td>\n      <td>0.197266</td>\n      <td>0.512000</td>\n      <td>0.784000</td>\n      <td>0.772000</td>\n      <td>0.712000</td>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>1.602800</td>\n      <td>1.965768</td>\n      <td>0.276000</td>\n      <td>0.192746</td>\n      <td>0.251516</td>\n      <td>0.564000</td>\n      <td>0.780000</td>\n      <td>0.700000</td>\n      <td>0.780000</td>\n      <td>0.728000</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>1.609400</td>\n      <td>2.131065</td>\n      <td>0.156000</td>\n      <td>0.056865</td>\n      <td>0.128003</td>\n      <td>0.732000</td>\n      <td>0.760000</td>\n      <td>0.736000</td>\n      <td>0.552000</td>\n      <td>0.532000</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>1.626300</td>\n      <td>1.916560</td>\n      <td>0.244000</td>\n      <td>0.096909</td>\n      <td>0.216015</td>\n      <td>0.692000</td>\n      <td>0.648000</td>\n      <td>0.824000</td>\n      <td>0.656000</td>\n      <td>0.668000</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>1.620500</td>\n      <td>1.891526</td>\n      <td>0.244000</td>\n      <td>0.118811</td>\n      <td>0.216324</td>\n      <td>0.688000</td>\n      <td>0.628000</td>\n      <td>0.824000</td>\n      <td>0.716000</td>\n      <td>0.632000</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>1.603900</td>\n      <td>1.711768</td>\n      <td>0.232000</td>\n      <td>0.076841</td>\n      <td>0.194129</td>\n      <td>0.692000</td>\n      <td>0.676000</td>\n      <td>0.824000</td>\n      <td>0.720000</td>\n      <td>0.552000</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>1.618900</td>\n      <td>2.046898</td>\n      <td>0.268000</td>\n      <td>0.162794</td>\n      <td>0.243562</td>\n      <td>0.676000</td>\n      <td>0.684000</td>\n      <td>0.832000</td>\n      <td>0.676000</td>\n      <td>0.668000</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>1.592500</td>\n      <td>2.229083</td>\n      <td>0.232000</td>\n      <td>0.136407</td>\n      <td>0.218108</td>\n      <td>0.672000</td>\n      <td>0.728000</td>\n      <td>0.768000</td>\n      <td>0.604000</td>\n      <td>0.692000</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>1.604700</td>\n      <td>1.814705</td>\n      <td>0.300000</td>\n      <td>0.153400</td>\n      <td>0.283510</td>\n      <td>0.572000</td>\n      <td>0.708000</td>\n      <td>0.808000</td>\n      <td>0.748000</td>\n      <td>0.764000</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>1.632500</td>\n      <td>1.671806</td>\n      <td>0.280000</td>\n      <td>0.054830</td>\n      <td>0.249811</td>\n      <td>0.512000</td>\n      <td>0.676000</td>\n      <td>0.820000</td>\n      <td>0.836000</td>\n      <td>0.716000</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>1.605300</td>\n      <td>1.686655</td>\n      <td>0.232000</td>\n      <td>0.080071</td>\n      <td>0.234471</td>\n      <td>0.664000</td>\n      <td>0.752000</td>\n      <td>0.676000</td>\n      <td>0.668000</td>\n      <td>0.704000</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>1.611200</td>\n      <td>1.731554</td>\n      <td>0.252000</td>\n      <td>0.084677</td>\n      <td>0.215225</td>\n      <td>0.652000</td>\n      <td>0.624000</td>\n      <td>0.720000</td>\n      <td>0.744000</td>\n      <td>0.764000</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>1.608400</td>\n      <td>1.766667</td>\n      <td>0.244000</td>\n      <td>0.082077</td>\n      <td>0.237523</td>\n      <td>0.704000</td>\n      <td>0.736000</td>\n      <td>0.676000</td>\n      <td>0.760000</td>\n      <td>0.612000</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>1.596800</td>\n      <td>1.692458</td>\n      <td>0.280000</td>\n      <td>0.083014</td>\n      <td>0.252742</td>\n      <td>0.648000</td>\n      <td>0.724000</td>\n      <td>0.764000</td>\n      <td>0.856000</td>\n      <td>0.568000</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>1.593400</td>\n      <td>1.816354</td>\n      <td>0.216000</td>\n      <td>-0.011153</td>\n      <td>0.186691</td>\n      <td>0.740000</td>\n      <td>0.700000</td>\n      <td>0.656000</td>\n      <td>0.636000</td>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>1.575600</td>\n      <td>1.741086</td>\n      <td>0.204000</td>\n      <td>0.045274</td>\n      <td>0.182582</td>\n      <td>0.744000</td>\n      <td>0.712000</td>\n      <td>0.700000</td>\n      <td>0.524000</td>\n      <td>0.728000</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>1.589000</td>\n      <td>1.829743</td>\n      <td>0.200000</td>\n      <td>0.157760</td>\n      <td>0.196822</td>\n      <td>0.632000</td>\n      <td>0.720000</td>\n      <td>0.652000</td>\n      <td>0.672000</td>\n      <td>0.724000</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>1.577900</td>\n      <td>1.894966</td>\n      <td>0.180000</td>\n      <td>0.168891</td>\n      <td>0.155708</td>\n      <td>0.628000</td>\n      <td>0.748000</td>\n      <td>0.672000</td>\n      <td>0.572000</td>\n      <td>0.740000</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>1.646200</td>\n      <td>1.817586</td>\n      <td>0.236000</td>\n      <td>0.232197</td>\n      <td>0.228482</td>\n      <td>0.696000</td>\n      <td>0.720000</td>\n      <td>0.724000</td>\n      <td>0.576000</td>\n      <td>0.756000</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>1.636200</td>\n      <td>1.811271</td>\n      <td>0.176000</td>\n      <td>0.123970</td>\n      <td>0.166097</td>\n      <td>0.684000</td>\n      <td>0.760000</td>\n      <td>0.700000</td>\n      <td>0.496000</td>\n      <td>0.712000</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>1.563700</td>\n      <td>1.771612</td>\n      <td>0.252000</td>\n      <td>0.076596</td>\n      <td>0.221259</td>\n      <td>0.728000</td>\n      <td>0.716000</td>\n      <td>0.716000</td>\n      <td>0.808000</td>\n      <td>0.536000</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>1.600900</td>\n      <td>1.953600</td>\n      <td>0.216000</td>\n      <td>0.108278</td>\n      <td>0.214748</td>\n      <td>0.700000</td>\n      <td>0.660000</td>\n      <td>0.584000</td>\n      <td>0.780000</td>\n      <td>0.708000</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>1.593200</td>\n      <td>1.754083</td>\n      <td>0.252000</td>\n      <td>0.102702</td>\n      <td>0.240063</td>\n      <td>0.656000</td>\n      <td>0.740000</td>\n      <td>0.660000</td>\n      <td>0.824000</td>\n      <td>0.624000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.594400</td>\n      <td>1.861477</td>\n      <td>0.220000</td>\n      <td>0.108334</td>\n      <td>0.203351</td>\n      <td>0.684000</td>\n      <td>0.716000</td>\n      <td>0.640000</td>\n      <td>0.816000</td>\n      <td>0.584000</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>1.568900</td>\n      <td>1.722611</td>\n      <td>0.288000</td>\n      <td>0.218885</td>\n      <td>0.252165</td>\n      <td>0.664000</td>\n      <td>0.716000</td>\n      <td>0.772000</td>\n      <td>0.832000</td>\n      <td>0.592000</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>1.610400</td>\n      <td>1.683325</td>\n      <td>0.300000</td>\n      <td>0.213588</td>\n      <td>0.231577</td>\n      <td>0.664000</td>\n      <td>0.724000</td>\n      <td>0.824000</td>\n      <td>0.848000</td>\n      <td>0.540000</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>1.606500</td>\n      <td>1.686888</td>\n      <td>0.276000</td>\n      <td>0.199333</td>\n      <td>0.242556</td>\n      <td>0.708000</td>\n      <td>0.664000</td>\n      <td>0.788000</td>\n      <td>0.720000</td>\n      <td>0.672000</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>1.586300</td>\n      <td>1.678154</td>\n      <td>0.300000</td>\n      <td>0.256307</td>\n      <td>0.244099</td>\n      <td>0.748000</td>\n      <td>0.640000</td>\n      <td>0.804000</td>\n      <td>0.720000</td>\n      <td>0.688000</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>1.582500</td>\n      <td>2.132709</td>\n      <td>0.216000</td>\n      <td>0.069946</td>\n      <td>0.190439</td>\n      <td>0.692000</td>\n      <td>0.772000</td>\n      <td>0.700000</td>\n      <td>0.736000</td>\n      <td>0.532000</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>1.578700</td>\n      <td>1.932246</td>\n      <td>0.252000</td>\n      <td>0.167769</td>\n      <td>0.225432</td>\n      <td>0.728000</td>\n      <td>0.720000</td>\n      <td>0.764000</td>\n      <td>0.588000</td>\n      <td>0.704000</td>\n    </tr>\n    <tr>\n      <td>3140</td>\n      <td>1.578600</td>\n      <td>1.742467</td>\n      <td>0.244000</td>\n      <td>0.178020</td>\n      <td>0.239857</td>\n      <td>0.724000</td>\n      <td>0.756000</td>\n      <td>0.784000</td>\n      <td>0.532000</td>\n      <td>0.692000</td>\n    </tr>\n    <tr>\n      <td>3160</td>\n      <td>1.632800</td>\n      <td>2.008698</td>\n      <td>0.288000</td>\n      <td>0.231514</td>\n      <td>0.275332</td>\n      <td>0.756000</td>\n      <td>0.748000</td>\n      <td>0.684000</td>\n      <td>0.680000</td>\n      <td>0.708000</td>\n    </tr>\n    <tr>\n      <td>3180</td>\n      <td>1.584100</td>\n      <td>1.820766</td>\n      <td>0.232000</td>\n      <td>0.170565</td>\n      <td>0.192512</td>\n      <td>0.700000</td>\n      <td>0.768000</td>\n      <td>0.736000</td>\n      <td>0.652000</td>\n      <td>0.608000</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>1.586100</td>\n      <td>1.659878</td>\n      <td>0.268000</td>\n      <td>0.226942</td>\n      <td>0.234929</td>\n      <td>0.648000</td>\n      <td>0.756000</td>\n      <td>0.824000</td>\n      <td>0.580000</td>\n      <td>0.728000</td>\n    </tr>\n    <tr>\n      <td>3220</td>\n      <td>1.570400</td>\n      <td>1.654373</td>\n      <td>0.312000</td>\n      <td>0.270111</td>\n      <td>0.282943</td>\n      <td>0.696000</td>\n      <td>0.728000</td>\n      <td>0.828000</td>\n      <td>0.656000</td>\n      <td>0.716000</td>\n    </tr>\n    <tr>\n      <td>3240</td>\n      <td>1.620700</td>\n      <td>1.788980</td>\n      <td>0.288000</td>\n      <td>0.197707</td>\n      <td>0.258836</td>\n      <td>0.684000</td>\n      <td>0.772000</td>\n      <td>0.800000</td>\n      <td>0.696000</td>\n      <td>0.624000</td>\n    </tr>\n    <tr>\n      <td>3260</td>\n      <td>1.587200</td>\n      <td>1.694668</td>\n      <td>0.328000</td>\n      <td>0.260580</td>\n      <td>0.321977</td>\n      <td>0.696000</td>\n      <td>0.764000</td>\n      <td>0.760000</td>\n      <td>0.728000</td>\n      <td>0.708000</td>\n    </tr>\n    <tr>\n      <td>3280</td>\n      <td>1.577200</td>\n      <td>1.676761</td>\n      <td>0.272000</td>\n      <td>0.076862</td>\n      <td>0.241726</td>\n      <td>0.672000</td>\n      <td>0.720000</td>\n      <td>0.808000</td>\n      <td>0.752000</td>\n      <td>0.592000</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>1.590700</td>\n      <td>1.706305</td>\n      <td>0.280000</td>\n      <td>0.079707</td>\n      <td>0.249649</td>\n      <td>0.704000</td>\n      <td>0.700000</td>\n      <td>0.780000</td>\n      <td>0.784000</td>\n      <td>0.592000</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='113' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [113/125 00:26 < 00:02, 4.19 it/s]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[[ 0  0  0  0 60]\n [ 0  0  0  0 54]\n [ 0  0  0  0 42]\n [ 0  0  0  1 35]\n [ 0  0  0  1 57]]\n[[ 0 11  0  2 47]\n [ 0  7  0  3 44]\n [ 0  5  0  5 32]\n [ 1  2  0  0 33]\n [ 0 10  1  2 45]]\n[[ 1 10 31  4 14]\n [ 0  7 27  6 14]\n [ 1 11 18  5  7]\n [ 1  3 21  2  9]\n [ 5 15 22  4 12]]\n[[14 14 11  3 18]\n [ 9 15  9  0 21]\n [ 1 17  7  0 17]\n [ 5  6  9  1 15]\n [ 8 21  7  4 18]]\n[[ 6 19  3  7 25]\n [ 4 23  1  4 22]\n [ 2 17  1  5 17]\n [ 4  9  2  6 15]\n [ 4 25  3  6 20]]\n[[ 2 14 12  0 32]\n [ 1 12 17  0 24]\n [ 0  9  6  3 24]\n [ 1  2  9  2 22]\n [ 0 12  8  2 36]]\n[[10 18  3  6 23]\n [13 13  7  1 20]\n [ 2 15  4  3 18]\n [ 7  4  2  5 18]\n [ 8 16  3  4 27]]\n[[27  7  3  7 16]\n [21 10  9  4 10]\n [ 9 11  6  4 12]\n [15  2  1  5 13]\n [16 12  3  7 20]]\n[[27  3  0  9 21]\n [19  3  7  3 22]\n [14  6  3  5 14]\n [16  0  1  4 15]\n [21  6  0  7 24]]\n[[18 13  5  8 16]\n [13 11 12  2 16]\n [10 11  4  7 10]\n [15  4  2  4 11]\n [12 14  3 13 16]]\n[[ 9 16  5  7 23]\n [12 15  6  1 20]\n [ 4 12  0  5 21]\n [ 7  4  1  4 20]\n [ 3 20  2  4 29]]\n[[ 6 20  3  0 31]\n [ 4 24  1  0 25]\n [ 2 12  1  2 25]\n [ 2  7  2  0 25]\n [ 4 21  0  2 31]]\n[[ 4  9 21  4 22]\n [10  5 18  1 20]\n [ 2  3 15  2 20]\n [ 1  2 16  4 13]\n [ 0 10 18  5 25]]\n[[ 8 13 10 15 14]\n [14  9  8  5 18]\n [ 5  8  8  4 17]\n [ 8  6  8 11  3]\n [ 5 18  1 12 22]]\n[[16  8  2 19 15]\n [23  6  5  6 14]\n [10  5  2 14 11]\n [10  3  3 11  9]\n [ 6  5  2 26 19]]\n[[14  6 10  9 21]\n [16  8  5  8 17]\n [ 9  6  3  5 19]\n [ 7  0  6  5 18]\n [14  4  3  8 29]]\n[[ 7  0  0  0 53]\n [ 5  0  3  0 46]\n [ 3  0  0  0 39]\n [ 3  4  0  0 29]\n [ 4  3  0  0 51]]\n[[15  2 13  0 30]\n [ 6  6 19  0 23]\n [ 9  4  4  0 25]\n [ 8  1 10  0 17]\n [14  1 15  0 28]]\n[[10 22  2  1 25]\n [15 15  5  0 19]\n [ 8 14  0  0 20]\n [ 5 14  0  1 16]\n [ 8 17  2  2 29]]\n[[16  8 12  3 21]\n [15 11  7  3 18]\n [ 6  2  5  2 27]\n [ 7  2  9  2 16]\n [21  1  6  2 28]]\n[[ 3  2 16  2 37]\n [ 8  2 10  0 34]\n [ 4  0  6  0 32]\n [ 2  1 12  2 19]\n [ 6  0  7  2 43]]\n[[28  9  6  2 15]\n [33  5  3  0 13]\n [15  2  1  0 24]\n [14  5  5  1 11]\n [23  4  4  0 27]]\n[[17  3 16 15  9]\n [18  9 10  9  8]\n [ 7  4 13 11  7]\n [ 9  3  8 15  1]\n [18  2 16  5 17]]\n[[ 6 14 11 27  2]\n [ 6 10 13 19  6]\n [ 2  7 10 19  4]\n [ 3  5 11 16  1]\n [ 4  8 13 26  7]]\n[[47  3  1  3  6]\n [45  1  1  0  7]\n [34  2  1  3  2]\n [24  1  2  2  7]\n [44  2  0  7  5]]\n[[43  8  0  3  6]\n [42  2  0  5  5]\n [33  5  0  2  2]\n [22  7  0  6  1]\n [41 12  0  3  2]]\n[[19 32  8  0  1]\n [16 33  2  1  2]\n [16 21  5  0  0]\n [12 22  0  2  0]\n [11 40  2  3  2]]\n[[19 24  4  9  4]\n [11 21 13  5  4]\n [12 18  6  0  6]\n [10 11 10  5  0]\n [17 30  6  2  3]]\n[[ 6 11  3 30 10]\n [ 7  6  3 29  9]\n [ 4  6  5 23  4]\n [ 2  4  3 20  7]\n [ 3 13  7 31  4]]\n[[10  9  2 27 12]\n [ 7  4  5 20 18]\n [ 3  5  1 21 12]\n [ 3  3  3 23  4]\n [ 5 14  1 27 11]]\n[[30  0 16  9  5]\n [27  0  9 11  7]\n [16  0 14  8  4]\n [18  0 10  4  4]\n [24  3 14  8  9]]\n[[12 26  0  8 14]\n [10 26  1  5 12]\n [ 9 22  1  3  7]\n [11 16  0  0  9]\n [14 30  0  3 11]]\n[[ 5 37  0  7 11]\n [ 7 33  1  3 10]\n [ 2 27  0  2 11]\n [ 3 23  0  5  5]\n [ 5 35  0  7 11]]\n[[12  8  9 18 13]\n [ 7 18  9  9 11]\n [ 3 10  8 13  8]\n [ 4  6 11  7  8]\n [ 3  5 19 26  5]]\n[[20  3  9  5 23]\n [12  4  7  7 24]\n [ 8  4  5  8 17]\n [10  1  6  3 16]\n [18  1 13  4 22]]\n[[ 0 18  5 14 23]\n [ 2 19  2 14 17]\n [ 0 14  3  7 18]\n [ 1 13  3  9 10]\n [ 0 18  1  9 30]]\n[[ 0 43  7  1  9]\n [ 0 36  8  2  8]\n [ 0 34  2  1  5]\n [ 0 22  7  0  7]\n [ 0 37  5  1 15]]\n[[ 1 19 19  5 16]\n [ 6  9 24  4 11]\n [ 3 16 14  3  6]\n [ 1  8 16  4  7]\n [ 3 29 19  2  5]]\n[[ 7 10 14 10 19]\n [12  4 17  7 14]\n [ 3  6  9  3 21]\n [ 5  5  4  5 17]\n [ 6  7  6  2 37]]\n[[ 4 12  7 37  0]\n [ 4  6 10 32  2]\n [ 4  7  6 24  1]\n [ 3  3  3 27  0]\n [ 4 16  5 30  3]]\n[[11 11  5 27  6]\n [ 5 16  4 23  6]\n [ 4 15  1 22  0]\n [ 3  6  0 26  1]\n [14 12  0 30  2]]\n[[ 6 14 15 21  4]\n [ 4 15 11 20  4]\n [ 3 17  6 16  0]\n [ 3 10  8 14  1]\n [14 19  5 19  1]]\n[[ 0 12  3 29 16]\n [ 0  9  9 27  9]\n [ 1  5  4 25  7]\n [ 0  5  4 16 11]\n [ 5  9  6 26 12]]\n[[ 0 21 12 22  5]\n [ 3 24  9 13  5]\n [ 2 19  4 12  5]\n [ 1 14  4 11  6]\n [ 5 21  5 21  6]]\n[[ 9  9 14 12 16]\n [ 8  3 10 13 20]\n [ 5  5  6  7 19]\n [ 3  4  5  6 18]\n [ 6 10  4  7 31]]\n[[ 3  5 28  3 21]\n [ 5  0 34  0 15]\n [ 2  2 16  1 21]\n [ 2  1 14  1 18]\n [ 5  4 25  0 24]]\n[[ 6 12 20 10 12]\n [ 0 10 26 11  7]\n [ 2  6 12  9 13]\n [ 1  5 10  8 12]\n [ 5 10 16 10 17]]\n[[ 6 18  1 16 19]\n [ 5 18  0 17 14]\n [ 2 15  0 12 13]\n [ 4  4  0 16 12]\n [ 6 14  0 23 15]]\n[[19 12 11  9  9]\n [16  6  8 19  5]\n [ 7 13  9  8  5]\n [ 8  4  7  8  9]\n [12 19 13 12  2]]\n[[ 3 15  1 24 17]\n [ 5 12  3 28  6]\n [ 2  7  1 18 14]\n [ 1  6  0 17 12]\n [ 2  7  0 19 30]]\n[[ 0 23  4 26  7]\n [ 1 17  2 31  3]\n [ 0 15  1 21  5]\n [ 0 13  0 19  4]\n [ 0 23  3 23  9]]\n[[ 2 41  3  6  8]\n [ 2 40  2  5  5]\n [ 2 32  0  3  5]\n [ 0 24  2  5  5]\n [ 1 41  1  8  7]]\n[[ 8 20 17  5 10]\n [13 19 13  2  7]\n [ 5 13 15  1  8]\n [ 2 11 11  3  9]\n [ 4 22 17  5 10]]\n[[13  7  2 16 22]\n [11  5  2 20 16]\n [ 8  5  2 15 12]\n [ 3  4  1 17 11]\n [13  8  1 16 20]]\n[[18 12  8 15  7]\n [22  6  8  8 10]\n [13 10  6  8  5]\n [ 9  6  5 11  5]\n [14 23  3  5 13]]\n[[12  9  2 10 27]\n [ 7  6  3  7 31]\n [ 5  8  3  5 21]\n [ 3  4  3 10 16]\n [ 6 16  4  6 26]]\n[[10 10  9  2 29]\n [ 6 15  2  0 31]\n [ 7 13  4  1 17]\n [ 7 10  2  1 16]\n [ 7 13  8  0 30]]\n[[ 5 22  5  4 24]\n [ 1 26  5  2 20]\n [ 2 17  1  3 19]\n [ 3 12  3  0 18]\n [ 4 24  2  1 27]]\n[[17 18  2  7 16]\n [15 13  4  6 16]\n [11 11  1  3 16]\n [ 8  7  1  6 14]\n [11 24  0  3 20]]\n[[42 14  0  1  3]\n [37  9  2  1  5]\n [28 12  0  0  2]\n [23  6  0  1  6]\n [31 20  0  2  5]]\n[[25  3  5  3 24]\n [27  4  2  5 16]\n [15  3  2  6 16]\n [15  1  3  1 16]\n [16  9  6  2 25]]\n[[30 11 16  2  1]\n [34  4 11  1  4]\n [21  4 12  2  3]\n [18  7 11  0  0]\n [21  6 25  0  6]]\n[[33  9 17  0  1]\n [26  2 18  1  7]\n [19  4 16  0  3]\n [17  5 12  1  1]\n [21  5 27  2  3]]\n[[30 15  2  1 12]\n [32  6  3  1 12]\n [21 10  2  0  9]\n [21  7  1  0  7]\n [27 20  5  0  6]]\n[[ 5 11 28  6 10]\n [ 5  8 29  5  7]\n [ 4  9 19  6  4]\n [ 3  6 18  1  8]\n [ 3 18 28  4  5]]\n[[13 11 17  4 15]\n [13  9 18  2 12]\n [ 4  9 16  2 11]\n [ 6  4 10  4 12]\n [ 5 20 17  5 11]]\n[[ 7 10  0 32 11]\n [ 8  9  0 32  5]\n [ 6  5  0 22  9]\n [ 3  7  0 21  5]\n [ 8  8  0 23 19]]\n[[11  3 19 15 12]\n [ 7  5 21 16  5]\n [ 9  5 11 11  6]\n [ 3  5  9 13  6]\n [10  4 22  9 13]]\n[[25  3 29  3  0]\n [18  2 30  3  1]\n [21  3 17  1  0]\n [12  3 20  1  0]\n [26  0 31  1  0]]\n[[ 9  5 32 12  2]\n [12  1 37  3  1]\n [ 9  2 25  4  2]\n [ 5  1 22  5  3]\n [10  3 34  8  3]]\n[[10  4 33  1 12]\n [12  2 22  4 14]\n [ 8  1 24  0  9]\n [ 8  0 20  0  8]\n [ 6  3 35  1 13]]\n[[ 9  5 35  7  4]\n [ 4  7 33  6  4]\n [ 4  7 23  5  3]\n [ 3  3 18  5  7]\n [ 6  6 33  4  9]]\n[[32  3  8 10  7]\n [17  5  9 12 11]\n [20  2  8  8  4]\n [19  0  4  9  4]\n [29  0  9 12  8]]\n[[23  1 10  9 17]\n [17  4  8 18  7]\n [15  1  6  8 12]\n [12  0  3 14  7]\n [23  0  3 13 19]]\n[[17  7  2 25  9]\n [ 7  9  4 27  7]\n [10  6  1 14 11]\n [ 7  3  2 18  6]\n [16  5  1 16 20]]\n[[27  8  4 16  5]\n [14  7  1 23  9]\n [17  5  5  9  6]\n [16  2  2 15  1]\n [23  5  4 14 12]]\n[[21  9  2  8 20]\n [13  5  1 12 23]\n [11  5  2  6 18]\n [16  3  1  9  7]\n [17  9  3  8 21]]\n[[ 6  9 17 16 12]\n [ 2 12 17 11 12]\n [ 1  7 14  4 16]\n [ 4  3  8 15  6]\n [ 2  4 15  7 30]]\n[[ 6 15  6 30  3]\n [ 0 18  2 32  2]\n [ 0 16  1 17  8]\n [ 3  6  3 23  1]\n [ 0 25  6 19  8]]\n[[27  5  5  1 22]\n [22  6  8  2 16]\n [16  5  4  0 17]\n [16  1  5  1 13]\n [18  5  7  1 27]]\n[[ 4  6 34  0 16]\n [ 5  8 33  0  8]\n [ 1  5 21  0 15]\n [ 1  2 24  0  9]\n [ 3  6 31  0 18]]\n[[ 4 11 23  0 22]\n [ 2  6 30  0 16]\n [ 2  9 18  0 13]\n [ 3  3 18  0 12]\n [ 0 10 28  0 20]]\n[[ 0 31 22  0  7]\n [ 1 28 20  0  5]\n [ 1 17 18  0  6]\n [ 0 16 13  0  7]\n [ 0 24 24  0 10]]\n[[ 1 12 26  1 20]\n [ 1  6 32  1 14]\n [ 1  9 20  0 12]\n [ 0  4 21  0 11]\n [ 0 11 29  0 18]]\n[[ 3  6 34  2 15]\n [ 1  2 34  6 11]\n [ 4  3 23  0 12]\n [ 3  2 21  2  8]\n [ 6  3 28  5 16]]\n[[ 2  0 51  0  7]\n [ 0  3 47  0  4]\n [ 2  0 39  0  1]\n [ 1  0 34  0  1]\n [ 0  0 50  0  8]]\n[[ 3  0 38  6 13]\n [ 0  1 36  8  9]\n [ 2  0 30  1  9]\n [ 3  0 23  4  6]\n [ 4  0 39  4 11]]\n[[ 3  0 18 24 15]\n [ 0  0 14 22 18]\n [ 2  0 12 21  7]\n [ 1  0 10 15 10]\n [ 2  1 18 22 15]]\n[[ 0  4 46  6  4]\n [ 0  4 38 11  1]\n [ 0  3 32  5  2]\n [ 0  2 26  3  5]\n [ 0  1 47  3  7]]\n[[ 0  6 31 14  9]\n [ 0  5 21 15 13]\n [ 0  9 20  5  8]\n [ 0  4 22  6  4]\n [ 0  9 34  1 14]]\n[[ 2 32 14 11  1]\n [ 0 29 12 10  3]\n [ 2 27  5  5  3]\n [ 1 18 10  5  2]\n [ 6 37  7  4  4]]\n[[36 16  0  8  0]\n [23 23  2  5  1]\n [26  7  4  5  0]\n [22  9  1  3  1]\n [39 12  1  6  0]]\n[[38  0 20  2  0]\n [29  1 22  2  0]\n [28  0 13  1  0]\n [17  0 18  1  0]\n [38  0 20  0  0]]\n[[ 8  0 39  5  8]\n [ 3  0 39  5  7]\n [ 9  0 28  2  3]\n [ 6  0 24  1  5]\n [16  1 35  0  6]]\n[[14  5 27  5  9]\n [14  6 28  2  4]\n [15  6 13  0  8]\n [11  1 19  2  3]\n [15  3 26  0 14]]\n[[18  3 12 13 14]\n [22  4 10 12  6]\n [20  2  5  7  8]\n [13  1  4 10  8]\n [21  3 10  9 15]]\n[[ 4  1 26 20  9]\n [ 4  1 29 17  3]\n [ 3  1 21 10  7]\n [ 3  0 16 12  5]\n [ 8  2 22  9 17]]\n[[ 7 14 14 18  7]\n [10  8  8 23  5]\n [ 6  8  9 15  4]\n [ 0  7  4 18  7]\n [ 8 15  5 20 10]]\n[[ 0  5 33 22  0]\n [ 0  3 34 17  0]\n [ 0  2 16 24  0]\n [ 0  1 24 11  0]\n [ 0  4 25 27  2]]\n[[ 0  8 14 26 12]\n [ 0  3 15 30  6]\n [ 0  7  8 22  5]\n [ 0  3  8 20  5]\n [ 0 10 10 21 17]]\n[[ 1  3 17 13 26]\n [ 3  0  6 20 25]\n [ 1  5  6 10 20]\n [ 1  1  4 11 19]\n [ 0  2 14 12 30]]\n[[20  0 10 11 19]\n [24  5  5 10 10]\n [12  2  6  6 16]\n [12  0  5  4 15]\n [12  0 17  5 24]]\n[[12 12 13 11 12]\n [ 8 22  5  9 10]\n [ 6 11 10  9  6]\n [ 8  7  7  7  7]\n [ 4 17 21  6 10]]\n[[ 4 12  6 33  5]\n [ 1 15  2 35  1]\n [ 2  5  6 26  3]\n [ 2  8  3 20  3]\n [ 3 10 16 22  7]]\n[[ 4  6  4 11 35]\n [ 4 12  3  8 27]\n [ 3  4  3  4 28]\n [ 4  5  2  4 21]\n [ 3  7  8  5 35]]\n[[12 11  5 23  9]\n [11 10  1 24  8]\n [10  5  5 16  6]\n [ 7  6  0 16  7]\n [ 9 10  5 24 10]]\n[[ 4 19  6 13 18]\n [ 1 28  1 10 14]\n [ 1 13  4  9 15]\n [ 3 12  0  9 12]\n [ 4 15  7  9 23]]\n[[ 0 16 11  7 26]\n [ 0 20  7  9 18]\n [ 0  9  4  8 21]\n [ 0 11  2  5 18]\n [ 0 16  9  4 29]]\n[[ 0 20  4  9 27]\n [ 1 29  3  3 18]\n [ 0 12  4  4 22]\n [ 0 16  3  3 14]\n [ 0 19  7  1 31]]\n[[13  5  4  7 31]\n [16  4  2  6 26]\n [ 9  2  6  1 24]\n [ 9  5  1  5 16]\n [11  6  5  2 34]]\n[[ 9  7  3  6 35]\n [16  4  1  0 33]\n [ 6  3  0  1 32]\n [ 4  7  1  0 24]\n [ 7  5  2  0 44]]\n[[15  1  0  7 37]\n [18  0  1  7 28]\n [ 9  0  0  6 27]\n [ 9  1  1  1 24]\n [ 9  0  2  3 44]]\n[[16  5  1 19 19]\n [20  3  0 15 16]\n [11  3  0 12 16]\n [ 9  1  0 15 11]\n [ 9  2  2 12 33]]\n[[21  8  0  4 27]\n [26  7  1  2 18]\n [15  6  0  3 18]\n [13  3  0  3 17]\n [18  4  0  5 31]]\n[[22  2  5  6 25]\n [21  3  2  7 21]\n [ 9  3  0  7 23]\n [13  0  2  6 15]\n [ 9  4  2  3 40]]\n[[14  2  2  4 38]\n [10  0  1  8 35]\n [ 3  3  1  4 31]\n [ 8  0  1  2 25]\n [12  0  1  1 44]]\n[[16  6  1  3 34]\n [16  7  3  3 25]\n [ 6  7  0  2 27]\n [ 9  5  0  0 22]\n [ 6  6  0  0 46]]\n[[25 19  0  5 11]\n [29 15  0  2  8]\n [22  9  0  1 10]\n [15 11  0  0 10]\n [17 15  1  0 25]]\n[[34 15  0  0 11]\n [33 14  0  0  7]\n [29  6  0  1  6]\n [20  9  0  0  7]\n [26 16  0  0 16]]\n[[16  9  4  6 25]\n [23  6  5  2 18]\n [11  6  4  8 13]\n [11  5  2  6 12]\n [10  4  6  7 31]]\n[[46  5  5  0  4]\n [49  2  1  0  2]\n [32  4  2  0  4]\n [30  1  3  0  2]\n [38  4  5  0 11]]\n[[36 14  5  1  4]\n [39 10  2  1  2]\n [26  8  3  0  5]\n [28  4  2  0  2]\n [37 14  4  0  3]]\n[[47  7  5  0  1]\n [44  4  1  3  2]\n [30  7  2  0  3]\n [31  3  2  0  0]\n [48  3  5  0  2]]\n[[27 22  5  5  1]\n [31 13  7  3  0]\n [21 12  3  3  3]\n [21 11  3  1  0]\n [32 20  3  3  0]]\n[[44  8  8  0  0]\n [44  4  5  1  0]\n [31  7  4  0  0]\n [29  3  4  0  0]\n [47  9  2  0  0]]\n[[31  5 16  3  5]\n [24  2 12  8  8]\n [22  3  6  7  4]\n [15  0 10  8  3]\n [20  3 10 17  8]]\n[[32  0  8 12  8]\n [30  0  4 10 10]\n [24  0  3  6  9]\n [17  0  4  7  8]\n [23  0  2 15 18]]\n[[30  0 12  7 11]\n [26  4  8  7  9]\n [23  1  4  5  9]\n [15  1  8  5  7]\n [15  3  9  5 26]]\n[[ 5  1 10 18 26]\n [ 6  1  7 22 18]\n [ 4  1  1 19 17]\n [ 0  3  3 11 19]\n [ 2  2  5 28 21]]\n[[ 8 18  1 15 18]\n [ 9 14  1 17 13]\n [ 7  6  0 18 11]\n [ 4 10  0 14  8]\n [ 5 14  0 14 25]]\n[[ 9 24  1  7 19]\n [10 22  1 11 10]\n [ 4 11  0 11 16]\n [ 6 11  0 10  9]\n [ 7 15  0 16 20]]\n[[10 13  0 10 27]\n [10  7  1 17 19]\n [ 7  3  0  9 23]\n [ 3  7  0  8 18]\n [ 7 11  1  6 33]]\n[[13 18  0 11 18]\n [11 18  0 15 10]\n [ 7  8  0 14 13]\n [ 8  8  0  9 11]\n [ 8  9  0 14 27]]\n[[ 5 11  8 17 19]\n [11 11  3 21  8]\n [ 8  4  3 17 10]\n [ 4  6  5 13  8]\n [ 4  4  3 21 26]]\n[[26 16  1  8  9]\n [18 22  1  8  5]\n [21  9  1  8  3]\n [15  9  2  7  3]\n [19  7  3 10 19]]\n[[29 20  3  2  6]\n [21 23  0  1  9]\n [23  8  4  1  6]\n [16 13  1  1  5]\n [31  9  3  2 13]]\n[[12  9 13 15 11]\n [10 16 10 12  6]\n [12  6 12  9  3]\n [ 6  6 10 10  4]\n [ 8  3 18 21  8]]\n[[13 28  9  9  1]\n [10 33  5  6  0]\n [ 9 14 11  8  0]\n [ 8 16  6  4  2]\n [13 15 19  9  2]]\n[[ 8 11 13  8 20]\n [ 8 12 11 10 13]\n [ 3  6 14  5 14]\n [ 5  4  8  6 13]\n [ 6  3 21  7 21]]\n[[15 15  7  0 23]\n [10 22  4  0 18]\n [ 9  9  7  0 17]\n [11  6  1  0 18]\n [13  7 12  0 26]]\n[[ 3 13 16 16 12]\n [ 1 21 11 17  4]\n [ 4  8 11 16  3]\n [ 3  9  7 16  1]\n [ 0 12 21 22  3]]\n[[ 3 10 12 29  6]\n [ 4 17  9 22  2]\n [ 1  6  8 22  5]\n [ 1  9  6 18  2]\n [ 1 10 14 28  5]]\n[[ 9  8 16 17 10]\n [24  6 13  9  2]\n [ 6  5 10 16  5]\n [ 5  4  8 13  6]\n [ 6  5 18 17 12]]\n[[12  7 13 25  3]\n [25  4 12 13  0]\n [ 5  3 11 20  3]\n [ 8  2  7 16  3]\n [ 7  1 19 29  2]]\n[[10 15  8 24  3]\n [15 17  6 15  1]\n [ 4  7  9 19  3]\n [ 3  7  7 17  2]\n [ 4  4 15 29  6]]\n[[10  5 10 27  8]\n [14  3  9 26  2]\n [ 6  1  9 22  4]\n [ 6  1  7 15  7]\n [ 3  2 16 30  7]]\n[[ 6 12 10  2 30]\n [ 3 12 12  8 19]\n [ 3  6 12  2 19]\n [ 5  4  4  1 22]\n [ 3  7 15  1 32]]\n[[ 8 16 19  7 10]\n [10 15 18 11  0]\n [ 4  9 13  4 12]\n [ 5 10 10  4  7]\n [ 4 11 28  1 14]]\n[[17  6 13  4 20]\n [17  9 11  6 11]\n [ 7  7 12  0 16]\n [10  3  9  2 12]\n [ 9  4 22  0 23]]\n[[11 11 11  4 23]\n [14  6 12  6 16]\n [ 5  7 11  2 17]\n [ 6  3 10  2 15]\n [ 5  2 26  0 25]]\n[[16 12  4  2 26]\n [20  9  4  3 18]\n [ 7  6  6  3 20]\n [ 7  5  3  2 19]\n [ 6  3 10  0 39]]\n[[15 12  0  0 33]\n [18  8  2  2 24]\n [ 7  6  1  2 26]\n [ 7  3  1  2 23]\n [ 7  2  0  0 49]]\n[[ 4 22  3 12 19]\n [ 8 19  4 10 13]\n [ 2 11  4 11 14]\n [ 2 10  4  6 14]\n [ 5  6  4  7 36]]\n[[ 1 30  2 12 15]\n [ 1 30  2  9 12]\n [ 2 15  2 11 12]\n [ 1 11  1 12 11]\n [ 0 10  4 14 30]]\n[[10  2  7 12 29]\n [12  2 12  9 19]\n [ 5  3  8  7 19]\n [ 5  0  5  6 20]\n [ 5  0 17  8 28]]\n[[ 2 12  7 25 14]\n [ 5  8 10 22  9]\n [ 1  5  8 18 10]\n [ 1  5  2 16 12]\n [ 3  2  6 18 29]]\n[[11  6  6 26 11]\n [ 9  6  5 23 11]\n [ 3  3  7 20  9]\n [ 5  3  3 17  8]\n [ 3  1  5 29 20]]\n[[ 7  8 12 20 13]\n [ 4 11 15 13 11]\n [ 1  6 10 11 14]\n [ 3  4  8 10 11]\n [ 0  2 12 10 34]]\n[[ 8  3 11 14 24]\n [10  1 13 13 17]\n [ 4  1  6 14 17]\n [ 6  1  4  6 19]\n [ 3  0  2 16 37]]\n[[16  4  1 27 12]\n [21  3  2 19  9]\n [ 8  3  1 19 11]\n [ 8  3  0 16  9]\n [ 7  0  0 20 31]]\n[[19  9  1 17 14]\n [16 12  0 15 11]\n [ 6  7  0 17 12]\n [ 7  6  0 12 11]\n [ 6  4  0 13 35]]\n[[11  6  3 15 25]\n [15  8  5 11 15]\n [ 5  3  4 13 17]\n [ 5  1  2 11 17]\n [ 5  1  2 12 38]]\n[[13 14  6 13 14]\n [ 8 18  8 10 10]\n [ 9  3  8  9 13]\n [ 8  4  3 11 10]\n [ 4  2  9 11 32]]\n[[ 5 16  2 10 27]\n [ 8 21  2  8 15]\n [ 5  7  3  8 19]\n [ 5  8  1  7 15]\n [ 9  6  4  7 32]]\n[[ 4 15  7  8 26]\n [ 6 18  4  9 17]\n [ 3  7 10  2 20]\n [ 3 10  4  4 15]\n [ 6  7  8  3 34]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 471257f8658cc55c4ec33930066c1c6d1f101821","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:43:44.360228Z","iopub.execute_input":"2024-05-26T13:43:44.360898Z","iopub.status.idle":"2024-05-26T13:43:44.366222Z","shell.execute_reply.started":"2024-05-26T13:43:44.360862Z","shell.execute_reply":"2024-05-26T13:43:44.365064Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"metrics = trainer.evaluate(test_ds)\ntrainer.log_metrics(\"eval\", metrics)\ntrainer.save_metrics(\"eval\", metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}