{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855425c6-7760-4e45-b27a-e1b0865a6cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "from torch.nn.modules.utils import _pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f25aed63-982d-4614-9254-f960272b5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "# !wandb login b248f05b86545578e213a3d77725b1793b6c237a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1dc7e5c-d454-4288-9519-337f1e0529c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ml-collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20699117-784e-4b16-97ca-06bcfac8817b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 23 18:04:54 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  |   00000000:06:00.0 Off |                    0 |\n",
      "| N/A   44C    P0             43W /  300W |       3MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-16GB           On  |   00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   49C    P0             64W /  300W |       3MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c9aaebc-c9bb-4610-92f9-daaeb0bce773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall numpy -y\n",
    "# !pip uninstall matplotlib -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06242382-fa92-43ab-9f8b-1c57927a2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f48f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" # is need to train on 'hachiko'\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from typing import Tuple\n",
    "from typing import List\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# import torchvision.transforms as T\n",
    "import torchvision.transforms.v2 as T\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import ViTImageProcessor\n",
    "from transformers import AutoImageProcessor\n",
    "from transformers import TrainingArguments\n",
    "from transformers import PretrainedConfig\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import Trainer\n",
    "\n",
    "# import of custom functions\n",
    "from validation_utils import get_compute_metrics\n",
    "from data_utils import resample\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088b88de-a140-4441-9738-7a218e2c9429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number CUDA Devices: 1\n",
      "Current cuda device:  0  **May not correspond to nvidia-smi ID above, check visibility parameter\n"
     ]
    }
   ],
   "source": [
    "print('Number CUDA Devices:', torch.cuda.device_count())\n",
    "print ('Current cuda device: ', torch.cuda.current_device(), ' **May not correspond to nvidia-smi ID above, check visibility parameter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0abf1068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.login('897cda038ea791f5f031be1adc101e476e229b31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4198a9ab-6165-45a8-b9c7-d3ac1cd747ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy import ndimage\n",
    "from os.path import join as pjoin\n",
    "\n",
    "\n",
    "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
    "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
    "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
    "FC_0 = \"MlpBlock_3/Dense_0\"\n",
    "FC_1 = \"MlpBlock_3/Dense_1\"\n",
    "ATTENTION_NORM = \"LayerNorm_0\"\n",
    "MLP_NORM = \"LayerNorm_2\"\n",
    "\n",
    "def np2th(weights, conv=False):\n",
    "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
    "    if conv:\n",
    "        weights = weights.transpose([3, 2, 0, 1])\n",
    "    return torch.from_numpy(weights)\n",
    "    \n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, vis, coeff_max=0.25):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.coeff_max = coeff_max\n",
    "\n",
    "        self.vis = vis\n",
    "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
    "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n",
    "\n",
    "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
    "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "        self.softmax2 = Softmax(dim=-2)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, mask=None):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "    \n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / self.sqrt_att_head_size\n",
    "\n",
    "        debug_mode = False\n",
    "        print_info = False\n",
    "\n",
    "\n",
    "        if mask is not None:\n",
    "\n",
    "            if debug_mode:\n",
    "                print_info = True if (random.random() < 0.000001) else False\n",
    "                x = random.random()\n",
    "                if (x > 0.00005) and (x < 0.00007):\n",
    "                    print_info = True\n",
    "                else:\n",
    "                    print_info = False\n",
    "            else:\n",
    "                print_info = False\n",
    "\n",
    "            max_as = torch.max(attention_scores[:, :, 0, :], dim=2, keepdim=False)[0]\n",
    "            # max_as = max_as.to(device='cuda')\n",
    "\n",
    "            if print_info:\n",
    "                print(\"mask before:\", mask)\n",
    "                print(\"attn scores before:\", attention_scores[:, :, 0, :])\n",
    "\n",
    "                print(\"attn scores max_min before:\")\n",
    "                print(max_as, torch.min(attention_scores[:, :, 0, :], dim=2, keepdim=False)[0])\n",
    "\n",
    "                print(torch.topk(attention_scores[:, :, 0, :], 5, largest=True), torch.topk(attention_scores[:, :, 0, :], 5, largest=False))\n",
    "\n",
    "            mask_626 = torch.zeros(mask.size(0), (mask.size(1) + 1)) #, dtype=torch.float64) # dtype=torch.double)\n",
    "            mask_626 = mask_626.to(device='cuda')\n",
    "            mask_626[:, 1:] = mask[:, :]\n",
    "            mask_626[:, 0] = 0\n",
    "\n",
    "            if print_info: print(\"mask626:\", mask_626)\n",
    "            \n",
    "            # positive only, obj + (max * coeff):\n",
    "            attention_scores[:, :, 0, :] = \\\n",
    "                torch.where( mask_626[:, None, :] < 0.5, \\\n",
    "                        torch.add( attention_scores[:, :, 0, :], \\\n",
    "                            torch.mul( max_as[:, :, None] , torch.tensor(self.coeff_max).cuda()) ), \\\n",
    "                        attention_scores[:, :, 0, :] #.float()\n",
    "                            )\n",
    "\n",
    "            if print_info:\n",
    "                print(\"attn scores after:\", attention_scores[:, :, 0, :])\n",
    "\n",
    "                print(\"attn scores max_min after:\")\n",
    "                print(torch.max(attention_scores[:, :, 0, :]), torch.min(attention_scores[:, :, 0, :]))\n",
    "          \n",
    "                print(torch.topk(attention_scores[:, :, 0, :], 5, largest=True), torch.topk(attention_scores[:, :, 0, :], 5, largest=False))\n",
    "\n",
    "\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        \n",
    "        weights = attention_probs if self.vis else None\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "\n",
    "        return attention_output, weights, self.softmax2(attention_scores)[:,:,:,0]\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
    "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
    "        self.act_fn = ACT2FN[\"gelu\"]\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, img_size, in_channels=3):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.hybrid = None\n",
    "        img_size = _pair(img_size)\n",
    "\n",
    "        # EXPERIMENTAL. Overlapping patches:\n",
    "        overlap = False\n",
    "        if overlap: slide = 12 # 14\n",
    "\n",
    "        if config.patches.get(\"grid\") is not None:\n",
    "            grid_size = config.patches[\"grid\"]\n",
    "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
    "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
    "            self.hybrid = True\n",
    "        else:\n",
    "            patch_size = _pair(config.patches[\"size\"])\n",
    "\n",
    "            if overlap:\n",
    "                n_patches = ((img_size[0] - patch_size[0]) // slide + 1) * ((img_size[1] - patch_size[1]) // slide + 1)\n",
    "            else:\n",
    "                n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "\n",
    "            self.hybrid = False\n",
    "\n",
    "        if overlap:\n",
    "            self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
    "                                        out_channels=config.hidden_size,\n",
    "                                        kernel_size=patch_size,\n",
    "                                        stride=(slide, slide) )                 \n",
    "        else:\n",
    "            self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
    "                                        out_channels=config.hidden_size,\n",
    "                                        kernel_size=patch_size,\n",
    "                                        stride=patch_size )\n",
    "\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        x = self.patch_embeddings(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "\n",
    "        embeddings = x + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, vis, coeff_max):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn = Mlp(config)\n",
    "        self.attn = Attention(config, vis, coeff_max)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        h = x\n",
    "        x = self.attention_norm(x)\n",
    "        x, weights, contribution = self.attn(x, mask)\n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + h\n",
    "        return x, weights, contribution\n",
    "\n",
    "    def load_from(self, weights, n_block):\n",
    "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
    "        with torch.no_grad():\n",
    "            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "\n",
    "            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
    "            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
    "            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
    "            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
    "\n",
    "            self.attn.query.weight.copy_(query_weight)\n",
    "            self.attn.key.weight.copy_(key_weight)\n",
    "            self.attn.value.weight.copy_(value_weight)\n",
    "            self.attn.out.weight.copy_(out_weight)\n",
    "            self.attn.query.bias.copy_(query_bias)\n",
    "            self.attn.key.bias.copy_(key_bias)\n",
    "            self.attn.value.bias.copy_(value_bias)\n",
    "            self.attn.out.bias.copy_(out_bias)\n",
    "\n",
    "            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
    "            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
    "            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
    "            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
    "\n",
    "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
    "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
    "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
    "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
    "\n",
    "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
    "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
    "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
    "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, vis, coeff_max):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.layer = nn.ModuleList()\n",
    "        num_layers = config.transformer[\"num_layers\"]\n",
    "\n",
    "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        for _ in range(num_layers):\n",
    "            layer = Block(config, vis, coeff_max)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, hidden_states, mask=None):\n",
    "        attn_weights = []\n",
    "        contributions = []\n",
    "        tokens = [[] for i in range(hidden_states.shape[0])]\n",
    "\n",
    "        for layer_block in self.layer:\n",
    "            hidden_states, weights, contribution = layer_block(hidden_states, mask)\n",
    "\n",
    "            if self.vis:\n",
    "                attn_weights.append(weights)\n",
    "                contributions.append(contribution)\n",
    "\n",
    "        encoded = self.encoder_norm(hidden_states)\n",
    "\n",
    "        return encoded, attn_weights\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, img_size, vis, coeff_max):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embeddings = Embeddings(config, img_size=img_size)\n",
    "        self.encoder = Encoder(config, vis, coeff_max)\n",
    "\n",
    "    def forward(self, input_ids, mask=None):\n",
    "        embedding_output = self.embeddings(input_ids)\n",
    "        encoded, attn_weights = self.encoder(embedding_output, mask)\n",
    "\n",
    "        return encoded, attn_weights\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config, img_size=400, num_classes=200, smoothing_value=0, zero_head=False, vis=False, dataset='CUB', coeff_max=0.25, contr_loss=False, focal_loss=False):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.zero_head = zero_head\n",
    "        self.smoothing_value = smoothing_value\n",
    "        self.classifier = config.classifier\n",
    "        self.dataset=dataset\n",
    "\n",
    "        self.contr_loss = contr_loss\n",
    "        self.focal_loss = focal_loss\n",
    "\n",
    "        self.transformer = Transformer(config, img_size, vis, coeff_max)\n",
    "        self.head = Linear(config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, labels=None, mask=None):\n",
    "        x, attn_weights = self.transformer(x, mask)\n",
    "        print(\"X shape\", x.shape)\n",
    "        logits = self.head(x[:, 0])\n",
    "        print(\"Logits shape\", logits.shape)\n",
    "\n",
    "        if labels is not None:\n",
    "            if self.smoothing_value == 0:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "            else:\n",
    "                loss_fct = LabelSmoothing(self.smoothing_value)\n",
    "\n",
    "            if self.focal_loss: # enforce another type of loss\n",
    "                loss_fct = FocalLoss()\n",
    "\n",
    "            ce_loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
    "\n",
    "            if self.contr_loss:\n",
    "                contrast_loss = con_loss(x[:, 0], labels.view(-1))\n",
    "                loss = ce_loss + contrast_loss\n",
    "            else:\n",
    "                loss = ce_loss # FFVT\n",
    "\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits, attn_weights\n",
    "\n",
    "    def load_from(self, weights):\n",
    "        with torch.no_grad():\n",
    "            if self.zero_head:\n",
    "                nn.init.zeros_(self.head.weight)\n",
    "                nn.init.zeros_(self.head.bias)\n",
    "            else:\n",
    "                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n",
    "                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n",
    "\n",
    "            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
    "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
    "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
    "\n",
    "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
    "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
    "\n",
    "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
    "            posemb_new = self.transformer.embeddings.position_embeddings\n",
    "            if posemb.size() == posemb_new.size():\n",
    "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
    "            else:\n",
    "                print(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
    "                ntok_new = posemb_new.size(1)\n",
    "\n",
    "                if self.classifier == \"token\":\n",
    "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "                    ntok_new -= 1\n",
    "                else:\n",
    "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "\n",
    "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
    "                gs_new = int(np.sqrt(ntok_new))\n",
    "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
    "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
    "\n",
    "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
    "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
    "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
    "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
    "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
    "\n",
    "            for bname, block in self.transformer.encoder.named_children():\n",
    "                if bname.startswith('ff') == False:\n",
    "                    for uname, unit in block.named_children():\n",
    "                        unit.load_from(weights, n_block=uname)\n",
    "\n",
    "            if self.transformer.embeddings.hybrid:\n",
    "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(weights[\"conv_root/kernel\"], conv=True))\n",
    "                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n",
    "                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
    "\n",
    "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
    "                    for uname, unit in block.named_children():\n",
    "                        unit.load_from(weights, n_block=bname, n_unit=uname)\n",
    "\n",
    "import ml_collections\n",
    "\n",
    "def get_l16_config():\n",
    "    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
    "    config.hidden_size = 1024\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 4096\n",
    "    config.transformer.num_heads = 16\n",
    "    config.transformer.num_layers = 24\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    config.classifier = 'token'\n",
    "    config.representation_size = None\n",
    "    return config\n",
    "    \n",
    "def get_b16_config():\n",
    "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
    "    config.hidden_size = 768\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 3072\n",
    "    config.transformer.num_heads = 12\n",
    "    config.transformer.num_layers = 12\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    config.classifier = 'token'\n",
    "    config.representation_size = None\n",
    "    return config\n",
    "\n",
    "cfgs = {\n",
    "    'l16': get_l16_config,\n",
    "    'b16': get_b16_config\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eec60610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to define config file to have an ability to \n",
    "# load pretrained weights received from previos train stage\n",
    "class SMViTConfig(PretrainedConfig):\n",
    "    model_type = \"sm-vit\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 512,\n",
    "        num_classes: int = 5,\n",
    "        smoothing_value: float = 0,\n",
    "        zero_head: bool = True,\n",
    "        vis: bool = False,\n",
    "        coeff_max: float = False,\n",
    "        focal_loss: bool = False,\n",
    "\n",
    "        # config = ml_collections.ConfigDict()\n",
    "        patches: dict = {'size': (16, 16)},\n",
    "        hidden_size: int = 1024,\n",
    "        # transformer = ml_collections.ConfigDict()\n",
    "        mlp_dim: int = 4096,\n",
    "        num_heads: int = 16,\n",
    "        num_layers: int = 24,\n",
    "        attention_dropout_rate: float = 0.0,\n",
    "        dropout_rate: float = 0.1,\n",
    "        classifier: str = 'token',\n",
    "        # representation_size = None\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "        self.smoothing_value = smoothing_value\n",
    "        self.zero_head = zero_head\n",
    "        self.vis = vis\n",
    "        self.coeff_max = coeff_max\n",
    "        self.focal_loss = focal_loss\n",
    "        self.config = 'b16'\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class SMViTClassification(PreTrainedModel):\n",
    "    config_class = SMViTConfig\n",
    "\n",
    "    def __init__(self, config, pretrained=False):\n",
    "        super().__init__(config)\n",
    "\n",
    "        cfg = cfgs[config.config]()\n",
    "\n",
    "        if pretrained is False: # without pretrained weights\n",
    "          print('Initialized with random weights:')\n",
    "          self.model = VisionTransformer(\n",
    "          img_size = config.img_size,\n",
    "          num_classes = config.num_classes,\n",
    "          smoothing_value = config.smoothing_value,\n",
    "          zero_head = config.zero_head,\n",
    "          vis = config.vis,\n",
    "          coeff_max = config.coeff_max,\n",
    "          focal_loss = config.focal_loss,\n",
    "          config = cfg\n",
    "          )\n",
    "\n",
    "        else:\n",
    "            \n",
    "            self.model = VisionTransformer(\n",
    "                img_size = config.img_size,\n",
    "                num_classes = config.num_classes,\n",
    "                smoothing_value = config.smoothing_value,\n",
    "                zero_head = config.zero_head,\n",
    "                vis = config.vis,\n",
    "                coeff_max = config.coeff_max,\n",
    "                focal_loss = config.focal_loss,\n",
    "                config = cfg)\n",
    "            print(\"Load weights\")\n",
    "            self.model.load_from(np.load(\"imagenet21k_ViT-B_16.npz\"))\n",
    "\n",
    "    # FIXME: need to add extraicting mask to original trainer\n",
    "    def forward(self, pixel_values, labels=None, masks = None):\n",
    "        # define function in transformers library maner\n",
    "        # logits = self.model(pixel_values, mask = masks)\n",
    "        if labels is not None:\n",
    "            loss, logits = self.model(pixel_values, labels=labels, mask=masks)\n",
    "            # loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        else:\n",
    "            logits, attn_weights = self.model(pixel_values, labels=labels, mask=masks)\n",
    "            return {\"logits\": logits, \"attn_weights\": attn_weights}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1515bcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights\n",
      "load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1025, 768])\n",
      "load_pretrained: grid-size from 14 to 32\n"
     ]
    }
   ],
   "source": [
    "smvit_pretrained_config = SMViTConfig()\n",
    "model = SMViTClassification(smvit_pretrained_config, pretrained=True)\n",
    "\n",
    "# from transformers import ViTForImageClassification\n",
    "\n",
    "# model = ViTForImageClassification.from_pretrained(\n",
    "#     # 'google/vit-hybrid-base-bit-384',\n",
    "#     'google/vit-base-patch16-384',\n",
    "#     ignore_mismatched_sizes=True,\n",
    "#     num_labels=5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d565d8f1-7608-4ada-ac36-c84e3799b6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape torch.Size([4, 1025, 768])\n",
      "Logits shape torch.Size([4, 5])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_batch = torch.rand(4, 3, 512, 512)\n",
    "output = model(test_batch)\n",
    "print(output['logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442ef868-6fce-4f8e-8b60-bf9a6d4afff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForMaskedImageModeling were not initialized from the model checkpoint at google/vit-base-patch16-384 and are newly initialized: ['decoder.0.bias', 'decoder.0.weight', 'vit.embeddings.mask_token']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoImageProcessor, ViTForMaskedImageModeling\n",
    "\n",
    "# model = ViTForMaskedImageModeling.from_pretrained('google/vit-base-patch16-384')\n",
    "# # num_patches = (model.config.image_size // model.config.patch_size) ** 2\n",
    "# print(model.config.image_size)\n",
    "# print(model.config.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "970c2328-4b6c-4202-8512-2a650f145889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35126, 2)\n",
      "(35126, 2)\n"
     ]
    }
   ],
   "source": [
    "labelsTable = pd.read_csv('../mnt/local/data/kalexu97/trainLabels.csv') # initial table\n",
    "print(labelsTable.shape)\n",
    "print(labelsTable.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00249399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: rewrite path and add mask path\n",
    "\n",
    "# load dataset via csv table\n",
    "labelsTable = pd.read_csv('../mnt/local/data/kalexu97/trainLabels.csv') # initial table\n",
    "\n",
    "error_images = ['15337_left.jpeg', '40764_right.jpeg']\n",
    "    # '15337_left.jpeg',\n",
    "                # '40551_left.jpeg',\n",
    "                # '20289_right.jpeg',\n",
    "                # '27991_right.jpeg',\n",
    "                # '39477_right.jpeg',\n",
    "                # '40758_left.jpeg',\n",
    "                # '17768_left.jpeg']\n",
    "\n",
    "for error_image in error_images:\n",
    "    error_image = error_image[:-5]\n",
    "    labelsTable = labelsTable[labelsTable.image != error_image]\n",
    "\n",
    "# add folder path 'mask_image'\n",
    "root_dir = '../mnt/local/data/kalexu97/processed_train'\n",
    "mask_dir = '../mnt/local/data/kalexu97/saliency_mask/'\n",
    "\n",
    "labelsTable['image_path'] = labelsTable['image'].apply(lambda x: os.path.join(root_dir, x+'.jpeg'))\n",
    "labelsTable['mask_image'] = labelsTable['image'].apply(lambda x: os.path.join(mask_dir, x+'.npy'))\n",
    "labelsTable['label'] = labelsTable['level']\n",
    "labelsTable = labelsTable.drop(columns=['image', 'level'], axis=1)\n",
    "\n",
    "# dataset is spliated to trian and test previously, and is constant for every training process\n",
    "test_dataset = pd.read_csv('test_dataset.csv')\n",
    "test_dataset['image'] = test_dataset['image_path'].apply(lambda x: x[33:])\n",
    "\n",
    "for error_image in error_images:\n",
    "    error_image = error_image\n",
    "    test_dataset = test_dataset[test_dataset.image != error_image]\n",
    "    \n",
    "test_dataset['image_path'] = test_dataset['image'].apply(lambda x: os.path.join(root_dir, x))\n",
    "test_dataset['mask_image'] = test_dataset['image'].apply(lambda x: os.path.join(mask_dir, x[:-5]+'.npy'))\n",
    "\n",
    "# subtract the test_dataset from the full dataset to get the train_dataset\n",
    "df = pd.concat([test_dataset, labelsTable])\n",
    "df = df.reset_index(drop=True)\n",
    "df_gpby = df.groupby(list(['image_path', 'label']))\n",
    "idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "\n",
    "train_dataset = df.reindex(idx).drop(columns=['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5ade9d2a-e079-4329-aa88-b723eed40a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset.image_path.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d10d5f1c-985c-4093-9309-cd82773f5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset.mask_image.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd1ef97c-460d-4f7d-bbd7-b716d0187830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset.image_path.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "783888c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: length: 19460\n",
      "1: length: 19460\n",
      "2: length: 19460\n",
      "3: length: 19460\n",
      "4: length: 19460\n",
      "N_added_rows:  26953\n",
      "N_all_rows:  28099\n",
      "Ratio of used rows:  0.9592156304494822\n"
     ]
    }
   ],
   "source": [
    "# RUS for major classes, ROS for minor classes\n",
    "# number of items in each class is equal to \n",
    "#           ratio * len(most_minor_dataset) \n",
    "\n",
    "# oversampling just repeating minority class items\n",
    "# enought times to be equal to major dataset in size\n",
    "train_dataset = resample(train_dataset, ratio = 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69da06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define preprocessor\n",
    "model_name_or_path = \"./saved_models/MedViT320_tr35_stg1_8bs_lr2e-5_30ep\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name_or_path)\n",
    "\n",
    "size = 512\n",
    "\n",
    "# Pre-Augmetations\n",
    "_transforms_train = T.Compose([\n",
    "    T.RandomHorizontalFlip(p = 0.5),\n",
    "    T.RandomVerticalFlip(p = 0.5),\n",
    "    T.RandomCrop(460, padding_mode='symmetric', pad_if_needed=True),\n",
    "    T.Resize((512, 512), interpolation=T.InterpolationMode.BICUBIC),\n",
    "    # T.TrivialAugmentWide(),\n",
    "    # Sharpness(),\n",
    "    # Blur()\n",
    "])\n",
    "\n",
    "tens2img = T.ToPILImage()\n",
    "img2tens = T.ToTensor()\n",
    "\n",
    "# for some models it is possible to change input size between training stage\n",
    "image_processor.size['height'] = size\n",
    "image_processor.size['width'] = size\n",
    "\n",
    "def load_image(path_image, mask_path, mode):\n",
    "    \"\"\"\n",
    "    The function loads image from path and make Pre-Augmentation.\n",
    "    \"\"\"\n",
    "    # print(path_image)\n",
    "    top_per = 0.4\n",
    "    image = Image.open(path_image)\n",
    "    orig_mask = np.load(mask_path, mmap_mode='r')\n",
    "    orig_mask = torch.from_numpy(orig_mask)\n",
    "\n",
    "    image, orig_mask = _transforms_train(image, tens2img(orig_mask))\n",
    "    orig_mask = img2tens(orig_mask)[0]\n",
    "\n",
    "    # mask_size = int(orig_mask.shape[0] // 16)\n",
    "    mask_size = int(size//16)\n",
    "\n",
    "    transform = T.Resize(mask_size, interpolation=Image.NEAREST)\n",
    "    resized_mask = transform(orig_mask[None, :, :])\n",
    "    # bool_resized_mask = (resized_mask > 0.1)*1 #### CHECK: that it should not be inverse\n",
    "\n",
    "    low_val_in_topl_p1 = torch.topk(resized_mask.flatten(), int(0.4*resized_mask.shape[1]**2)).values[-1]\n",
    "    # low_val_in_topl_p2 = torch.topk(resized_mask.flatten(), int(0.55*resized_mask.shape[1]**2)).values[-1]\n",
    "    \n",
    "    # rand_region_bids = torch.logical_and(resized_mask[0]>low_val_in_topl_p2, resized_mask[0]<low_val_in_topl_p1)\n",
    "    # bool_masked_pos = torch.randint(low=0, high=2, size=(rand_region_bids.shape)).bool()\n",
    "    # rand_region_bids = torch.logical_and(rand_region_bids, bool_masked_pos)\n",
    "    \n",
    "    # final_mask = torch.logical_or(resized_mask[0]>low_val_in_topl_p1, rand_region_bids)\n",
    "    final_mask = resized_mask[0]>low_val_in_topl_p1\n",
    "\n",
    "    mask = torch.flatten(final_mask) \n",
    "\n",
    "\n",
    "    if mode == 'train':\n",
    "        # image = _transforms_train(image)\n",
    "        # FIXME: add trainsforms !!!\n",
    "        return [image, mask]\n",
    "        \n",
    "    else:\n",
    "        # image = _transforms_test(image)\n",
    "        return [image, mask]\n",
    "\n",
    "\n",
    "def func_transform(examples):\n",
    "    \"\"\"\n",
    "    The function is used to preprocess train dataset.\n",
    "    \"\"\"\n",
    "    # pre-augmentation and preprocessing\n",
    "    transformed_inputs = [load_image(path_img, path_msk, 'train') for path_img, path_msk in zip(examples['image_path'], examples['mask_image'])]\n",
    "    images = [item[0] for item in transformed_inputs]\n",
    "    masks = [item[1] for item in transformed_inputs]\n",
    "\n",
    "    # print(masks)\n",
    "    # print(images)\n",
    "    # inputs = image_processor([load_image(path, lb, 'train')\n",
    "                                # for path, lb in zip(examples['image_path'], examples['label'])], return_tensors='pt')\n",
    "    inputs = image_processor(images, return_tensors='pt')\n",
    "    # print(inputs)\n",
    "    # print(masks)\n",
    "    inputs['mask'] = masks\n",
    "    inputs['label'] = examples['label']\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def func_transform_test(examples):\n",
    "    \"\"\"\n",
    "    The function is used to preprocess test dataset.\n",
    "    \"\"\"\n",
    "    # pre-augmentation and preprocessing\n",
    "    transformed_inputs = [load_image(path_img, path_msk, 'test') for path_img, path_msk in zip(examples['image_path'], examples['mask_image'])]\n",
    "    images = [item[0] for item in transformed_inputs]\n",
    "    masks = [item[1] for item in transformed_inputs]\n",
    "    \n",
    "    inputs = image_processor(images, return_tensors='pt')\n",
    "    inputs['mask'] = masks\n",
    "    inputs['label'] = examples['label']\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# to dataset\n",
    "train_ds = Dataset.from_pandas(train_dataset, preserve_index=False)\n",
    "test_ds = Dataset.from_pandas(test_dataset, preserve_index=False)\n",
    "\n",
    "# apply preprocessing\n",
    "prepared_ds_train = train_ds.with_transform(func_transform)\n",
    "prepared_ds_test = test_ds.with_transform(func_transform_test)\n",
    "\n",
    "# for sorted datasets shuffling can be usefull\n",
    "prepared_ds_train = prepared_ds_train.shuffle(seed=42)\n",
    "prepared_ds_test = prepared_ds_test.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfb041be-062b-4ec5-8ba0-34dd61ca3a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepared_ds_train[[0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b613308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to define collate function\n",
    "def collate_fn(batch):\n",
    "    # print([x['mask'] for x in batch])\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch]),\n",
    "        'masks': torch.stack([x['mask'] for x in batch]),\n",
    "        # 'masks': None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6789b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset is alse defined previously, so we just need to load its indexes\n",
    "# with open('test_indeces.npy', 'rb') as f:\n",
    "#     sample_ids = np.load(f)\n",
    "#     inv_sample_ids = np.load(f)\n",
    "\n",
    "sample_ids = np.random.choice(len(prepared_ds_test), size=1000, replace=False)\n",
    "inv_sample_ids = np.setdiff1d(np.arange(len(prepared_ds_test)), sample_ids)\n",
    "\n",
    "val_ds = prepared_ds_test.select(sample_ids)\n",
    "test_ds = prepared_ds_test.select(inv_sample_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e29c2c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name is used to log metadata in wandb for tracking\n",
    "r_name = \"SM512Pr_Mask04_bs8_10ep\"\n",
    "\n",
    "# define the function to compute metrics\n",
    "compute_metrics = get_compute_metrics(r_name, 'EyE', save_cm=False)\n",
    "\n",
    "# arguments for training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./SMViT-withMask-04rnd055\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    "    run_name=r_name,  # name of the W&B run (optional)\n",
    "    \n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers = 16,\n",
    "    # lr_scheduler_type = 'constant_with_warmup', # 'constant', 'cosine'\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    # label_smoothing_factor = 0.6,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0.02,\n",
    "    \n",
    "    metric_for_best_model=\"kappa\", # select the best model via metric kappa\n",
    "    greater_is_better = True,\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds_train,\n",
    "    eval_dataset=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8ca474a-0773-4be5-ae7c-97a1cefee1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../mnt/local/data/kalexu97/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "243589ef-ce83-4532-b5bd-e21682fdea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../mnt/local/data/kalexu97/processed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bc661ee-4355-4cf8-830b-f42db821c5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35124\n"
     ]
    }
   ],
   "source": [
    "!ls ../mnt/local/data/kalexu97/processed_train -1 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8afe5959-36c9-48e5-8cd6-494ef79acd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35126\n"
     ]
    }
   ],
   "source": [
    "!ls ../mnt/local/data/kalexu97/train -1 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f124a850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexu97\u001b[0m (\u001b[33malexu97-skoltech\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/models/wandb/run-20240716_101927-vizavexx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexu97-skoltech/huggingface/runs/vizavexx' target=\"_blank\">SM512Pr_Mask04_bs8_10ep</a></strong> to <a href='https://wandb.ai/alexu97-skoltech/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexu97-skoltech/huggingface' target=\"_blank\">https://wandb.ai/alexu97-skoltech/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexu97-skoltech/huggingface/runs/vizavexx' target=\"_blank\">https://wandb.ai/alexu97-skoltech/huggingface/runs/vizavexx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5501' max='30400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5501/30400 5:59:07 < 27:06:03, 0.26 it/s, Epoch 1.81/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Class 0</th>\n",
       "      <th>Class 1</th>\n",
       "      <th>Class 2</th>\n",
       "      <th>Class 3</th>\n",
       "      <th>Class 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.609400</td>\n",
       "      <td>1.609355</td>\n",
       "      <td>0.158000</td>\n",
       "      <td>0.024705</td>\n",
       "      <td>0.074782</td>\n",
       "      <td>0.634044</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.933000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>0.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.608400</td>\n",
       "      <td>1.605865</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.188696</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.669936</td>\n",
       "      <td>0.276000</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.969000</td>\n",
       "      <td>0.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.602500</td>\n",
       "      <td>1.596006</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.292862</td>\n",
       "      <td>0.019616</td>\n",
       "      <td>0.698309</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.837000</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.589100</td>\n",
       "      <td>1.575572</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.293250</td>\n",
       "      <td>0.028336</td>\n",
       "      <td>0.699300</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.564800</td>\n",
       "      <td>1.545631</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.311681</td>\n",
       "      <td>0.020143</td>\n",
       "      <td>0.718975</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>0.849000</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.534700</td>\n",
       "      <td>1.534902</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.265629</td>\n",
       "      <td>0.013247</td>\n",
       "      <td>0.691915</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.327000</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.497000</td>\n",
       "      <td>1.482901</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.293065</td>\n",
       "      <td>0.016013</td>\n",
       "      <td>0.709456</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.473200</td>\n",
       "      <td>1.468750</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.291671</td>\n",
       "      <td>0.013485</td>\n",
       "      <td>0.690130</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.283000</td>\n",
       "      <td>0.847000</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.440700</td>\n",
       "      <td>1.482553</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.229803</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>0.697875</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.941000</td>\n",
       "      <td>0.749000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.418300</td>\n",
       "      <td>1.360159</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.395734</td>\n",
       "      <td>0.015134</td>\n",
       "      <td>0.714610</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.370900</td>\n",
       "      <td>1.290379</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.560284</td>\n",
       "      <td>0.652439</td>\n",
       "      <td>0.736747</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.933000</td>\n",
       "      <td>0.851000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.923000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.320600</td>\n",
       "      <td>1.264991</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.554035</td>\n",
       "      <td>0.649263</td>\n",
       "      <td>0.740391</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.933000</td>\n",
       "      <td>0.847000</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.324000</td>\n",
       "      <td>1.288215</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.430488</td>\n",
       "      <td>0.033746</td>\n",
       "      <td>0.734872</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.911000</td>\n",
       "      <td>0.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.293000</td>\n",
       "      <td>1.265372</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.449187</td>\n",
       "      <td>0.054678</td>\n",
       "      <td>0.748918</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.917000</td>\n",
       "      <td>0.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.248100</td>\n",
       "      <td>1.171919</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.647462</td>\n",
       "      <td>0.697529</td>\n",
       "      <td>0.770182</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.937000</td>\n",
       "      <td>0.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.217900</td>\n",
       "      <td>1.163115</td>\n",
       "      <td>0.729000</td>\n",
       "      <td>0.609129</td>\n",
       "      <td>0.676412</td>\n",
       "      <td>0.779999</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.937000</td>\n",
       "      <td>0.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.222400</td>\n",
       "      <td>1.209916</td>\n",
       "      <td>0.653000</td>\n",
       "      <td>0.556334</td>\n",
       "      <td>0.661829</td>\n",
       "      <td>0.772510</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.887000</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.976000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.185800</td>\n",
       "      <td>1.121252</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.644471</td>\n",
       "      <td>0.702522</td>\n",
       "      <td>0.788321</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.849000</td>\n",
       "      <td>0.941000</td>\n",
       "      <td>0.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.178600</td>\n",
       "      <td>1.101940</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.665847</td>\n",
       "      <td>0.724609</td>\n",
       "      <td>0.788135</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.887000</td>\n",
       "      <td>0.857000</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.151800</td>\n",
       "      <td>1.092346</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.660689</td>\n",
       "      <td>0.704269</td>\n",
       "      <td>0.796213</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.851000</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.122900</td>\n",
       "      <td>1.093632</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.652569</td>\n",
       "      <td>0.698499</td>\n",
       "      <td>0.791597</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.939000</td>\n",
       "      <td>0.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.126500</td>\n",
       "      <td>1.138038</td>\n",
       "      <td>0.536000</td>\n",
       "      <td>0.582066</td>\n",
       "      <td>0.595769</td>\n",
       "      <td>0.794895</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>0.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.101600</td>\n",
       "      <td>1.146851</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>0.555926</td>\n",
       "      <td>0.522694</td>\n",
       "      <td>0.791349</td>\n",
       "      <td>0.564000</td>\n",
       "      <td>0.641000</td>\n",
       "      <td>0.773000</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.089100</td>\n",
       "      <td>1.089224</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.620176</td>\n",
       "      <td>0.602910</td>\n",
       "      <td>0.805526</td>\n",
       "      <td>0.641000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.074900</td>\n",
       "      <td>1.072790</td>\n",
       "      <td>0.579000</td>\n",
       "      <td>0.651149</td>\n",
       "      <td>0.634985</td>\n",
       "      <td>0.810389</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.937000</td>\n",
       "      <td>0.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.058100</td>\n",
       "      <td>1.073073</td>\n",
       "      <td>0.639000</td>\n",
       "      <td>0.645881</td>\n",
       "      <td>0.666553</td>\n",
       "      <td>0.801255</td>\n",
       "      <td>0.734000</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.837000</td>\n",
       "      <td>0.949000</td>\n",
       "      <td>0.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.032400</td>\n",
       "      <td>1.074645</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.598855</td>\n",
       "      <td>0.554327</td>\n",
       "      <td>0.791906</td>\n",
       "      <td>0.597000</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.035500</td>\n",
       "      <td>1.077967</td>\n",
       "      <td>0.583000</td>\n",
       "      <td>0.620882</td>\n",
       "      <td>0.638424</td>\n",
       "      <td>0.800516</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.027000</td>\n",
       "      <td>1.095464</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>0.541901</td>\n",
       "      <td>0.413125</td>\n",
       "      <td>0.791151</td>\n",
       "      <td>0.469000</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>0.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.014100</td>\n",
       "      <td>1.046085</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>0.652176</td>\n",
       "      <td>0.641868</td>\n",
       "      <td>0.808601</td>\n",
       "      <td>0.695000</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.839000</td>\n",
       "      <td>0.947000</td>\n",
       "      <td>0.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.998200</td>\n",
       "      <td>1.088087</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>0.599557</td>\n",
       "      <td>0.622450</td>\n",
       "      <td>0.789177</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.983100</td>\n",
       "      <td>1.075434</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.571420</td>\n",
       "      <td>0.486760</td>\n",
       "      <td>0.808229</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.568000</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.943000</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.962200</td>\n",
       "      <td>1.033573</td>\n",
       "      <td>0.556000</td>\n",
       "      <td>0.626915</td>\n",
       "      <td>0.620924</td>\n",
       "      <td>0.813830</td>\n",
       "      <td>0.658000</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.949000</td>\n",
       "      <td>0.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.959800</td>\n",
       "      <td>1.016966</td>\n",
       "      <td>0.616000</td>\n",
       "      <td>0.657408</td>\n",
       "      <td>0.666163</td>\n",
       "      <td>0.806397</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.908500</td>\n",
       "      <td>1.025765</td>\n",
       "      <td>0.629000</td>\n",
       "      <td>0.669889</td>\n",
       "      <td>0.670427</td>\n",
       "      <td>0.795802</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>0.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.922800</td>\n",
       "      <td>1.001578</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.670700</td>\n",
       "      <td>0.715780</td>\n",
       "      <td>0.801263</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.857000</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.921600</td>\n",
       "      <td>0.992566</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.669090</td>\n",
       "      <td>0.703381</td>\n",
       "      <td>0.813943</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>0.813000</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.954000</td>\n",
       "      <td>0.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.930200</td>\n",
       "      <td>1.072870</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>0.561767</td>\n",
       "      <td>0.575093</td>\n",
       "      <td>0.801357</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>0.987284</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.681693</td>\n",
       "      <td>0.710799</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.985570</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>0.683674</td>\n",
       "      <td>0.692604</td>\n",
       "      <td>0.822128</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.881500</td>\n",
       "      <td>1.035375</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.644316</td>\n",
       "      <td>0.660355</td>\n",
       "      <td>0.804273</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.942000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.850900</td>\n",
       "      <td>1.039503</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.621805</td>\n",
       "      <td>0.620648</td>\n",
       "      <td>0.798681</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.949000</td>\n",
       "      <td>0.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.883100</td>\n",
       "      <td>1.004034</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.655252</td>\n",
       "      <td>0.674220</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.868500</td>\n",
       "      <td>1.014120</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>0.626994</td>\n",
       "      <td>0.663430</td>\n",
       "      <td>0.809141</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.783000</td>\n",
       "      <td>0.813000</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.858400</td>\n",
       "      <td>1.007559</td>\n",
       "      <td>0.562000</td>\n",
       "      <td>0.628445</td>\n",
       "      <td>0.625117</td>\n",
       "      <td>0.823262</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>0.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.849100</td>\n",
       "      <td>1.032268</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>0.577911</td>\n",
       "      <td>0.806137</td>\n",
       "      <td>0.614000</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.825900</td>\n",
       "      <td>1.054752</td>\n",
       "      <td>0.517000</td>\n",
       "      <td>0.591340</td>\n",
       "      <td>0.583610</td>\n",
       "      <td>0.805479</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.941000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.813700</td>\n",
       "      <td>0.991535</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.649471</td>\n",
       "      <td>0.682163</td>\n",
       "      <td>0.814100</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>0.954000</td>\n",
       "      <td>0.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>1.008518</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.630103</td>\n",
       "      <td>0.683614</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>0.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.778600</td>\n",
       "      <td>0.986263</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.651681</td>\n",
       "      <td>0.710027</td>\n",
       "      <td>0.808932</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.849000</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.812100</td>\n",
       "      <td>0.960225</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.688840</td>\n",
       "      <td>0.731626</td>\n",
       "      <td>0.802624</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.825500</td>\n",
       "      <td>1.056627</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.561864</td>\n",
       "      <td>0.585952</td>\n",
       "      <td>0.796423</td>\n",
       "      <td>0.614000</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.776400</td>\n",
       "      <td>0.976244</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.661446</td>\n",
       "      <td>0.691140</td>\n",
       "      <td>0.817218</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.779700</td>\n",
       "      <td>1.058018</td>\n",
       "      <td>0.639000</td>\n",
       "      <td>0.612697</td>\n",
       "      <td>0.672019</td>\n",
       "      <td>0.798966</td>\n",
       "      <td>0.734000</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.927000</td>\n",
       "      <td>0.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.785500</td>\n",
       "      <td>1.094403</td>\n",
       "      <td>0.551000</td>\n",
       "      <td>0.551292</td>\n",
       "      <td>0.606481</td>\n",
       "      <td>0.799101</td>\n",
       "      <td>0.646000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.734000</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.785100</td>\n",
       "      <td>0.941202</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.706369</td>\n",
       "      <td>0.732747</td>\n",
       "      <td>0.825182</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.726100</td>\n",
       "      <td>1.038710</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>0.600563</td>\n",
       "      <td>0.661668</td>\n",
       "      <td>0.805335</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.759700</td>\n",
       "      <td>0.968765</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.649845</td>\n",
       "      <td>0.682857</td>\n",
       "      <td>0.819626</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.833000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.745900</td>\n",
       "      <td>0.988045</td>\n",
       "      <td>0.635000</td>\n",
       "      <td>0.645092</td>\n",
       "      <td>0.679008</td>\n",
       "      <td>0.805145</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.726800</td>\n",
       "      <td>1.031621</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.622824</td>\n",
       "      <td>0.670539</td>\n",
       "      <td>0.812630</td>\n",
       "      <td>0.719000</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.954000</td>\n",
       "      <td>0.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.720900</td>\n",
       "      <td>0.992657</td>\n",
       "      <td>0.548000</td>\n",
       "      <td>0.638674</td>\n",
       "      <td>0.619467</td>\n",
       "      <td>0.816184</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.669000</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.699100</td>\n",
       "      <td>0.969184</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.677214</td>\n",
       "      <td>0.721152</td>\n",
       "      <td>0.820998</td>\n",
       "      <td>0.771000</td>\n",
       "      <td>0.837000</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.730100</td>\n",
       "      <td>0.990792</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.678741</td>\n",
       "      <td>0.707912</td>\n",
       "      <td>0.800590</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.704900</td>\n",
       "      <td>1.076486</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.520376</td>\n",
       "      <td>0.537129</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.559000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.771000</td>\n",
       "      <td>0.954000</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.699200</td>\n",
       "      <td>1.013206</td>\n",
       "      <td>0.582000</td>\n",
       "      <td>0.642034</td>\n",
       "      <td>0.641465</td>\n",
       "      <td>0.797356</td>\n",
       "      <td>0.681000</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>1.047673</td>\n",
       "      <td>0.579000</td>\n",
       "      <td>0.591832</td>\n",
       "      <td>0.635263</td>\n",
       "      <td>0.797810</td>\n",
       "      <td>0.667000</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.949000</td>\n",
       "      <td>0.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.664700</td>\n",
       "      <td>1.008381</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>0.638804</td>\n",
       "      <td>0.649049</td>\n",
       "      <td>0.808867</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.675100</td>\n",
       "      <td>1.007018</td>\n",
       "      <td>0.568000</td>\n",
       "      <td>0.616126</td>\n",
       "      <td>0.629646</td>\n",
       "      <td>0.805004</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.657800</td>\n",
       "      <td>0.946983</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.684146</td>\n",
       "      <td>0.722941</td>\n",
       "      <td>0.816967</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.851000</td>\n",
       "      <td>0.961000</td>\n",
       "      <td>0.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.625900</td>\n",
       "      <td>0.974647</td>\n",
       "      <td>0.637000</td>\n",
       "      <td>0.648032</td>\n",
       "      <td>0.680546</td>\n",
       "      <td>0.811940</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.656600</td>\n",
       "      <td>1.141334</td>\n",
       "      <td>0.377000</td>\n",
       "      <td>0.475458</td>\n",
       "      <td>0.431886</td>\n",
       "      <td>0.797137</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.972793</td>\n",
       "      <td>0.653000</td>\n",
       "      <td>0.631878</td>\n",
       "      <td>0.688983</td>\n",
       "      <td>0.828733</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.959676</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.651994</td>\n",
       "      <td>0.705684</td>\n",
       "      <td>0.825719</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.833000</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>0.918660</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.679870</td>\n",
       "      <td>0.737736</td>\n",
       "      <td>0.822237</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.963000</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.647100</td>\n",
       "      <td>0.957407</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.699173</td>\n",
       "      <td>0.739574</td>\n",
       "      <td>0.802630</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.653300</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.683078</td>\n",
       "      <td>0.735381</td>\n",
       "      <td>0.810827</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.604200</td>\n",
       "      <td>0.918962</td>\n",
       "      <td>0.734000</td>\n",
       "      <td>0.688223</td>\n",
       "      <td>0.740462</td>\n",
       "      <td>0.820309</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.961000</td>\n",
       "      <td>0.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.633600</td>\n",
       "      <td>0.949288</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.677506</td>\n",
       "      <td>0.719559</td>\n",
       "      <td>0.825220</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.617200</td>\n",
       "      <td>0.982182</td>\n",
       "      <td>0.629000</td>\n",
       "      <td>0.644179</td>\n",
       "      <td>0.677803</td>\n",
       "      <td>0.814118</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.773000</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.622800</td>\n",
       "      <td>0.974855</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.650296</td>\n",
       "      <td>0.685797</td>\n",
       "      <td>0.807862</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.833000</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.616700</td>\n",
       "      <td>0.941621</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.680464</td>\n",
       "      <td>0.728641</td>\n",
       "      <td>0.819113</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.606200</td>\n",
       "      <td>0.964654</td>\n",
       "      <td>0.674000</td>\n",
       "      <td>0.662639</td>\n",
       "      <td>0.708701</td>\n",
       "      <td>0.809841</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.961000</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.632800</td>\n",
       "      <td>0.957405</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.688511</td>\n",
       "      <td>0.714238</td>\n",
       "      <td>0.820847</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>0.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.613600</td>\n",
       "      <td>1.030080</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>0.588254</td>\n",
       "      <td>0.690392</td>\n",
       "      <td>0.814111</td>\n",
       "      <td>0.729000</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.915586</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.714383</td>\n",
       "      <td>0.744920</td>\n",
       "      <td>0.818102</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.837000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>0.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.592600</td>\n",
       "      <td>0.942382</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>0.659377</td>\n",
       "      <td>0.718226</td>\n",
       "      <td>0.822382</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.963000</td>\n",
       "      <td>0.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.562000</td>\n",
       "      <td>1.050547</td>\n",
       "      <td>0.588000</td>\n",
       "      <td>0.611705</td>\n",
       "      <td>0.641440</td>\n",
       "      <td>0.798983</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.575900</td>\n",
       "      <td>0.990256</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.597327</td>\n",
       "      <td>0.592306</td>\n",
       "      <td>0.817802</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.662000</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>0.961000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.565300</td>\n",
       "      <td>1.047395</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>0.535830</td>\n",
       "      <td>0.494340</td>\n",
       "      <td>0.816333</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>0.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.570900</td>\n",
       "      <td>1.003287</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.582357</td>\n",
       "      <td>0.579657</td>\n",
       "      <td>0.813548</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.967000</td>\n",
       "      <td>0.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.598900</td>\n",
       "      <td>0.948944</td>\n",
       "      <td>0.582000</td>\n",
       "      <td>0.650756</td>\n",
       "      <td>0.653048</td>\n",
       "      <td>0.837223</td>\n",
       "      <td>0.659000</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.967000</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.586300</td>\n",
       "      <td>0.979331</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.607837</td>\n",
       "      <td>0.634792</td>\n",
       "      <td>0.815698</td>\n",
       "      <td>0.653000</td>\n",
       "      <td>0.734000</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.961000</td>\n",
       "      <td>0.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.536600</td>\n",
       "      <td>0.990788</td>\n",
       "      <td>0.604000</td>\n",
       "      <td>0.618645</td>\n",
       "      <td>0.658775</td>\n",
       "      <td>0.806496</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.599500</td>\n",
       "      <td>1.005528</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>0.585743</td>\n",
       "      <td>0.549851</td>\n",
       "      <td>0.814851</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.961000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.575300</td>\n",
       "      <td>1.037913</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.520581</td>\n",
       "      <td>0.443533</td>\n",
       "      <td>0.816880</td>\n",
       "      <td>0.469000</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.961000</td>\n",
       "      <td>0.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.584200</td>\n",
       "      <td>1.006867</td>\n",
       "      <td>0.574000</td>\n",
       "      <td>0.613919</td>\n",
       "      <td>0.635989</td>\n",
       "      <td>0.792678</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.576600</td>\n",
       "      <td>0.961616</td>\n",
       "      <td>0.671000</td>\n",
       "      <td>0.668038</td>\n",
       "      <td>0.708701</td>\n",
       "      <td>0.810480</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.571800</td>\n",
       "      <td>1.075773</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.519533</td>\n",
       "      <td>0.514864</td>\n",
       "      <td>0.805160</td>\n",
       "      <td>0.534000</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.574900</td>\n",
       "      <td>0.989724</td>\n",
       "      <td>0.527000</td>\n",
       "      <td>0.605667</td>\n",
       "      <td>0.601184</td>\n",
       "      <td>0.809308</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.849000</td>\n",
       "      <td>0.961000</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>1.045710</td>\n",
       "      <td>0.523000</td>\n",
       "      <td>0.567039</td>\n",
       "      <td>0.589876</td>\n",
       "      <td>0.808108</td>\n",
       "      <td>0.603000</td>\n",
       "      <td>0.673000</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.963000</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.527100</td>\n",
       "      <td>0.967668</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.652895</td>\n",
       "      <td>0.683695</td>\n",
       "      <td>0.814724</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.555700</td>\n",
       "      <td>1.006687</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>0.578032</td>\n",
       "      <td>0.636066</td>\n",
       "      <td>0.808179</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.538500</td>\n",
       "      <td>0.975837</td>\n",
       "      <td>0.537000</td>\n",
       "      <td>0.591300</td>\n",
       "      <td>0.604676</td>\n",
       "      <td>0.824892</td>\n",
       "      <td>0.612000</td>\n",
       "      <td>0.667000</td>\n",
       "      <td>0.839000</td>\n",
       "      <td>0.967000</td>\n",
       "      <td>0.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.540900</td>\n",
       "      <td>1.028056</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>0.578781</td>\n",
       "      <td>0.624115</td>\n",
       "      <td>0.803588</td>\n",
       "      <td>0.639000</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.813000</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.518800</td>\n",
       "      <td>1.096268</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>0.523532</td>\n",
       "      <td>0.538343</td>\n",
       "      <td>0.814573</td>\n",
       "      <td>0.557000</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.513900</td>\n",
       "      <td>0.991744</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.646808</td>\n",
       "      <td>0.672117</td>\n",
       "      <td>0.809559</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.537600</td>\n",
       "      <td>1.037699</td>\n",
       "      <td>0.602000</td>\n",
       "      <td>0.580978</td>\n",
       "      <td>0.650683</td>\n",
       "      <td>0.800568</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>0.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.537700</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.627636</td>\n",
       "      <td>0.686319</td>\n",
       "      <td>0.814535</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>0.966000</td>\n",
       "      <td>0.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.556000</td>\n",
       "      <td>1.059371</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.561955</td>\n",
       "      <td>0.574337</td>\n",
       "      <td>0.824945</td>\n",
       "      <td>0.594000</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.987000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 75/125 00:19 < 00:13, 3.82 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 17   0 633   0  81]\n",
      " [  0   0  60   0   7]\n",
      " [  5   0 131   0  14]\n",
      " [  0   0  28   0   4]\n",
      " [  0   0  10   0  10]]\n",
      "[[  8 616  52   0  55]\n",
      " [  0  59   6   0   2]\n",
      " [  1 104  28   0  17]\n",
      " [  0  15   8   2   7]\n",
      " [  0   4   2   1  13]]\n",
      "[[  0 663  11   4  53]\n",
      " [  0  65   1   0   1]\n",
      " [  0 107   2   9  32]\n",
      " [  0  12   3   1  16]\n",
      " [  0   1   0   1  18]]\n",
      "[[  1 651   9   8  62]\n",
      " [  0  62   1   1   3]\n",
      " [  0  99   7   2  42]\n",
      " [  0  10   2   0  20]\n",
      " [  0   1   0   0  19]]\n",
      "[[  0 662   3  13  53]\n",
      " [  0  64   0   1   2]\n",
      " [  0  96   3   9  42]\n",
      " [  0  12   1   0  19]\n",
      " [  0   1   0   0  19]]\n",
      "[[  0 593   3  21 114]\n",
      " [  0  55   0   1  11]\n",
      " [  0  65   0  11  74]\n",
      " [  0   3   2   1  26]\n",
      " [  0   0   0   1  19]]\n",
      "[[  0 642   0   9  80]\n",
      " [  0  62   0   0   5]\n",
      " [  0  87   0   4  59]\n",
      " [  0   8   0   2  22]\n",
      " [  0   1   0   0  19]]\n",
      "[[  0 624   2   7  98]\n",
      " [  0  54   1   1  11]\n",
      " [  0  74   0   2  74]\n",
      " [  0   6   0   1  25]\n",
      " [  0   0   0   0  20]]\n",
      "[[  0 568   7  21 135]\n",
      " [  0  51   0   2  14]\n",
      " [  0  73   0   4  73]\n",
      " [  0   3   0   1  28]\n",
      " [  0   0   0   1  19]]\n",
      "[[  0 688   0  12  31]\n",
      " [  0  62   0   2   3]\n",
      " [  0  91   0   9  50]\n",
      " [  0   7   0   1  24]\n",
      " [  0   2   0   0  18]]\n",
      "[[707   0   1   6  17]\n",
      " [ 67   0   0   0   0]\n",
      " [ 99   0   2  13  36]\n",
      " [  9   0   0   2  21]\n",
      " [  2   0   0   1  17]]\n",
      "[[704   0   3   8  16]\n",
      " [ 66   0   0   0   1]\n",
      " [103   0   2  10  35]\n",
      " [ 10   0   0   1  21]\n",
      " [  0   0   2   0  18]]\n",
      "[[  0 659  33  26  13]\n",
      " [  0  55   8   3   1]\n",
      " [  0  76  10  32  32]\n",
      " [  0   4   1   5  22]\n",
      " [  0   0   0   1  19]]\n",
      "[[  0 649  55  18   9]\n",
      " [  0  51  13   3   0]\n",
      " [  0  62  27  37  24]\n",
      " [  0   2   3   9  18]\n",
      " [  0   0   0   2  18]]\n",
      "[[699  11   7   7   7]\n",
      " [ 60   6   1   0   0]\n",
      " [ 85   6  16  29  14]\n",
      " [  5   0   5   7  15]\n",
      " [  0   0   1   2  17]]\n",
      "[[693   9  11  11   7]\n",
      " [ 61   2   4   0   0]\n",
      " [ 90   5  12  23  20]\n",
      " [  6   1   2   5  18]\n",
      " [  1   0   0   2  17]]\n",
      "[[580  39  81  26   5]\n",
      " [ 51   4  10   2   0]\n",
      " [ 51  10  36  46   7]\n",
      " [  3   0   4  19   6]\n",
      " [  0   1   0   5  14]]\n",
      "[[686  17  14   7   7]\n",
      " [ 60   4   3   0   0]\n",
      " [ 78  10  20  33   9]\n",
      " [  5   0   4  17   6]\n",
      " [  0   0   0   4  16]]\n",
      "[[665  42  19   2   3]\n",
      " [ 53  10   4   0   0]\n",
      " [ 72  14  35  26   3]\n",
      " [  4   0   5  22   1]\n",
      " [  1   0   0   7  12]]\n",
      "[[638  62  25   2   4]\n",
      " [ 54  10   3   0   0]\n",
      " [ 61  19  33  31   6]\n",
      " [  3   2   4  19   4]\n",
      " [  1   0   0   8  11]]\n",
      "[[645  45  29   6   6]\n",
      " [ 55   9   3   0   0]\n",
      " [ 60  22  27  33   8]\n",
      " [  3   0   7  16   6]\n",
      " [  0   0   0   6  14]]\n",
      "[[444 190  80  13   4]\n",
      " [ 40  18   8   1   0]\n",
      " [ 20  38  36  51   5]\n",
      " [  2   0   4  25   1]\n",
      " [  0   0   1   6  13]]\n",
      "[[329 287 106   6   3]\n",
      " [ 22  30  15   0   0]\n",
      " [ 11  34  51  47   7]\n",
      " [  0   1   7  24   0]\n",
      " [  1   0   0   4  15]]\n",
      "[[422 243  61   4   1]\n",
      " [ 33  22  11   1   0]\n",
      " [ 17  47  50  34   2]\n",
      " [  0   5   4  22   1]\n",
      " [  0   0   2   5  13]]\n",
      "[[486 190  48   6   1]\n",
      " [ 39  21   7   0   0]\n",
      " [ 24  44  37  41   4]\n",
      " [  0   4   5  22   1]\n",
      " [  0   0   1   6  13]]\n",
      "[[545 137  38   2   9]\n",
      " [ 39  24   4   0   0]\n",
      " [ 40  33  30  37  10]\n",
      " [  1   2   1  24   4]\n",
      " [  0   0   0   4  16]]\n",
      "[[369 314  39   1   8]\n",
      " [ 24  35   7   1   0]\n",
      " [ 17  56  39  27  11]\n",
      " [  0   5   4  15   8]\n",
      " [  0   0   1   2  17]]\n",
      "[[476 183  60   6   6]\n",
      " [ 30  25  10   1   1]\n",
      " [ 21  42  44  36   7]\n",
      " [  1   1   6  23   1]\n",
      " [  0   0   1   4  15]]\n",
      "[[220 437  66   4   4]\n",
      " [  9  45  12   0   1]\n",
      " [ 11  46  44  42   7]\n",
      " [  0   3   5  20   4]\n",
      " [  0   0   1   4  15]]\n",
      "[[484 200  38   4   5]\n",
      " [ 36  24   7   0   0]\n",
      " [ 21  49  39  36   5]\n",
      " [  1   2   4  24   1]\n",
      " [  0   0   1   5  14]]\n",
      "[[465 176  74  10   6]\n",
      " [ 32  19  15   1   0]\n",
      " [ 25  31  44  46   4]\n",
      " [  1   1   3  25   2]\n",
      " [  0   0   0   6  14]]\n",
      "[[269 370  89   1   2]\n",
      " [ 16  37  13   1   0]\n",
      " [  8  31  70  41   0]\n",
      " [  0   1   6  25   0]\n",
      " [  0   0   1   7  12]]\n",
      "[[437 220  70   2   2]\n",
      " [ 31  30   6   0   0]\n",
      " [ 17  43  53  35   2]\n",
      " [  0   4   3  24   1]\n",
      " [  0   0   2   6  12]]\n",
      "[[509 172  47   1   2]\n",
      " [ 37  26   4   0   0]\n",
      " [ 23  48  48  28   3]\n",
      " [  2   2   5  20   3]\n",
      " [  1   0   2   4  13]]\n",
      "[[525 143  58   4   1]\n",
      " [ 38  20   9   0   0]\n",
      " [ 29  28  47  44   2]\n",
      " [  1   2   3  23   3]\n",
      " [  0   1   0   5  14]]\n",
      "[[607  67  50   0   7]\n",
      " [ 44   8  14   0   1]\n",
      " [ 42  17  55  27   9]\n",
      " [  2   0   5  21   4]\n",
      " [  0   0   1   4  15]]\n",
      "[[573 102  51   2   3]\n",
      " [ 42  15   9   1   0]\n",
      " [ 35  32  52  30   1]\n",
      " [  2   1   5  24   0]\n",
      " [  0   0   2   5  13]]\n",
      "[[376 232 116   4   3]\n",
      " [ 25  24  17   0   1]\n",
      " [ 15  27  72  34   2]\n",
      " [  0   2   6  23   1]\n",
      " [  0   0   1   7  12]]\n",
      "[[576  96  54   3   2]\n",
      " [ 46  15   5   0   1]\n",
      " [ 31  29  61  26   3]\n",
      " [  1   2   6  20   3]\n",
      " [  0   0   1   6  13]]\n",
      "[[561 116  50   1   3]\n",
      " [ 44  17   6   0   0]\n",
      " [ 33  29  47  40   1]\n",
      " [  1   1   6  24   0]\n",
      " [  0   0   1   5  14]]\n",
      "[[506 138  80   4   3]\n",
      " [ 36  18  12   0   1]\n",
      " [ 23  31  53  40   3]\n",
      " [  1   0   5  23   3]\n",
      " [  0   0   0   5  15]]\n",
      "[[460 206  55   2   8]\n",
      " [ 38  20   8   0   1]\n",
      " [ 20  41  44  34  11]\n",
      " [  1   0   7  21   3]\n",
      " [  0   0   1   4  15]]\n",
      "[[522 148  55   1   5]\n",
      " [ 41  17   9   0   0]\n",
      " [ 25  36  54  29   6]\n",
      " [  1   2   7  21   1]\n",
      " [  0   0   2   2  16]]\n",
      "[[489 149  87   3   3]\n",
      " [ 32  24  11   0   0]\n",
      " [ 25  22  67  32   4]\n",
      " [  0   3   5  24   0]\n",
      " [  0   0   1   6  13]]\n",
      "[[433 211  86   0   1]\n",
      " [ 30  29   8   0   0]\n",
      " [ 16  35  64  33   2]\n",
      " [  0   2   6  24   0]\n",
      " [  0   0   2   6  12]]\n",
      "[[391 256  78   1   5]\n",
      " [ 30  27  10   0   0]\n",
      " [ 16  41  53  32   8]\n",
      " [  0   2   5  21   4]\n",
      " [  0   0   1   5  14]]\n",
      "[[385 236 101   5   4]\n",
      " [ 21  31  14   1   0]\n",
      " [ 13  31  60  42   4]\n",
      " [  1   0   3  26   2]\n",
      " [  0   0   0   5  15]]\n",
      "[[526 131  69   3   2]\n",
      " [ 38  19  10   0   0]\n",
      " [ 27  25  64  26   8]\n",
      " [  2   1   7  22   0]\n",
      " [  0   0   1   7  12]]\n",
      "[[536 108  76   5   6]\n",
      " [ 45  11  11   0   0]\n",
      " [ 28  22  66  29   5]\n",
      " [  0   3   6  21   2]\n",
      " [  0   0   2   2  16]]\n",
      "[[584  76  64   3   4]\n",
      " [ 45  13   8   1   0]\n",
      " [ 37  21  60  25   7]\n",
      " [  3   0   4  22   3]\n",
      " [  0   0   3   3  14]]\n",
      "[[669  26  31   0   5]\n",
      " [ 56   5   6   0   0]\n",
      " [ 55  19  41  25  10]\n",
      " [  3   1   3  21   4]\n",
      " [  0   1   1   4  14]]\n",
      "[[393 218 113   0   7]\n",
      " [ 30  20  16   1   0]\n",
      " [ 17  20  76  27  10]\n",
      " [  1   1   8  19   3]\n",
      " [  0   0   2   4  14]]\n",
      "[[533 129  66   0   3]\n",
      " [ 43  14  10   0   0]\n",
      " [ 22  29  77  17   5]\n",
      " [  0   3  10  12   7]\n",
      " [  1   0   3   1  15]]\n",
      "[[531  97  86  12   5]\n",
      " [ 37  16  11   1   2]\n",
      " [ 28  16  51  50   5]\n",
      " [  1   2   2  27   0]\n",
      " [  0   0   1   5  14]]\n",
      "[[418 141 167   2   3]\n",
      " [ 28  16  21   1   1]\n",
      " [ 13  17  80  36   4]\n",
      " [  0   1   6  24   1]\n",
      " [  0   0   2   5  13]]\n",
      "[[602  86  39   1   3]\n",
      " [ 51   8   7   0   1]\n",
      " [ 32  24  74  17   3]\n",
      " [  1   2  11  15   3]\n",
      " [  0   0   2   3  15]]\n",
      "[[501 107 109  10   4]\n",
      " [ 42  13  11   0   1]\n",
      " [ 20  19  72  36   3]\n",
      " [  1   0   5  22   4]\n",
      " [  0   0   2   4  14]]\n",
      "[[520 138  69   0   4]\n",
      " [ 42  14  10   0   1]\n",
      " [ 21  30  74  21   4]\n",
      " [  0   4   9  16   3]\n",
      " [  0   0   3   3  14]]\n",
      "[[502 138  85   4   2]\n",
      " [ 36  15  16   0   0]\n",
      " [ 21  19  82  26   2]\n",
      " [  1   1   6  22   2]\n",
      " [  0   0   2   4  14]]\n",
      "[[506 115 100   4   6]\n",
      " [ 37  16  14   0   0]\n",
      " [ 18  22  70  29  11]\n",
      " [  1   0   4  24   3]\n",
      " [  0   0   1   5  14]]\n",
      "[[412 257  57   1   4]\n",
      " [ 24  32  11   0   0]\n",
      " [ 14  36  69  26   5]\n",
      " [  0   3   5  21   3]\n",
      " [  0   0   2   4  14]]\n",
      "[[570  92  62   2   5]\n",
      " [ 40  15  10   1   1]\n",
      " [ 27  18  75  25   5]\n",
      " [  1   1   6  21   3]\n",
      " [  0   0   2   3  15]]\n",
      "[[569 111  43   3   5]\n",
      " [ 41  18   8   0   0]\n",
      " [ 30  25  60  28   7]\n",
      " [  1   3   7  17   4]\n",
      " [  0   0   2   4  14]]\n",
      "[[316 264 142   4   5]\n",
      " [ 16  32  18   0   1]\n",
      " [  9  21  91  28   1]\n",
      " [  1   0   8  22   1]\n",
      " [  0   0   2   4  14]]\n",
      "[[467 201  57   1   5]\n",
      " [ 33  22  11   0   1]\n",
      " [ 22  30  59  31   8]\n",
      " [  0   2   7  20   3]\n",
      " [  0   0   2   4  14]]\n",
      "[[436 179 106   3   7]\n",
      " [ 24  25  17   0   1]\n",
      " [ 13  19  82  34   2]\n",
      " [  1   1   6  23   1]\n",
      " [  0   0   2   5  13]]\n",
      "[[462 182  82   2   3]\n",
      " [ 32  21  14   0   0]\n",
      " [ 15  27  74  30   4]\n",
      " [  0   3   6  20   3]\n",
      " [  0   0   2   4  14]]\n",
      "[[441 195  92   0   3]\n",
      " [ 34  20  13   0   0]\n",
      " [ 14  31  76  24   5]\n",
      " [  1   1   9  18   3]\n",
      " [  0   0   2   5  13]]\n",
      "[[556 109  63   1   2]\n",
      " [ 39  20   8   0   0]\n",
      " [ 24  20  83  22   1]\n",
      " [  2   1   8  21   0]\n",
      " [  0   0   3   5  12]]\n",
      "[[519 136  72   1   3]\n",
      " [ 35  22  10   0   0]\n",
      " [ 25  32  65  24   4]\n",
      " [  0   2  10  17   3]\n",
      " [  0   1   1   4  14]]\n",
      "[[222 338 160   6   5]\n",
      " [ 13  30  24   0   0]\n",
      " [  6  19  88  34   3]\n",
      " [  0   1   7  24   0]\n",
      " [  0   0   1   6  13]]\n",
      "[[523 117  84   1   6]\n",
      " [ 42  17   8   0   0]\n",
      " [ 25  17  81  26   1]\n",
      " [  0   1  12  19   0]\n",
      " [  0   0   2   5  13]]\n",
      "[[531 115  79   4   2]\n",
      " [ 36  23   8   0   0]\n",
      " [ 17  26  83  22   2]\n",
      " [  1   1  11  19   0]\n",
      " [  1   0   2   5  12]]\n",
      "[[593  54  82   0   2]\n",
      " [ 48  10   9   0   0]\n",
      " [ 27  13  91  19   0]\n",
      " [  1   1  11  19   0]\n",
      " [  0   0   3   5  12]]\n",
      "[[639  34  53   0   5]\n",
      " [ 48   9  10   0   0]\n",
      " [ 40  14  59  34   3]\n",
      " [  1   0   9  21   1]\n",
      " [  0   0   2   5  13]]\n",
      "[[593  64  70   0   4]\n",
      " [ 44  10  12   1   0]\n",
      " [ 31  11  87  16   5]\n",
      " [  1   0   9  19   3]\n",
      " [  0   0   2   5  13]]\n",
      "[[622  56  48   0   5]\n",
      " [ 52   6   9   0   0]\n",
      " [ 37  18  75  20   0]\n",
      " [  2   1  12  17   0]\n",
      " [  0   0   2   4  14]]\n",
      "[[565 110  51   1   4]\n",
      " [ 39  19   9   0   0]\n",
      " [ 30  20  78  21   1]\n",
      " [  1   1  12  16   2]\n",
      " [  0   0   4   4  12]]\n",
      "[[492 153  80   2   4]\n",
      " [ 33  21  13   0   0]\n",
      " [ 17  28  80  24   1]\n",
      " [  1   0   7  23   1]\n",
      " [  0   0   2   5  13]]\n",
      "[[515 145  68   0   3]\n",
      " [ 34  21  12   0   0]\n",
      " [ 27  21  79  19   4]\n",
      " [  1   1  14  15   1]\n",
      " [  0   0   2   5  13]]\n",
      "[[605  70  48   3   5]\n",
      " [ 53   6   8   0   0]\n",
      " [ 31  25  68  22   4]\n",
      " [  1   2   7  22   0]\n",
      " [  0   1   1   5  13]]\n",
      "[[549 120  55   2   5]\n",
      " [ 42  14   9   1   1]\n",
      " [ 25  22  82  15   6]\n",
      " [  1   2  12  15   2]\n",
      " [  0   0   2   4  14]]\n",
      "[[571 108  47   1   4]\n",
      " [ 49   9   9   0   0]\n",
      " [ 27  28  72  22   1]\n",
      " [  1   1  10  19   1]\n",
      " [  0   0   2   5  13]]\n",
      "[[523  84 110   5   9]\n",
      " [ 40  13  11   2   1]\n",
      " [ 22  14  90  21   3]\n",
      " [  1   0   8  22   1]\n",
      " [  0   0   2   5  13]]\n",
      "[[608  82  39   0   2]\n",
      " [ 50  11   6   0   0]\n",
      " [ 33  24  76  16   1]\n",
      " [  2   1   8  21   0]\n",
      " [  0   0   3   5  12]]\n",
      "[[555 104  67   0   5]\n",
      " [ 38  18  11   0   0]\n",
      " [ 27  20  85  17   1]\n",
      " [  1   2  12  17   0]\n",
      " [  0   0   2   5  13]]\n",
      "[[466 168  89   3   5]\n",
      " [ 37  13  16   1   0]\n",
      " [ 22  18  73  36   1]\n",
      " [  1   0   8  23   0]\n",
      " [  0   0   1   6  13]]\n",
      "[[374 272  81   1   3]\n",
      " [ 25  31  11   0   0]\n",
      " [ 13  29  89  16   3]\n",
      " [  0   1  14  15   2]\n",
      " [  0   0   2   5  13]]\n",
      "[[272 351 100   2   6]\n",
      " [ 19  38   9   1   0]\n",
      " [ 10  27  83  23   7]\n",
      " [  0   2   7  20   3]\n",
      " [  0   0   2   3  15]]\n",
      "[[353 285  92   0   1]\n",
      " [ 23  30  14   0   0]\n",
      " [ 15  25  95  15   0]\n",
      " [  0   2  12  18   0]\n",
      " [  0   0   3   4  13]]\n",
      "[[431 259  35   0   6]\n",
      " [ 25  36   6   0   0]\n",
      " [ 16  35  84  14   1]\n",
      " [  0   3  11  18   0]\n",
      " [  0   0   2   5  13]]\n",
      "[[427 203  99   0   2]\n",
      " [ 27  28  12   0   0]\n",
      " [ 16  22  91  20   1]\n",
      " [  0   2  12  17   1]\n",
      " [  0   0   3   4  13]]\n",
      "[[469 179  77   0   6]\n",
      " [ 26  26  15   0   0]\n",
      " [ 21  27  79  19   4]\n",
      " [  0   2  12  16   2]\n",
      " [  0   0   3   3  14]]\n",
      "[[328 349  49   0   5]\n",
      " [ 21  40   6   0   0]\n",
      " [ 13  46  70  19   2]\n",
      " [  0   2  12  17   1]\n",
      " [  0   1   1   5  13]]\n",
      "[[219 414  95   0   3]\n",
      " [ 13  43  11   0   0]\n",
      " [  6  35  90  19   0]\n",
      " [  0   1  13  17   1]\n",
      " [  0   0   3   5  12]]\n",
      "[[430 228  65   2   6]\n",
      " [ 24  31  11   0   1]\n",
      " [ 19  21  83  22   5]\n",
      " [  1   1  12  17   1]\n",
      " [  0   0   2   5  13]]\n",
      "[[537 131  59   1   3]\n",
      " [ 38  21   7   1   0]\n",
      " [ 24  22  86  17   1]\n",
      " [  1   0  16  15   0]\n",
      " [  0   0   4   4  12]]\n",
      "[[288 326 104   6   7]\n",
      " [ 14  36  17   0   0]\n",
      " [  9  25  93  22   1]\n",
      " [  0   1  13  18   0]\n",
      " [  0   0   2   5  13]]\n",
      "[[386 289  51   1   4]\n",
      " [ 20  38   9   0   0]\n",
      " [ 20  36  74  17   3]\n",
      " [  0   3  13  16   0]\n",
      " [  0   0   2   5  13]]\n",
      "[[373 269  77   2  10]\n",
      " [ 19  34  14   0   0]\n",
      " [ 20  23  83  18   6]\n",
      " [  0   2   8  18   4]\n",
      " [  0   0   2   3  15]]\n",
      "[[502 181  44   0   4]\n",
      " [ 30  28   9   0   0]\n",
      " [ 23  40  77   9   1]\n",
      " [  1   2  17  10   2]\n",
      " [  0   0   3   4  13]]\n",
      "[[428 203  94   1   5]\n",
      " [ 28  25  12   1   1]\n",
      " [ 16  24  97  13   0]\n",
      " [  1   1  13  15   2]\n",
      " [  1   0   3   3  13]]\n",
      "[[375 270  83   0   3]\n",
      " [ 20  36  11   0   0]\n",
      " [ 12  29 101   8   0]\n",
      " [  0   3  16  12   1]\n",
      " [  0   0   2   5  13]]\n",
      "[[409 210 107   0   5]\n",
      " [ 22  27  18   0   0]\n",
      " [ 16  16 104  14   0]\n",
      " [  1   1  13  14   3]\n",
      " [  0   0   3   4  13]]\n",
      "[[318 279 127   1   6]\n",
      " [ 18  30  19   0   0]\n",
      " [ 12  21  98  17   2]\n",
      " [  0   0  15  15   2]\n",
      " [  0   0   2   5  13]]\n",
      "[[482 172  71   2   4]\n",
      " [ 30  22  15   0   0]\n",
      " [ 18  19  88  21   4]\n",
      " [  0   2  12  16   2]\n",
      " [  0   0   2   6  12]]\n",
      "[[469 149 108   0   5]\n",
      " [ 31  19  16   1   0]\n",
      " [ 21  22  90  12   5]\n",
      " [  1   0  15  12   4]\n",
      " [  0   0   5   3  12]]\n",
      "[[504 154  65   1   7]\n",
      " [ 31  24  12   0   0]\n",
      " [ 25  26  82  14   3]\n",
      " [  0   2  11  18   1]\n",
      " [  0   0   3   5  12]]\n",
      "[[355 264 108   1   3]\n",
      " [ 20  33  14   0   0]\n",
      " [ 10  28  90  20   2]\n",
      " [  0   2  14  16   0]\n",
      " [  0   0   3   5  12]]\n"
     ]
    }
   ],
   "source": [
    "# trainer.train(\"./MedViT-base/checkpoint-22800\")\n",
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0a03225-4eb7-4293-858b-636148009e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35124\n"
     ]
    }
   ],
   "source": [
    "!ls ../mnt/local/data/kalexu97/saliency_mask -1 | wc -l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
