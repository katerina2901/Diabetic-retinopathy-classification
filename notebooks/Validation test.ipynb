{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8fb8848-e455-40a1-a5e9-85eab4fcc4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from PIL import Image\n",
    "# from torchinfo import summary\n",
    "import torch\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from typing import Tuple\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "# import wandb\n",
    "# wandb.login(\"1fa58b4e42c64c2531b3abeb43c04f5991be307e\")\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe740bd-d5be-4b90-bffc-a3bc82b93a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MedViT\n",
    "!git clone https://github.com/Omid-Nejati/MedViT.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4aa136-d639-4d46-8ea4-148ebd78c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!pip install -r MedViT/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2be4bc43-c2ea-4d16-95af-2f755063a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls ../mnt/local/data/kalexu97/DR_grading/DR_grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27f174c4-0ad9-4d39-8601-eba691e0c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsTable = pd.read_csv('../mnt/local/data/kalexu97/DR_grading.csv')\n",
    "root_dir = '../mnt/local/data/kalexu97/DR_grading/DR_grading'\n",
    "labelsTable['image_path'] = labelsTable['id_code'].apply(lambda x: os.path.join(root_dir, x))\n",
    "labelsTable['label'] = labelsTable['diagnosis']\n",
    "\n",
    "test_dataset = labelsTable.drop(columns=['id_code', 'diagnosis'], axis=1)\n",
    "\n",
    "# test_dataset = pd.read_csv('test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38dd5604-9351-4955-a9e5-fd6681cb56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "model_name_or_path = \"./saved_models/MedViT512_tr35_stage4(2)_Spot2HTrvlAug_fastvitprepr_lr2e5\" \n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name_or_path)\n",
    "image_processor.size['height'] = 512\n",
    "image_processor.size['width'] = 512\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "class CenterCrop(torch.nn.Module):\n",
    "    def __init__(self, size=None, ratio=\"1:1\"):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.ratio = ratio\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image or Tensor): Image to be cropped.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image or Tensor: Cropped image.\n",
    "        \"\"\"\n",
    "        if self.size is None:\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                h, w = img.shape[-2:]\n",
    "            else:\n",
    "                w, h = img.size\n",
    "            ratio = self.ratio.split(\":\")\n",
    "            ratio = float(ratio[0]) / float(ratio[1])\n",
    "            # Size must match the ratio while cropping to the edge of the image\n",
    "            # print(ratio, w, h)\n",
    "            ratioed_w = int(h * ratio)\n",
    "            ratioed_h = int(w / ratio)\n",
    "            if w>=h:\n",
    "                if ratioed_h <= h:\n",
    "                    size = (ratioed_h, w)\n",
    "                else:\n",
    "                    size = (h, ratioed_w)\n",
    "            else:\n",
    "                if ratioed_w <= w:\n",
    "                    size = (h, ratioed_w)\n",
    "                else:\n",
    "                    size = (ratioed_h, w)\n",
    "        else:\n",
    "            size = self.size\n",
    "        return torchvision.transforms.functional.center_crop(img, size)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(size={self.size})\"\n",
    "\n",
    "_transforms_test = T.Compose([\n",
    "    CenterCrop(),\n",
    "])\n",
    "\n",
    "def load_image(path_image, label, mode):\n",
    "    # load image\n",
    "    image = Image.open(path_image)\n",
    "    # print(image.size)\n",
    "\n",
    "    if mode == 'train':\n",
    "        image = _transforms_train(image)\n",
    "\n",
    "        return image\n",
    "    else:\n",
    "        image = _transforms_test(image)\n",
    "        # print(image.size)\n",
    "        return image\n",
    "\n",
    "def func_transform_test(examples):\n",
    "    \n",
    "    # loaded_images = [load_image(path, lb, 'test').convert(\"RGB\") for path, lb in zip(examples['image_path'], examples['label'])]\n",
    "    inputs = image_processor([load_image(path, lb, 'test').convert(\"RGB\")\n",
    "                                for path, lb in zip(examples['image_path'], examples['label'])], return_tensors='pt')\n",
    "    inputs['label'] = examples['label']\n",
    "    # print(inputs)\n",
    "    return inputs\n",
    "\n",
    "test_ds = Dataset.from_pandas(test_dataset, preserve_index=False)\n",
    "prepared_ds_test = test_ds.with_transform(func_transform_test)\n",
    "prepared_ds_test = prepared_ds_test.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "727f99f2-6649-47c0-9508-8a2fc9e27bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_path', 'label'],\n",
       "    num_rows: 12522\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_ds_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecba950b-b00f-4edb-980b-e70f70123442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows in test_dataset:  12522\n",
      "ID2label:  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n"
     ]
    }
   ],
   "source": [
    "print(\"rows in test_dataset: \", len(prepared_ds_test))\n",
    "\n",
    "labels = [0, 1, 2, 3, 4]\n",
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "print(\"ID2label: \", id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b0ffd51-fd89-48da-9e66-d520af393c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1e3f741-cfbb-47b5-925e-6249fd9325a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score #, kappa\n",
    "# from sklearn import metrics\n",
    "\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions_proba, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(predictions_proba, axis=1)\n",
    "    result_accuracy = accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    result = {\n",
    "             'accuracy': np.mean([result_accuracy['accuracy']]),\n",
    "             'kappa': np.mean([cohen_kappa_score(labels, predictions, weights = \"quadratic\")]),\n",
    "             'f1': np.mean([f1_score(labels, predictions, average='weighted')]),\n",
    "             }\n",
    "\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    print(cm)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9901d1e-81a8-4c1b-adbb-2ea8fceb9224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with random weights:\n",
      "initialize_weights...\n"
     ]
    }
   ],
   "source": [
    "# Initialise a MedViT class\n",
    "from transformers import PreTrainedModel\n",
    "from MedViT.MedViT import MedViT, MedViT_large\n",
    "\n",
    "# Define configuration\n",
    "from transformers import PretrainedConfig\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class MedViTConfig(PretrainedConfig):\n",
    "    model_type = \"medvit\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        stem_chs: List[int] = [64, 32, 64],\n",
    "        depths: List[int] = [3, 4, 30, 3],\n",
    "        path_dropout: float = 0.2,\n",
    "        attn_drop: int = 0,\n",
    "        drop: int = 0,\n",
    "        num_classes: int = 5,\n",
    "        strides: List[int] = [1, 2, 2, 2],\n",
    "        sr_ratios: List[int] = [8, 4, 2, 1],\n",
    "        head_dim: int = 32,\n",
    "        mix_block_ratio: float = 0.75,\n",
    "        use_checkpoint: bool = False,\n",
    "        pretrained: bool = False,\n",
    "        pretrained_cfg: str = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.stem_chs = stem_chs\n",
    "        self.depths = depths\n",
    "        self.path_dropout = path_dropout\n",
    "        self.attn_drop = attn_drop\n",
    "        self.drop = drop\n",
    "        self.num_classes = num_classes\n",
    "        self.strides = strides\n",
    "        self.sr_ratios = sr_ratios\n",
    "        self.head_dim = head_dim\n",
    "        self.mix_block_ratio = mix_block_ratio\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.pretrained = pretrained,\n",
    "        self.pretrained_cfg = pretrained_cfg\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class MedViTClassification(PreTrainedModel):\n",
    "    config_class = MedViTConfig\n",
    "\n",
    "    def __init__(self, config, pretrained=False):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if pretrained is False:\n",
    "          print('Initialized with random weights:')\n",
    "          self.model = MedViT(\n",
    "          stem_chs = config.stem_chs,\n",
    "          depths = config.depths,\n",
    "          path_dropout = config.path_dropout,\n",
    "          attn_drop = config.attn_drop,\n",
    "          drop = config.drop,\n",
    "          num_classes = config.num_classes,\n",
    "          strides = config.strides,\n",
    "          sr_ratios = config.sr_ratios,\n",
    "          head_dim = config.head_dim,\n",
    "          mix_block_ratio = config.mix_block_ratio,\n",
    "          use_checkpoint = config.use_checkpoint)\n",
    "        else:\n",
    "          print('Initialized with pretrained weights:')\n",
    "          self.model = MedViT_large(use_checkpoint = config.use_checkpoint)\n",
    "          # self.state_dict = torch.load(config.pretrained_cfg, weights_only=True) #, weights_only=True\n",
    "          self.model.load_state_dict(torch.load(config.pretrained_cfg, weights_only=True)['model'])\n",
    "          # self.model.load_state_dict(torch.load('MedViT_large_im1k.pth'))\n",
    "          self.model.proj_head = nn.Linear(1024, 5)        \n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        logits = self.model(pixel_values)\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "model = MedViTClassification.from_pretrained(\"./saved_models/MedViT512_tr35_stage5(2)_Spot2HTrvlAug_fastvitprepr_lr1e5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2b9641b-22ea-45b1-af78-57ede4e0c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./MedViT-base_test\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers = 16,\n",
    "    lr_scheduler_type = 'constant_with_warmup', # 'constant', #'cosine', #'constant_with_warmup',\n",
    "    \n",
    "    learning_rate=1e-5,\n",
    "    # label_smoothing_factor = 0.6,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    warmup_ratio=0.02,\n",
    "    \n",
    "    metric_for_best_model=\"kappa\", \n",
    "    greater_is_better = True,\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds_test,\n",
    "    eval_dataset=prepared_ds_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7db0047f-2087-48ad-8e19-c44fdfdc4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = np.random.choice(len(prepared_ds_test), size=100, replace=False)\n",
    "inv_sample_ids = np.setdiff1d(np.arange(len(prepared_ds_test)), sample_ids)\n",
    "val_ds = prepared_ds_test.select(sample_ids)\n",
    "test_ds = prepared_ds_test.select(inv_sample_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "846c9c14-ffbe-4ff9-b7c4-96dd7419bfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3131' max='3131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3131/3131 08:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5553  449  182   11   71]\n",
      " [ 205  291  102    8   24]\n",
      " [ 577  450 2199  635  616]\n",
      " [   1    5   47  122   61]\n",
      " [  58    7   96   61  691]]\n",
      "***** eval metrics *****\n",
      "  eval_accuracy           =     0.7072\n",
      "  eval_f1                 =      0.723\n",
      "  eval_kappa              =     0.7685\n",
      "  eval_loss               =     0.9081\n",
      "  eval_runtime            = 0:08:29.45\n",
      "  eval_samples_per_second =     24.579\n",
      "  eval_steps_per_second   =      6.146\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(prepared_ds_test)\n",
    "trainer.log_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51d2388b-55f1-4d5a-acee-0b1c38599980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_accuracy           =      0.175\n",
      "  eval_f1                 =     0.2025\n",
      "  eval_kappa              =     0.0473\n",
      "  eval_loss               =     3.1161\n",
      "  eval_runtime            = 0:08:32.99\n",
      "  eval_samples_per_second =      24.41\n",
      "  eval_steps_per_second   =      6.103\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0dc3d857-8ed0-41fb-a5ea-fd08809de3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4679  348  135   10   31]\n",
      " [ 188  260   44    1    6]\n",
      " [ 167  188  531   90   41]\n",
      " [   6    5   46   73   25]\n",
      " [   4    1   26   11  110]]\n",
      "***** eval metrics *****\n",
      "  eval_accuracy           =     0.8046\n",
      "  eval_f1                 =     0.8126\n",
      "  eval_kappa              =     0.7629\n",
      "  eval_loss               =     0.6082\n",
      "  eval_runtime            = 0:04:46.89\n",
      "  eval_samples_per_second =      24.49\n",
      "  eval_steps_per_second   =      6.124\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(prepared_ds_test)\n",
    "trainer.save_metrics(\"eval\", metrics)\n",
    "trainer.log_metrics(\"eval\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
